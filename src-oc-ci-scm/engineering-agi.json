{
  "title": "Engineering General Intelligence: APPENDICES B-H",
  "authors": [
    "Ben Goertzel",
    "Cassio Pennachin",
    "Nil Geisweiller",
    "the OpenCog Team"
  ],
  "date": "December 14, 2013",
  "table_of_contents": [
    {
      "id": "B",
      "title": "Steps Toward a Formal Theory of Cognitive Structure and Dynamics",
      "page": 1,
      "subsections": [
        {
          "id": "B.1",
          "title": "Introduction",
          "page": 1
        },
        {
          "id": "B.2",
          "title": "Modeling Memory Types Using Category Theory",
          "page": 2,
          "subsections": [
            {
              "id": "B.2.1",
              "title": "The Category of Procedural Memory",
              "page": 2
            },
            {
              "id": "B.2.2",
              "title": "The Category of Declarative Memory",
              "page": 2
            },
            {
              "id": "B.2.3",
              "title": "The Category of Episodic Memory",
              "page": 3
            },
            {
              "id": "B.2.4",
              "title": "The Category of Intentional Memory",
              "page": 3
            },
            {
              "id": "B.2.5",
              "title": "The Category of Attentional Memory",
              "page": 3
            }
          ]
        },
        {
          "id": "B.3",
          "title": "Modeling Memory Type Conversions Using Functors",
          "page": 3,
          "subsections": [
            {
              "id": "B.3.1",
              "title": "Converting Between Declarative and Procedural Knowledge",
              "page": 3
            },
            {
              "id": "B.3.2",
              "title": "Symbol Grounding: Converting Between Episodic and Declarative Knowledge",
              "page": 4
            },
            {
              "id": "B.3.3",
              "title": "Converting Between Episodic and Procedural Knowledge",
              "page": 7
            },
            {
              "id": "B.3.4",
              "title": "Converting Intentional or Attentional Knowledge into Declarative or Procedural Knowledge",
              "page": 7
            },
            {
              "id": "B.3.5",
              "title": "Converting Episodic Knowledge into Intentional or Attentional Knowledge",
              "page": 7
            }
          ]
        },
        {
          "id": "B.4",
          "title": "Metrics on Memory Spaces",
          "page": 7,
          "subsections": [
            {
              "id": "B.4.1",
              "title": "Information Geometry on Memory Spaces",
              "page": 8
            },
            {
              "id": "B.4.2",
              "title": "Algorithmic Distance on Memory Spaces",
              "page": 9
            }
          ]
        },
        {
          "id": "B.5",
          "title": "Three Hypotheses About the Geometry of Mind",
          "page": 10,
          "subsections": [
            {
              "id": "B.5.1",
              "title": "Hypothesis 1: Syntax-Semantics Correlation",
              "page": 10
            },
            {
              "id": "B.5.2",
              "title": "Hypothesis 2: Cognitive Geometrodynamics",
              "page": 10
            },
            {
              "id": "B.5.3",
              "title": "Hypothesis 3: Cognitive Synergy",
              "page": 11
            }
          ]
        },
        {
          "id": "B.6",
          "title": "Next Steps in Refining These Ideas",
          "page": 12
        },
        {
          "id": "B.7",
          "title": "Returning to Our Basic Claims About CogPrime",
          "page": 12
        }
      ]
    },
    {
      "id": "C",
      "title": "Emergent Reflexive Mental Structures",
      "page": 17,
      "subsections": [
        {
          "id": "C.1",
          "title": "Introduction",
          "page": 17
        },
        {
          "id": "C.2",
          "title": "Hypersets and Patterns",
          "page": 18,
          "subsections": [
            {
              "id": "C.2.1",
              "title": "Hypersets as Patterns in Physical or Computational Systems",
              "page": 19
            }
          ]
        },
        {
          "id": "C.3",
          "title": "A Hyperset Model of Reflective Consciousness",
          "page": 20
        },
        {
          "id": "C.4",
          "title": "A Hyperset Model of Will",
          "page": 23,
          "subsections": [
            {
              "id": "C.4.1",
              "title": "In What Sense Is Will Free?",
              "page": 25
            },
            {
              "id": "C.4.2",
              "title": "Connecting Will and Consciousness",
              "page": 26
            }
          ]
        },
        {
          "id": "C.5",
          "title": "A Hyperset Model of Self",
          "page": 26
        },
        {
          "id": "C.6",
          "title": "Validating Hyperset Models of Experience",
          "page": 28
        },
        {
          "id": "C.7",
          "title": "Implications for Practical Work on Machine Consciousness",
          "page": 29,
          "subsections": [
            {
              "id": "C.7.1",
              "title": "Attentional Focus in CogPrime",
              "page": 29
            },
            {
              "id": "C.7.2",
              "title": "Maps and Focused Attention in CogPrime",
              "page": 30
            },
            {
              "id": "C.7.3",
              "title": "Reflective Consciousness, Self and Will in CogPrime",
              "page": 31
            },
            {
              "id": "C.7.4",
              "title": "Encouraging the Recognition of Self-Referential Structures in the AtomSpace",
              "page": 32
            }
          ]
        },
        {
          "id": "C.8",
          "title": "Algebras of the Social Self",
          "page": 33
        },
        {
          "id": "C.9",
          "title": "The Intrinsic Sociality of the Self",
          "page": 33
        },
        {
          "id": "C.10",
          "title": "Mirror Neurons and Associated Neural Systems",
          "page": 34,
          "subsections": [
            {
              "id": "C.10.1",
              "title": "Mirror Systems",
              "page": 35
            }
          ]
        },
        {
          "id": "C.11",
          "title": "Quaternions and Octonions",
          "page": 36
        },
        {
          "id": "C.12",
          "title": "Modeling Mirrorhouses Using Quaternions and Octonions",
          "page": 38
        },
        {
          "id": "C.13",
          "title": "Specific Instances of Mental Mirrorhousing",
          "page": 43
        },
        {
          "id": "C.14",
          "title": "Mirroring in Development",
          "page": 45
        },
        {
          "id": "C.15",
          "title": "Concluding Remarks",
          "page": 46
        }
      ]
    },
    {
      "id": "D",
      "title": "GOLEM: Toward an AGI Meta-Architecture Enabling Both Goal Preservation and Radical Self-Improvement",
      "page": 47,
      "subsections": [
        {
          "id": "D.1",
          "title": "Introduction",
          "page": 47
        },
        {
          "id": "D.2",
          "title": "The Goal Oriented Learning Meta-Architecture",
          "page": 48,
          "subsections": [
            {
              "id": "D.2.1",
              "title": "Optimizing the GoalEvaluator",
              "page": 50
            },
            {
              "id": "D.2.2",
              "title": "Conservative Meta-Architecture Preservation",
              "page": 51
            },
            {
              "id": "D.2.3",
              "title": "Complexity and Convergence Rate",
              "page": 51
            }
          ]
        },
        {
          "id": "D.3",
          "title": "The Argument For GOLEM’s Steadfastness",
          "page": 52
        },
        {
          "id": "D.4",
          "title": "A Partial Formalization of the Architecture and Steadfastness Argument",
          "page": 52,
          "subsections": [
            {
              "id": "D.4.1",
              "title": "Toward a Formalization of GOLEM",
              "page": 53
            },
            {
              "id": "D.4.2",
              "title": "Some Conjectures About GOLEM",
              "page": 53
            }
          ]
        },
        {
          "id": "D.5",
          "title": "Comparison to a Reinforcement Learning Based Formulation",
          "page": 55
        },
        {
          "id": "D.6",
          "title": "Specifying the Letter and Spirit of Goal Systems (Are Both Difficult Tasks)",
          "page": 56
        },
        {
          "id": "D.7",
          "title": "A More Radically Self-Modifying GOLEM",
          "page": 57
        },
        {
          "id": "D.8",
          "title": "Concluding Remarks",
          "page": 58
        }
      ]
    },
    {
      "id": "E",
      "title": "Lojban++: A Novel Linguistic Mechanism for Teaching AGI Systems",
      "page": 61,
      "subsections": [
        {
          "id": "E.1",
          "title": "Introduction",
          "page": 61
        },
        {
          "id": "E.2",
          "title": "Lojban versus Lojban++",
          "page": 62
        },
        {
          "id": "E.3",
          "title": "Some Simple Examples",
          "page": 63
        },
        {
          "id": "E.4",
          "title": "The Need for Lojban Software",
          "page": 65
        },
        {
          "id": "E.5",
          "title": "Lojban and Inference",
          "page": 66,
          "subsections": [
            {
              "id": "E.5.1",
              "title": "Lojban versus Predicate Logic",
              "page": 66
            }
          ]
        },
        {
          "id": "E.6",
          "title": "Discussion",
          "page": 67
        },
        {
          "id": "E.7",
          "title": "Postscript: Basic Principles for Using English Words in Lojban++",
          "page": 68
        },
        {
          "id": "E.8",
          "title": "Syntax-based Argument Structure Conventions for English Words",
          "page": 69
        },
        {
          "id": "E.9",
          "title": "Semantics-based Argument Structure Conventions for English Words",
          "page": 70
        },
        {
          "id": "E.10",
          "title": "Lojban gismu of clear use within Lojban++",
          "page": 72
        },
        {
          "id": "E.11",
          "title": "Special Lojban++ cmavo",
          "page": 72,
          "subsections": [
            {
              "id": "E.11.1",
              "title": "qui",
              "page": 73
            },
            {
              "id": "E.11.2",
              "title": "it, quu",
              "page": 73
            },
            {
              "id": "E.11.3",
              "title": "quay",
              "page": 74
            }
          ]
        }
      ]
    },
    {
      "id": "F",
      "title": "PLN and the Brain",
      "page": 75,
      "subsections": [
        {
          "id": "F.1",
          "title": "How Might Probabilistic Logic Networks Emerge from Neural Structures and Dynamics?",
          "page": 75
        },
        {
          "id": "F.2",
          "title": "Avoiding Issues with Circular Inference",
          "page": 77
        },
        {
          "id": "F.3",
          "title": "Neural Representation of Recursion and Abstraction",
          "page": 79
        }
      ]
    },
    {
      "id": "G",
      "title": "Possible Worlds Semantics and Experiential Semantics",
      "page": 81,
      "subsections": [
        {
          "id": "G.1",
          "title": "Introduction",
          "page": 81
        },
        {
          "id": "G.2",
          "title": "Inducing a Distribution over Predicates and Concepts",
          "page": 82
        },
        {
          "id": "G.3",
          "title": "Grounding Possible Worlds Semantics in Experiential Semantics",
          "page": 83
        },
        {
          "id": "G.4",
          "title": "Reinterpreting Indefinite Probabilities",
          "page": 86,
          "subsections": [
            {
              "id": "G.4.1",
              "title": "Reinterpreting Indefinite Quantifiers",
              "page": 87
            }
          ]
        },
        {
          "id": "G.5",
          "title": "Specifying Complexity for Intensional Inference",
          "page": 88
        },
        {
          "id": "G.6",
          "title": "Reinterpreting Implication between Inheritance Relationships",
          "page": 88
        },
        {
          "id": "G.7",
          "title": "Conclusion",
          "page": 89
        }
      ]
    },
    {
      "id": "H",
      "title": "Propositions About Environments in Which CogPrime Components Are Useful",
      "page": 91,
      "subsections": [
        {
          "id": "H.1",
          "title": "Propositions about MOSES",
          "page": 91,
          "subsections": [
            {
              "id": "H.1.1",
              "title": "Proposition: ENF Helps to Guide Syntax-Based Program Space Search",
              "page": 91
            },
            {
              "id": "H.1.2",
              "title": "Demes are Useful if Syntax/Semantics Correlations in Program Space Have a Small Scale",
              "page": 92
            },
            {
              "id": "H.1.3",
              "title": "Probabilistic Program Tree Modeling Helps in the Presence of Cross-Modular Dependencies",
              "page": 92
            },
            {
              "id": "H.1.4",
              "title": "Relating ENF to BOA",
              "page": 92
            },
            {
              "id": "H.1.5",
              "title": "Conclusion Regarding Speculative MOSES Theory",
              "page": 93
            }
          ]
        },
        {
          "id": "H.2",
          "title": "Propositions About CogPrime",
          "page": 94,
          "subsections": [
            {
              "id": "H.2.1",
              "title": "When PLN Inference Beats BOA",
              "page": 94
            },
            {
              "id": "H.2.2",
              "title": "Conditions for the Usefulness of Hebbian Inference Control",
              "page": 94
            },
            {
              "id": "H.2.3",
              "title": "Clustering-together of Smooth Theorems",
              "page": 95
            },
            {
              "id": "H.2.4",
              "title": "When PLN is Useful Within MOSES",
              "page": 95
            },
            {
              "id": "H.2.5",
              "title": "When MOSES is Useful Within PLN",
              "page": 95
            },
            {
              "id": "H.2.6",
              "title": "On the Smoothness of Some Relevant Theorems",
              "page": 96
            },
            {
              "id": "H.2.7",
              "title": "Recursive Use of “MOSES with PLN” to Help With Attention Allocation",
              "page": 96
            },
            {
              "id": "H.2.8",
              "title": "The Value of Conceptual Blending",
              "page": 97
            },
            {
              "id": "H.2.9",
              "title": "A Justification of Map Formation",
              "page": 97
            }
          ]
        },
        {
          "id": "H.3",
          "title": "Concluding Remarks",
          "page": 97
        }
      ]
    }
  ],
  "appendices": [
    {
      "id": "B",
      "title": "Steps Toward a Formal Theory of Cognitive Structure and Dynamics",
      "content": [
        {
          "type": "section",
          "level": 1,
          "id": "B.1",
          "title": "Introduction",
          "text": "Transforming the conceptual and formal ideas of Section ?? into rigorous mathematical theory\nwill be a large enterprise, and is not something we have achieved so far. However, we do believe\nwe have some idea regarding what kind of mathematical and conceptual toolset will be useful for\nenacting this transformation. In this appendix we will elaborate our ideas regarding this toolset,\nand in the process present some concrete notions such as a novel mathematical formulation of\nthe concept of cognitive synergy, and a more formal statement of many of the \"key claims\"\nregarding CogPrime given in Chapter ??.\nThe key ideas involved here are: modeling multiple memory types as mathematical categories\n(with functors mapping between them), modeling memory items as probability distributions,\nand measuring distance between memory items using two metrics, one based on algorithmic\ninformation theory and one on classical information geometry. Building on these ideas, core\nhypotheses are then presented:\n\n• a syntax-semantics correlation principle, stating that in a successful AGI system, these\ntwo metrics should be roughly correlated\n• a cognitive geometrodynamics principle, stating that on the whole intelligent minds\ntend to follow geodesics (shortest paths) in mindspace, according to various appropriately\ndefined metrics (e.g. the metric measuring the distance between two entities in terms of the\nlength and/or runtime of the shortest programs computing one from the other).\n• a cognitive synergy principle, stating that shorter paths may be found through the com posite mindspace formed by considering multiple memory types together, than by following\nthe geodesics in the mindspaces corresponding to individual memory types.\n\nThese ideas are not strictly necessary for understanding the CogPrime design as outlined in\nPart 2 of this book. However, our hope is that they will be helpful later on for elaborating a\ndeeper theoretical understanding of CogPrime, and hence in developing the technical aspects\nof the CogPrime design beyond the stage presented in Part 2. Our sense is that, ultimately,\nthe theory and practice of AGI will both go most smoothly if they can proceed together,\nwith theory guiding algorithm and architecture tuning, but also inspired by lessons learned via\npractical experimentation. At present the CogPrime design has been inspired by a combination\nof broad theoretical notions about the overall architecture, and specific theoretical calculations\nregarding specific components. One of our hopes is that in later versions of CogPrime, precise\ntheoretical calculations regarding the overall architecture may also be possible, perhaps using\nideas descending from those in this appendix."
        },
        {
          "type": "section",
          "level": 1,
          "id": "B.2",
          "title": "Modeling Memory Types Using Category Theory",
          "text": "We begin by formalizing the different types of memory critical for a human-like integrative AGI\nsystem, in a manner that makes it easy to study mappings between different memory types. One\nway to do this is to consider each type of memory as a category, in the sense of category theory.\nSpecifically, in this section we roughly indicate how one may model declarative, procedural,\nepisodic, attentional and intentional categories, thus providing a framework in which mapping\nbetween these different memory types can be modeled using functors. The discussion is quite\nbrief and general, avoiding commitments about how memories are implemented.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "B.2.1",
              "title": "The Category of Procedural Memory",
              "text": "We model the space of procedures as a graph. We assume there exists a set T of “atomic\ntransformations” on the category CP roc of procedures, so that each t ∈ T maps an input\nprocedure into a unique output procedure. We then consider a labeled digraph whose nodes\nare objects in CP roc (i.e. procedures), and which has a link labeled t between procedure P1\nand P2 if t maps P1 into P2. Morphisms on program space may then be taken as paths in this\ndigraph, i.e. as composite procedure transformations defined by sequences of atomic procedure\ntransformations.\nAs an example, if procedures are represented as ensembles of program trees, where program\ntrees are defined in the manner suggested in [?] and ??, then one can consider tree edit operations\nas defined in [?] as one’s atomic transformations. If procedures are represented as formal neural\nnets or ensembles thereof, one can take a similar approach."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.2.2",
              "title": "The Category of Declarative Memory",
              "text": "The category CDec of declarative knowledge may be handled somewhat similarly, via assuming\nthe existence of a set of transformations between declarative knowledge items, constructing\na labeled digraph induced by these transformations, and defining morphisms as paths in this\ndigraph. For example, if declarative knowledge items are represented as expressions in some\nlogical language, then transformations may be naturally taken to correspond to inference steps\nin the associated logic system. Morphisms then represent sequences of inference steps that\ntransform one logical expression into another."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.2.3",
              "title": "The Category of Episodic Memory",
              "text": "What about episodic memory – the record an intelligence keeps of its own experiences? Given\nthat we are talking about intelligences living in a world characterized by three spatial dimensions\nand one temporal dimension, one way to model a remembered episode (i.e., an object in the\ncategory CEp of episodic memories) is as as a scalar field defined over a grid-cell discretization\nof 4D spacetime. The scalar field, integrated over some region of spacetime, tells the extent to\nwhich that region belongs to the episode. In this way one may also consider episodes as fuzzy\nsets of spacetime regions. We may then consider a category whose objects are episode-sets, i.e.\nfuzzy sets of fuzzy sets of spacetime regions.\nTo define morphisms on the space of episode-sets, one approach is to associate an episode E\nwith the set PE, of programs that calculate the episode within a given error . One may then\nconstruct a graph whose nodes are episode-sets, and in which E1 is linked to E2 if applying an\natomic procedure-transformation to some program in PE1, yields a program in PE2, ."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.2.4",
              "title": "The Category of Intentional Memory",
              "text": "To handle the category CInt of intentional knowledge, we recall that in our formal agents model,\ngoals are functions. Therefore, to specify the category of goals a logic of functions may be used\n(e.g. as in [?]), transformations corresponding to logical inference steps in the logic of functions."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.2.5",
              "title": "The Category of Attentional Memory",
              "text": "Finally, the category CAtt of attentional knowledge is handled somewhat similarly to goals.\nAttentional evaluations may be modeled as maps from elements of CInt ∪ CDec ∪ CEp ∪ CP roc\ninto a space V of AttentionValues. As such, attentional evaluations are functions, and may be\nconsidered as a category in a manner similar to the goal functions."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "B.3",
          "title": "Modeling Memory Type Conversions Using Functors",
          "text": "Having modeled memory types as categories, we may now model conversions between memory\ntypes as mappings between categories. This is one step on the path to formalizing the notion\nof cognitive synergy within the formal cognitive architecture presented in the previous section.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "B.3.1",
              "title": "Converting Between Declarative and Procedural Knowledge",
              "text": "To understand conversion back and forth between declarative and procedural knowledge, con sider the cases:\n\n• the category blue versus the procedure isBlue that outputs a number in [0, 1] indicating\nthe degree of blueness of its input\n• the statement “the sky is blue” versus the procedure that outputs a number [0, 1] indicating\nthe degree to which its input is semantically similar to the statement “the sky is blue”\n• a procedure for serving a tennis ball on the singles boundary at the edge of the service box,\nas close as possible to the net; versus a detailed description of this procedure, of the sort\nthat could be communicated verbally (though it might take a long time)\n• a procedure for multiplying numbers, versus a verbal description of that procedure\n• a logical description of the proof of a theorem based on some axioms; versus a procedure\nthat produces the theorem given the axioms as inputs\n\nFrom these examples we can see that procedural and declarative knowledge are in a sense\ninterchangeable; and yet, some entities seem more naturally represented procedurally, whereas\nother seem more naturally represented declaratively. Relatedly, it seems that some knowledge\nis more easily obtained via learning algorithms that operate on procedural representations; and\nother knowledge is more easily obtained via learning algorithms that operate on declarative\nrepresentations.\nFormally, we may define a “procedure declaratization” as a functor from KP roc to KDec; in\nother words, a pair of mappings (r, s) so that\n\n• r maps each object in KP roc into some object in KDec\n• s maps each morphism fP roc,i in KP roc into some morphism in KDec, in a way that obeys\ns(fP roc,i ◦ fP roc,j ) = s(fP roc,i) ◦ s(fP roc,j )\n\nSimilarly, we may define a “declaration procedurization” as a functor from KDec to KP roc."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.3.2",
              "title": "Symbol Grounding: Converting Between Episodic and Declarative Knowledge",
              "text": "Next we consider converting back and forth between episodic and declarative knowledge. Partic ular cases of this conversion have received significant attention in the cognitive science literature,\nreferred to by the term “symbol grounding.”\nIt is relatively straightforward to define “episode declaratization” and “declaration episodiza tion” functors formally in the manner of the above definitions regarding declarative/procedural\nconversion. Conceptually,\n\n• Episode declaratization produces a declaration describing an episode-set (naturally this\ndeclaration may be a conjunction of many simple declarations)\n• Declaration episodization produces an episode-set defined as the set of episodes whose\ndescriptions include a certain declaration\n\nAs a very simple example of declaration episodization: the predicate isCat(x) could be\nmapped into the fuzzy set E of episodes containing cats, where the degree of membership of e\nin E could be measured as the degree to which e contains a cat. In this case, the episode-set\nwould commonly be called the “grounding” of the predicate. Similarly, a relationship such as\na certain sense of the preposition “with” could be mapped into the set of episodes containing\nrelationships between physical entities that embody this word-sense.\nAs a very simple example of episode declaratization: an episode that an agent experienced\nwhile playing fetch with someone, could be mapped into a description of the episode including\ninformation about the kind of ball being used in the “fetch” game, the name and other properties\nof the other person participating in the “fetch” game, the length of time the game lasted, etc.\n\nB.3.2.1 Algorithmically Performing Episodic/Declarative Conversion\nOne way that these processes could occur in an intelligent system would be for episode declara tization to guide both processes. That is, the system would need some capability to abstract\ndeclarative knowledge from observed or remembered episodes. Then, given a description, the\nsystem could carry out declaration episodization via solving the “inverse problem” of episode\ndeclaratization, i.e. given a declarative object D\n1. First it could search episodic memory for episodes Ei whose (stored or on-the-fly-computed)\ndescriptions fully or approximately match D\n2. If any of the Ei is extremely accurately describable by D, then it is returned as the answer\n3. Otherwise, if some of the Ei are moderately but not extremely accurately describable by\nD, they are used as initial guesses for local search aimed at finding some episode E whose\ndescription closely matches D\n4. If no sufficiently promising Ei can be found, then a more complex cognitive process is carried\nout, for instance in an CogPrime system,\n  • Inference may be carried out to find Ei that lead to descriptions Di that are inferentially\n  found to be closely equivalent to D (in spite of this near-equivalence not being obvious\n  without inference)\n  • Evolutionary learning may be carried out to “evolve” episodes, with the fitness function\n  defined in terms of describability by D\n\nB.3.2.2 Development of Better Symbol Groundings as Natural Transformation\nAs an application of the modeling of memory types as categories, it’s interesting to think about\nthe interpretation of functor categories and natural transformations in the context of memory\ntypes, and in particular in the context of “symbol groundings” of declarative knowledge in\nepisodic knowledge.\nFirst of all, the functor category (CEp)CDec\n\n• has as objects all functors from CDec to CEp (e.g. all methods of assigning experiences\nto the sets of declarations satisfying them, which nicely map transformation paths into\ntransformation paths)\n• has as morphisms the natural transformations between these functors.\n\nThat is, suppose F and G are functors between CDec and CEp ; that is, F and G are\ntwo different ways of grounding declarative knowledge in episodic knowledge. Then, a natural\ntransformation η from F to G associates to every object X in CDec (i.e., to every declaration\nX) a morphism ηX : F(X) → G(X) in CEp (that is, ηX is a composite transformation mapping\nthe episode-set F(X) into the episode-set G(X)) so that: for every morphism f : X → Y in\nCDec we have ηY ◦ F(f) = G(f) ◦ ηX.\nAn easier way to conceptualize this may be to note that in the commutative diagram\nF(X) --F(f)--> F(Y )\n  | ηX          | ηY\n  v             v\nG(X) --G(f)--> G(Y )\nwe have a situation where\n\n• X and Y represent declarations\n• f represents a sequence of atomic transformations between declarations\n• all corners of the diagram correspond to episode-sets\n• all arrows correspond to sequences of atomic transformations between episode-sets\n• ηX and ηY represent sequences of atomic transformations between episode-sets\n\nIn other words, a natural transformation between two methods of grounding is: a mapping\nthat assigns to each declaration, a morphism on episodic memory that preserves the commuta tive diagram with respect to the two methods of grounding.\nCognitively, what this suggest is that developing better and better groundings is a mat ter of starting with one grounding and then naturally transforming it into better and better\ngroundings.\nTo make things a little clearer, we now present the above commutative diagram using a more\ntransparent, application-specific notation. Let us consider a specific example wherein:\n\n• X is represented by the predicate isT iger, Y is represented by the predicate isCat\n• f is represented by an example inference trail (i.e. transformation process) leading from\nisT iger to isCat, which we will denote isa(tiger, cat)\n• F and G are relabeled Grounding1 and Grounding2 (as these are two functors that ground\ndeclarative knowledge in episodic knowledge)\n• F(X) is relabeled T igerEpisodes1 (as it’s the set of episodes associated with isT iger under\nthe grounding Grounding1; similarly, F(Y ) is relabeled CatEpisodes1, G(X) is relabeled\nT igerEpisodes2, and G(Y ) is relabeled CatEpisodes2\n• F(f) is relabeled Grounding1(isa(tiger, cat)); and G(f) is relabeled Grounding2(isa(tiger, cat))\n• ηX and ηY become ηisT iger and ηisCat respectively\n\nWith these relabelings, the above commutative diagram looks like\nT igerEpisodes1 --Grounding1(isa(tiger,cat))--> CatEpisodes1\n  | ηisT iger                                 | ηisCat\n  v                                           v\nT igerEpisodes2 --Grounding2(isa(tiger,cat))--> CatEpisodes2\n\nOne may draw similar diagrams involving the other pairs of memory types, with similar\ninterpretations."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.3.3",
              "title": "Converting Between Episodic and Procedural Knowledge",
              "text": "Mapping between episodic and procedural knowledge may be done indirectly via the mappings\nalready described above. Of course, such mappings could also be constructed directly but for\nour present purposes, the indirect approach will suffice.\nEpisode procedurization maps an episode-set into the set of procedures whose execution is\npart of the description of the episode-set. A simple example of episode procedurization would\nbe: mapping a set of episodes involving playing “fetch” into procedures for coordinating the\nfetch game, throwing an object, catching an object, walking, and so forth.\nProcedure episodization maps a procedure into the set of episodes appearing to contain\nexecutions of the procedure. For instance, a procedure for playing fetch would map into a set\nof episodes involving playing fetch; or, a procedure for adding numbers would map into a set of\nepisodes involving addition, which might include a variety of things such as:\n\n• “textbook examples” such as: a set of two apples, and a set of three apples, merging to form\na set of five apples\n• a financial transaction at a cash register in a store, involving the purchase of several items\nand the summing of their prices into a composite price"
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.3.4",
              "title": "Converting Intentional or Attentional Knowledge into Declarative or Procedural Knowledge",
              "text": "Attentional valuations and goals are considered as functions, thus, though they may be rep resented in various “native” forms, their conversion into procedural knowledge is conceptually\nstraightforward.\nConversion to declarative knowledge may occur by way of procedural knowledge, or may be\nmore easily considered directly in some cases. For instance, the assignment of attention values to\ndeclarative knowledge items is easily represented as declarative knowledge, i.e. using statements\nof the form “Knowledge item K1 has attention value V1.”"
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.3.5",
              "title": "Converting Episodic Knowledge into Intentional or Attentional Knowledge",
              "text": "Episodes may contain implicit information about which entities should be attended in which\ncontexts, and which goals have which subgoals in which contexts. Mining this information is\nnot a simple process and requires application of significant intelligence."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "B.4",
          "title": "Metrics on Memory Spaces",
          "text": "Bringing together the ideas from the previous sections, we now explain how to use the above\nideas to define geometric structures for cognitive space, via defining two metrics on the space of\nmemory store dynamic states. Specifically, we define the dynamic state or d-state of a memory\nstore (e.g. attentional, procedural, etc.) as the series of states of that memory store (as a\nwhole) during a time-interval. Generally speaking, it is necessary to look at d-states rather\nthan instantaneous memory states because sometimes memory systems may store information\nusing dynamical patterns rather than fixed structures.\nIt’s worth noting that, according to the metrics introduced here, the above-described map pings between memory types are topologically continuous, but involve considerable geometric\ndistortion – so that e.g., two procedures that are nearby in the procedure-based mindspace,\nmay be distant in the declarative-based mindspace. This observation will lead us to the notion\nof cognitive synergy, below.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "B.4.1",
              "title": "Information Geometry on Memory Spaces",
              "text": "Our first approach involves viewing memory store d-states as probability distributions. A d state spanning time interval (p, q) may be viewed as a mapping whose input is the state of the\nworld and the other memory stores during a given interval of time (r, s), and whose output is\nthe state of the memory itself during interval (t, u). Various relations between these endpoints\nmay be utilized, achieving different definitions of the mapping e.g. p = r = t, q = s = u (in\nwhich case the d-state and its input and output are contemporaneous) or else p = r, q = s = t\n(in which case the output occurs after the simultaneous d-state and input), etc. In many cases\nthis mapping will be stochastic. If one assumes that the input is an approximation of the state\nof the world and the other memory stores, then the mapping will nearly always be stochastic.\nSo in this way, we may model the total contents of a given memory store at a certain point in\ntime as a probability distribution. And the process of learning is then modeled as one of coupled\nchanges in multiple memory stores, in such a way as to enable ongoingly improved achievement\nof system goals.\nHaving modeled memory store states as probability distributions, the problem of measuring\ndistance between memory store states is reduced to the problem of measuring distance between\nprobability distributions. But this problem has a well-known solution: the Fisher-Rao metric!\nFisher information is a statistical quantity which has a a variety of applications, ranging\nbeyond statistical data analysis, including physics [?], psychology and AI [?]. Put simply, FI\nis a formal way of measuring the amount of information that an observable random variable\nX carries about an unknown parameter θ upon which the probability of X depends. FI forms\nthe basis of the Fisher-Rao metric, which has been proved the only Riemannian metric on\nthe space of probability distributions satisfying certain natural properties regarding invariance\nwith respect to coordinate transformations. Typically θ in the FI is considered to be a real\nmultidimensional vector; however, [?] has presented a FI variant that imposes basically no\nrestrictions on the form of θ, which is what we need here.\nSuppose we have a random variable X with a probability function f(X, θ) that depends\non a parameter θ that lives in some space M that is not necessarily a dimensional space. Let\nE ⊆ R have a limit point at t ∈ R, and let γ : E → M be a path. We may then consider a\nfunction G(t) = ln f(X, γ(t)); and, letting γ(0) = θ, we may then define the generalized Fisher\ninformation as I(θ)γ = IX(θ)γ = E [ ( ∂/∂t ln f(X; γ(t)) )^2 |θ ] .\nNext, Dabak [?] has shown that the geodesic between θ and θ' is given by the exponential\nweighted curve (γ(t)) (x) = f(x,θ)^(1−t) * f(x,θ')^t / ∫ f(y,θ)^(1−t) * f(y,θ')^t dy , under the weak condition that the log-likelihood\nratios with respect to f(X, θ) and f(X, θ') are finite. It follows that if we use this form of curve,\nthen the generalized Fisher information reduces properly to the Fisher information in the case\nof dimensional spaces. Also, along this sort of curve, the sum of the Kullback-Leibler distances\nbetween θ and θ', known as the J-divergence, equals the integral of the Fisher information along\nthe geodesic connecting θ and θ'.\nFinally, another useful step for our purposes is to bring Fisher information together with\nimprecise and indefinite probabilities as discussed in [?]. For instance an indefinite probability\ntakes the form ((L, U), k, b) and represents an envelope of probability distributions, whose means\nafter k more observations lie in (L, U) with probability b. The Fisher-Rao metric between\nprobability distributions is naturally extended to yield a metric between indefinite probability\ndistributions."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.4.2",
              "title": "Algorithmic Distance on Memory Spaces",
              "text": "A conceptually quite different way to measure the distance between two d-states, on the other\nhand, is using algorithmic information theory. Assuming a fixed Universal Turing Machine M,\none may define H(S1, S2) as the length of the shortest self-delimiting program which, given\nas input d-state S1, produces as output d-state S2. A metric is then obtained via setting\nd(S1, S2) = (H(S1, S2) + H(S2, S1)/2. This tells you the computational cost of transforming S1\ninto S2.\nThere are variations of this which may also be relevant; for instance [?] defines the generalized\ncomplexity criterion KΦ(x) = mini∈N {Φ(i, τi)|L(pi)) = x}, where L is a programming language,\npi is the i’th program executable by L under an enumeration in order of nonincreasing program\nlength, τi is the execution time of the program pi, L(x) is the result of L executing pi to obtain\noutput x, and Φ is a function mapping pairs of integers into positive reals, representing the trade off between program length and memory. Via modulating Φ, one may cause this complexity\ncriterion to weight only program length (like standard algorithmic information theory), only\nruntime (like the speed prior), or to balance the two against each other in various ways.\nSuppose one uses the generalized complexity criterion, but looking only at programs pi that\nare given S1 as input. Then KΦ(S2), relative to this list of programs, yields an asymmetric\ndistance HΦ(S1, S2), which may be symmetrized as above to yield dΦ(S1, S2). This gives a more\nflexible measure of how hard it is to get to one of (S1, S2) from the other one, in terms of both\nmemory and processing time.\nOne may discuss geodesics in this sort of algorithmic metric space, just as in Fisher-Rao\nspace. A geodesic in algorithmic metric space has the property that, between any two points on\nthe path, the integral of the algorithmic complexity incurred while following the path is less than\nor equal to that which would be incurred by following any other path between those two points.\nThe algorithmic metric is not equivalent to the Fisher-Rao metric, a fact that is consistent\nwith Cencov’s Theorem because the algorithmic metric is not Riemannian (i.e. it is not locally\napproximated by a metric defined via any inner product)."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "B.5",
          "title": "Three Hypotheses About the Geometry of Mind",
          "text": "Now we present three hypotheses regarding generally intelligent systems, using the conceptual\nand mathematical machinery we have built.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "B.5.1",
              "title": "Hypothesis 1: Syntax-Semantics Correlation",
              "text": "The informational and algorithmic metrics, as defined above, are not equivalent nor neces sarily closely related; however, we hypothesize that on the whole, systems will operate more\nintelligently if the two metrics are well correlated, implying that geodesics in one space should\ngenerally be relatively short paths (even if not geodesics) in another.\nThis hypothesis is a more general version of the “syntax-semantics correlation\" property\nstudied in [?] in the context of automated program learning. There, it is shown empirically\nthat program learning is more effective when programs with similar syntax also have similar\nbehaviors. Here, we are suggesting that an intelligent system will be more effective if memory\nstores with similar structure and contents lead to similar effects (both externally to the agent,\nand on other memory systems). Hopefully the basic reason for this is clear. If syntax-semantics\ncorrelation holds, then learning based on the internal properties of the memory store, can help\nfigure out things about the external effects of the memory store. On the other hand, if it doesn’t\nhold, then it becomes quite difficult to figure out how to adjust the internals of the memory to\nachieve desired effects.\nThe assumption of syntax-semantics correlation has huge implications for the design of learn ing algorithms associated with memory stores. All of CogPrime’s learning algorithms are built\non this assumption. For, example CogPrime’s MOSES procedure learning component [?] as sumes syntax-semantics correlation for individual programs, from which it follows that the\nproperty holds also on the level of the whole declarative memory store. And CogPrime’s PLN\nprobabilistic inference component [?] uses an inference control mechanism that seeks to guide\na new inference via analogy to prior similar inferences, thus embodying an assumption that\nstructurally similar inferences will lead to similar behaviors (conclusions)."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.5.2",
              "title": "Hypothesis 2: Cognitive Geometrodynamics",
              "text": "In general relativity theory there is the notion of “geometrodynamics,\" referring to the feedback\nby which matter curves space, and then space determines the movement of matter (via the\nrule that matter moves along geodesics in curved spacetime) [?]. One may wonder whether an\nanalogous feedback exists in cognitive geometry. We hypothesize that the answer is yes, to a\nlimited extent. On the one hand, according to the above formalism, the curvature of mindspace\nis induced by the knowledge in the mind. On the other hand, one may view cognitive activity\nas approximately following geodesics in mindspace.\nLet’s say an intelligent system has the goal of producing knowledge meeting certain charac teristics (and note that the desired achievement of a practical system objective may be framed\nin this way, as seeking the true knowledge that the objective has been achieved). The goal\nthen corresponds to some set of d-states for some of the mind’s memory stores. A simplified but\nmeaningful view of cognitive dynamics is, then, that the system seeks the shortest path from the\ncurrent d-state to the region in d-state space comprising goal d-states. For instance, considering\nthe algorithmic metric, this reduces to the statement that at each time point, the system seeks\nto move itself along a path toward its goal, in a manner that requires the minimum computa tional cost – i.e. along some algorithmic geodesic. And if there is syntax-semantics correlation,\nthen this movement is also approximately along a Fisher-Rao geodesic.\nAnd as the system progresses from its current state toward its goal-state, it is creating new\nmemories – which then curve mindspace, possibly changing it substantially from the shape it\nhad before the system started moving toward its goal. This is a feedback conceptually analogous\nto, though in detail very different from, general-relativistic geometrodynamics.\nThere is some subtlety here related to fuzziness. A system’s goals may be achievable to\nvarious degrees, so that the goal region may be better modeled as a fuzzy set of lists of regions.\nAlso, the system’s current state may be better viewed as a fuzzy set than as a crisp set. This\nis the case with CogPrime, where uncertain knowledge is labeled with confidence values along\nwith probabilities; in this case the confidence of a logical statement may be viewed as the fuzzy\ndegree with which it belongs to the system’s current state. But this doesn’t change the overall\ncognitive-geometrodynamic picture, it just adds a new criterion; one may say that the cognition\nseeks a geodesic from a high-degree portion of the current-state region to a high-degree portion\nof the goal region."
            },
            {
              "type": "section",
              "level": 2,
              "id": "B.5.3",
              "title": "Hypothesis 3: Cognitive Synergy",
              "text": "Cognitive synergy, discussed extensively above, is a conceptual explanation of what makes it\npossible for certain sorts of integrative, multi-component cognitive systems to achieve powerful\ngeneral intelligence [?]. The notion pertains to systems that possess knowledge creation (i.e.\npattern recognition / formation / learning) mechanisms corresponding to each multiple memory\ntypes. For such a system to display cognitive synergy, each of these cognitive processes must\nhave the capability to recognize when it lacks the information to perform effectively on its own;\nand in this case, to dynamically and interactively draw information from knowledge creation\nmechanisms dealing with other types of knowledge. Further, this cross-mechanism interaction\nmust have the result of enabling the knowledge creation mechanisms to perform much more\neffectively in combination than they would if operated non-interactively.\nHow does cognitive synergy manifest itself in the geometric perspective we’ve sketched here?\nPerhaps the most straightforward way to explore it is to construct a composite metric, merging\ntogether the individual metrics associated with specific memory spaces.\nIn general, given N metrics dk(x, z), k = 1 . . . N defined on the same finite space M, we can\ndefine the \"min-combination\" metric\nd_d1,...,dN (x, z) = min_{y0=x,yn+1=z,yi∈M,r(i)∈{1,...,N},i∈{1,...,n},n∈Z} Σ_{i=0 to n} d_r(i)(yi, yi+1)\nThis metric is conceptually similar to (and mathematically generalizes) min-cost metrics like\nthe Levenshtein distance used to compare strings [?]. To see that it obeys the metric axioms is\nstraightforward; the triangle inequality follows similarly to the case of the Levenshtein metric.\nIn the case where M is infinite, one replaces min with inf (the infimum) and things proceed\nsimilarly. The min-combination distance from x to z tells you the length of the shortest path\nfrom x to z, using the understanding that for each portion of the path, one can choose any one of\nthe metrics being combined. Here we are concerned with cases such as dsyn = ddP roc,dDec,dEp,dAtt .\nWe can now articulate a geometric version of the principle of cognitive synergy. Basically:\ncognitive synergy occurs when the synergetic metric yields significantly shorter distances be tween relevant states and goals than any of the memory-type-specific metrics. Formally, one\nmay say that:\n\nDefinition B.1. An intelligent agent A (modeled by SRAM) displays cognitive synergy to\nthe extent\nsyn(A) ≡ ∫ (dsynergetic(x, z) − min (dP roc(x, z), dDec(x, z), dEp(x, z), dAtt(x, z))) dμ(x)dμ(z)\nwhere μ measures the relevance of a state to the system’s goal-achieving activity."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "B.6",
          "title": "Next Steps in Refining These Ideas",
          "text": "These ideas may be developed in both practical and theoretical directions. On the practical\nside, we have already had an interesting preliminary success, described briefly in ?? where we\nshow that (in some small examples at any rate) replacing CogPrime’s traditional algorithm\nfor attentional learning with an explicitly information-geometric algorithm leads to dramatic\nincreases in the intelligence of the attentional component. This work needs to be validated via\nimplementation of a scalable version of the information geometry algorithm in question, and\nempirical work also needs to be done to validate the (qualitatively fairly clear) syntax-semantics\ncorrelation in this case. But tentatively, this seems to be an early example of improvement to an\nAGI system resulting from modifying its design to more explicitly exploit the mind-geometric\nprinciples outlined here.\nPotentially, each of the inter-cognitive-process synergies implicit in the CogPrime design may\nbe formalized in the geometric terms outlined here, and doing so is part of our research program\ngoing forward.\nMore generally, on the theoretical side, a mass of open questions looms. The geometry of\nspaces defined by the min-combination metric is not yet well-understood, and neither is the\nFisher-Rao metric over nondimensional spaces or the algorithmic metric (especially in the case of\ngeneralized complexity criteria). Also the interpretation of various classes of learning algorithms\nin terms of cognitive geometrodynamics is a subtle matter, and may prove especially fruitful\nfor algorithms already defined in probabilistic or information-theoretic terms."
        },
        {
          "type": "section",
          "level": 1,
          "id": "B.7",
          "title": "Returning to Our Basic Claims About CogPrime",
          "text": "Finally, we return to the list of basic claims about CogPrime given at the end of Chapter ??,\nand review their connection with the ideas in this appendix. Not all of the claims there are\ndirectly related to the ideas given here, but many of them are; to wit:\n6. It is most effective to teach an AGI system aimed at roughly human-like general intelli gence via a mix of spontaneous learning and explicit instruction, and to instruct it via a\ncombination of imitation, reinforcement and correction, and a combination of linguistic and\nnonlinguistic instruction\n  • Mindspace interpretation. Different sorts of learning are primarily focused on differ ent types of memory, and hence on different mindspaces. The effectiveness of learning\n  focused on a particular memory type depends on multiple factors including: the general\n  competence of the agent’s learning process corresponding to that memory store, the\n  amount of knowledge already built up in that memory store, and the degree of syntax semantics correlation corresponding to that memory store. In terms of geometrodynam ics, learning in a manner focused on a certain memory type, has significant impact in\n  terms of reshaping the mindspace implied by that memory store.\n7. One effective approach to teaching an AGI system human language is to supply it with\nsome in-built linguistic facility, in the form of rule-based and statistical-linguistics-based\nNLP systems, and then allow it to improve and revise this facility based on experience\n  • Mindspace interpretation. Language learning purely in declarative space (formal\n  grammar rules), or purely in attentional space (statistical correlations between linguistic\n  inputs), or purely in episodic or procedural space (experiential learning), will not be\n  nearly so effective as language learning which spans multiple memory spaces. Language\n  learning (like many other kinds of humanly natural learning) is better modeled as\n  cognitive-synergetic cognitive geometrodynamics, rather than as single-memory-type\n  cognitive geometrodynamics.\n8. An AGI system with adequate mechanisms for handling the key types of knowledge men tioned above, and the capability to explicitly recognize large-scale pattern in itself, should,\n  upon sustained interaction with an appropriate environment in pursuit of ap propriate goals, emerge a variety of complex structures in its internal knowledge network,\n  including (but not limited to)\n  • a hierarchical network, representing both a spatiotemporal hierarchy and an approxi mate \"default inheritance\" hierarchy, cross-linked\n  • a heterarchical network of associativity, roughly aligned with the hierarchical network\n  • a self network which is an approximate micro image of the whole network\n  • inter-reflecting networks modeling self and others, reflecting a \"mirrorhouse\" design\n  pattern\n  What does this mean geometrically?\n  • Mindspace interpretation. The self network and mirrorhouse networks imply a\n  roughly fractal structure for mindspace, especially when considered across multiple\n  memory types (since the self network spans multiple memory types). Peripherally, it’s\n  interesting that the physical universe has a very roughly fractal structure too, e.g. with\n  solar systems within galaxies within galactic clusters; so doing geometrodynamics in\n  roughly fractal curved spaces is not a new idea.\n9. Given the strengths and weaknesses of current and near-future digital computers,\n  a. A (loosely) neural-symbolic network is a good representation for directly storing many\n  kinds of memory, and interfacing between those that it doesn’t store directly\n    • Mindspace interpretation. The \"neural\" aspect stores associative knowledge,\n    and the \"symbolic\" aspect stores declarative knowledge; and the superposition of\n    the two in a single network makes it convenient to implement cognitive processes\n    embodying cognitive synergy between the two types of knowledge.\n  b. Uncertain logic is a good way to handle declarative knowledge\n    • Mindspace interpretation. There are many senses in which uncertain logic is\n    \"good\" for AGI; but the core points are that:\n    – it makes representation of real-world relationships relatively compact\n    – it makes inference chains of real-world utility relatively short\n    – it gives high syntax-semantics correlation for logical relationships involving un certainty (because it lends itself to syntactic distance measures that treat un certainty naturally, gauging distance between two logical relationships based\n    partly on the distances between the corresponding uncertainty values; e.g. the\n    PLN metric defined in terms of SimilarityLink truth values)\n    – because the statistical formulas for truth value calculation are related to statis tical formulas for association-finding, it makes synergy between declarative and\n    associative knowledge relatively straightforward.\n  c. Programs are a good way to represent procedures (both cognitive and physical-action,\n  but perhaps not including low-level motor-control procedures)\n  d. Evolutionary program learning is a good way to handle difficult program learning prob lems\n    • Probabilistic learning on normalized programs is one effective approach to evolu tionary program learning\n    • MOSES is one good realization\n      – Mindspace interpretation. Program normalization creates relatively high\n      syntax-semantics correlation in procedural knowledge (program) space, and\n      MOSES is an algorithm that systematically exploits this knowledge.\n  e. Multistart hill-climbing on normalized programs, with a strong Occam prior, is a good\n  way to handle relatively straightforward program learning problems\n  f. Activation spreading is a reasonable way to handle attentional knowledge (though other\n  approaches, with greater overhead cost, may provide better accuracy and may be ap propriate in some situations)\n    • Artificial economics is an effective approach to activation spreading in the context\n    of neural-symbolic network.\n    • ECAN is one good realization, with Hebbian learning as one route of learning asso ciative relationships, and more sophisticated methods such as information-geometric\n    ones potentially also playing a role\n    • A good trade-off between comprehensiveness and efficiency is to focus on two kinds\n    of attention: processor attention (represented in CogPrime by ShortTermImpor tance) and memory attention (represented in CogPrime by LongTermImportance)\n  The mindspace interpretation includes the observations that\n    • Artificial economics provides more convenient conversion between attentional and\n    declarative knowledge, compared to more biologically realistic neural net type mod els of attentional knowledge\n    • In one approach to structuring the attentional mindspace, historical knowledge re garding what was worth attending (i.e. high-strength HebbianLinks between Atoms\n    that were in the AttentionalFocus at the same time, and linkages between these\n    maps and system goals) serves to shape the mindspace, and learning the other Heb bianLinks in the network may be viewed as an attempt to follow short paths through\n    attentional mindspace (as explicitly shown in Chapter ??).\n  g. Simulation is a good way to handle episodic knowledge (remembered and imagined)\n    • Running an internal \"world simulation engine\" is an effective way to handle simu lation\n  What’s the mindspace interpretation? For example,\n    • The world simulation engine takes a certain set of cues and scattered memories re lated to an episode, and creatively fills in the gaps to create a full-fledged simulation\n    of the episode. Syntax-semantics correlation means that stating \"sets of cues and\n    scattered memories\" A and B are similar, is approximately the same as stating that\n    the corresponding full-fledged simulations are similar.\n    • Many dreams seem to be examples of following paths through episode space, from\n    one episode to another semantically related one, etc. But these paths are often aim less, though generally following semantic similarity. Trying to think of or remember\n    an episode matching certain constraints, is a process where following short paths\n    through episodic mindspace is relevant.\n  h. One effective way to handle goals is to represent them declaratively, and allocate atten tion among them economically\n    • CogPrime’s PLN/ECAN based framework for handling intentional knowledge is one\n    good realization\n  One aspect of the mindspace interpretation is that using PLN and ECAN together\n  to represent goals, aids with the cognitive synergy between declarative, associative and\n  intentional space. Achieving a goal is then (among other things) about finding short\n  paths to the goal thru declarations, associations and actions.\n10. It is important for an intelligent system to have some way of recognizing large-scale patterns\nin itself, and then embodying these patterns as new, localized knowledge items in its memory\n  • Given the use of a neural-symbolic network for knowledge representation, a graph mining based \"map formation\" heuristic is one good way to do this\n  Key aspects of the mindspace interpretation are that:\n  • via map formation, associative (map) and declarative, procedural or episodic (localized)\n  knowledge are correlated, promoting cognitive synergy\n  • approximate and emergent inference on concept maps, occurring via associational pro cesses, roughly mirrors portions of PLN reasoning on declarative concepts and rela tionships. This aids greatly with cognitive synergy, and in fact one can draw \"natural\n  transformations\" (in the language of category theory) between map inference and lo calized, declarative concept inference.\n11. Occam’s Razor: Intelligence is closely tied to the creation of procedures that achieve goals\nin environments in the simplest possible way.\n  • Each of an AGI system’s cognitive algorithms should embody a \"simplicity bias\" in\n  some explicit or implicit form\n  Obviously, one aspect of the mindspace interpretation of this principle is simply the\n  geometrodynamic idea of following the shortest path through mindspace, toward the ap-pointed set of goal states. Also, this principle is built into the definition of semantic space\n  used in the mindspace framework developed above, since computational simplicity is used\n  to define the semantic metric between memory items.\n\nWhile the abstract \"mind geometry\" theory presented in this appendix doesn’t (yet) provide\na way of deriving the CogPrime design from first principles, it does provide a useful general\nvocabulary for discussing the various memory types and cognitive processes in CogPrime in\na unified way. And it also has some power to suggest novel algorithms to operated within\ncognitive processes, as in the case of our work on information geometry and ECAN. Whether\nmind geometry will prove a really useful ingredient in CogPrime theory or AGI theory more\nbroadly, remains to be determined; but we are cautiously optimistic and intend to pursue further\nin this direction."
        }
      ]
    },
    {
      "id": "C",
      "title": "Emergent Reflexive Mental Structures",
      "content": [
        {
          "type": "section",
          "level": 1,
          "id": "C.1",
          "title": "Introduction",
          "text": "This appendix deals with some complex emergent structures we suspect may emerge in advanced\nCogPrime and other AGI systems. The ideas presented here are flatly conjectural, and we stress\nthat the CogPrime design is not dependent thereupon. The more engineering-oriented reader\nmay skip them without any near-term loss. However, we do believe that this sort of rigorous\nlateral thinking is an important part of any enterprise as ambitious as building a human-level\nAGI.\nWe have stated that the crux of an AGI system really lies on the emergent level – on the\nstructures and dynamics that arise in the system as a result of its own self-organization and\nits coupling with other minds and the external world. We have talked a bit about some of\nthese emergent patterns – e.g. maps and various sorts of networks – but by and large they have\nstayed in the background. In this appendix we will indulge in a bit of speculative thinking about\nsome of the high-level emergent patterns that we believe may emerge in AGI systems once they\nbegin to move toward human-level intelligence, and specifically once they acquire a reasonably\nsophisticated ability to model themselves and other minds. 1 These patterns go beyond the\nrelatively well-accepted network structures reviewed in chapter ??, and constitute an edgier,\nmore ambitious hypothesis regarding the emergent network structures of general intelligence.\nMore specifically, the thesis of this appendix is that there are certain abstract algebraic\nstructures that typify the self-structure of human beings and any other intelligent systems re lying on empathy for social intelligence. These structures may be modeled using various sorts\nof mathematics, including hypersets and also algebraic structures called quaternions and octo nions (which also play a critical role in modern theoretical physics [?]). And, assuming mature,\nreasonably intelligent AGI systems are created, it will be possible to empirically determine\nwhether the mathematical structures posited here do or do not emerge in them.\n\n1 A note from Ben Goertzel. In connection with the material in this appendix, I would like to warmly acknowledge\nLouis Kauffman for an act of kindness that occurred back in 1986, when I was a 19 year old PhD student,\nwhen he mailed me a copy of his manuscript Sign and Space, which contained so many wonderful ideas and\ndrawings related to the themes considered here. Lou’s manuscript wasn’t my first introduction to the meme of\nconsciousness and self-reference – I got into these ideas first via reading Douglas Hofstadter at age 13 in 1979,\nand then later via reading G. Spencer-Brown. But my brief written correspondence with Lou (this was before\nemail was common even in universities) and his lovely hand-written and -drawn manuscript solidified my passion\nfor these sorts of ideas, and increased my confidence that they are not only fascinating but deeply meaningful."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.2",
          "title": "Hypersets and Patterns",
          "text": "The first set of hypotheses we will pursue in this appendix is that the abstract structures\ncorresponding to free will, reflective consciousness and phenomenal self are effectively modeled\nusing the mathematics of hypersets.\nWhat are these things called hypersets, which we posit as cognitive models?\nIn the standard axiomatizations of set theory, such as Zermelo-Frankel set theory [?], there\nis an axiom called the Axiom of Foundation, which implies that no set can contain itself as a\nmember. That is, it implies that all sets are \"well founded\" – they are built up from other sets,\nwhich in turn are built up from other sets, etc., ultimately being built up from the empty set\nor from atomic elements. The hierarchy via which sets are built from other sets may be infinite\n(according to the usual Axiom of Infinity), but it goes in only one direction – if set A is built\nfrom set B (or from some other set built from set B), then set B can’t be built from set A (or\nfrom any other set built from set A).\nHowever, since very shortly after the Axiom of Foundation was formulated, there have been\nvarious alternative axiomatizations which allow \"non-well-founded\" sets (aka hypersets), i.e. sets\nthat can contain themselves as members, or have more complex circular membership structures.\nHyperset theory is generally formulated as an extension of classical set theory rather than a\nreplacement – i.e., the well-founded sets within a hyperset domain conform to classical set\ntheory. In recent decades the theory of non-well-founded sets has been applied in computer\nscience (e.g. process algebra [?]), linguistics and natural language semantics (situation theory\n[?]), philosophy (work on the Liar Paradox [?]), and other areas.\nFor instance, in hyperset theory you can have\nA = {A}\nA = {B, {A}}\nand so forth. Using hypersets you can have functions that take themselves as arguments, and\nmany other interesting phenomena that aren’t permitted by the standard axioms of set theory.\nThe main work of this appendix is to suggest specific models of free will, reflective consciousness\nand phenomenal self in terms of hyperset mathematics.\nThe reason the Axiom of Foundation was originally introduced was to avoid paradoxes like\nthe Russell Set (the set of all sets that do not contain themselves). None of these variant set\ntheories allow all possible circular membership structures; but they allow restricted sets of such,\nsculpted to avoid problems like the Russell Paradox.\nOne currently popular form of hyperset theory is obtained by replacing the Axiom of Foun dation with the Anti-Foundation Axiom (AFA) which, roughly speaking, permits circular mem bership structures that map onto graphs in a certain way. All the hypersets discussed here are\neasily observed to be allowable under the AFA (according to the the Solution Lemma stated in\n[?]).\nSpecifically, the AFA uses the notion of an accessible pointed graph – a directed graph with\na distinguished element (the \"root\") such that for any node in the graph there is at least one\npath in the directed graph from the root to that node. The AFA states that every accessible\npointed graph corresponds to a unique set. For example, the graph consisting of a single vertex\nwith a loop corresponds to a set which contains only itself as element,\nWhile the specific ideas presented here are novel, the idea of analyzing consciousness and\nrelated structures in terms of infinite recursions and non-foundational structures has occurred\nbefore, for instance in the works of Douglas Hofstadter [?], G. Spencer-Brown [?], Louis Kauff mann [?] and Francisco Varela [?]. None of these works uses hypersets in particular; but a more\nimportant difference is that none of them attempts to deal with particular psychological phe nomena in terms of correlation, causation, pattern theory or similar concepts; they essentially\nstop at the point of noting the presence of a formalizable pattern of infinite recursion in reflec tive consciousness. [?] does venture into practical psychology via porting some of R.D. Laing’s\npsychosocial \"knots\" [?] into a formal non-foundational language; but this is a very specialized\nexercise that doesn’t involve modeling general psychological structures or processes. Situation\nsemantics [?] does analyze various commonsense concepts and relationships using hypersets;\nhowever, it doesn’t address issues of subjective experience explicitly, and doesn’t present formal\ntreatments of the phenomena considered here.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "C.2.1",
              "title": "Hypersets as Patterns in Physical or Computational Systems",
              "text": "Hypersets are large infinite sets – they are certainly not computable – and so one might won der if a hyperset model of consciousness supports Penrose [?] and Hameroff’s [?] notion of\nconsciousness as involving as-yet unknown physical dynamics involving uncomputable mathe matics. However, this is not our perspective.\nIn the following we will present a number of particular hypersets and discuss their presence as\npatterns in intelligent systems. But this does not imply that we are positing intelligent systems\nto fundamentally be hypersets, in the sense that, for instance, classical physics posits intelligent\nsystems to be matter in 3 + 1 dimensional space. Rather, we are positing that it is possible for\nhypersets to serve as patterns in physical systems, where the latter may be described in terms\nof classical or modern physics, or in terms of computation.\nHow is this possible? If a hyperset can produce a somewhat accurate model of a physical\nsystem, and is judged simpler than a detailed description of the physical system, then it may\nbe a pattern in that system according to the definition of pattern given above.\nRecall the definition of pattern given in chapter ??:\n\nDefinition 1 Given a metric space (M, d), and two functions c : M → [0, ∞] (the “simplicity\nmeasure”) and F : M → M (the “production relationship”), we say that P ∈ M is a pattern\nin X ∈ M to the degree\nι^P_X = ( 1 − d(F(P), X) / c(X) ) * ( c(X) − c(P) / c(X) )^+\nThis degree is called the pattern intensity of P in X.\n\nTo use this definition to bridge the gap between hypersets and ordinary computer programs\nand physical systems, we may define the metric space M to contain both hypersets and computer\nprograms, and also tuples whose elements may be freely drawn from either of these classes.\nDefine the partial order < so that if X is an entry in a tuple T, then X < T.\nDistance between two programs may be defined using the algorithmic information metric\ndI (A, B) = I(A|B) + I(B|A)\nwhere I(A|B) is the length of the shortest self-delimiting program for computing A given B [?].\nDistance between two hypersets X and Y may be defined as\ndH(X, Y ) = dI (g(A), g(B))\nwhere g(A) is the graph (A’s apg, in AFA lingo) picturing A’s membership relationship. If A is\na program and X is a hyperset, we may set d(A, X) = ∞.\nNext, the production relation F may be defined to act on a (hyperset,program) pair P =\n(X, A) via feeding the graph representing X (in some standard encoding) to A as an input.\nAccording to this production relation, P may be a pattern in the bit string B = A(g(X)); and\nsince X < P, the hyperset X may be a subpattern in the bit string B.\nIt follows from the above that a hyperset can be part of the mind of a finite system described\nby a bit string, a computer program, or some other finite representation. But what sense does\nthis make conceptually? Suppose that a finite system S contains entities of the form\nC\nG(C)\nG(G(C))\nG(G(G(C)))\n...\nThen it may be effective to compute S using a (hyperset, program) pair containing the hyperset\nX = G(X)\nand a program that calculates the first k iterates of the hyperset. If so, then the hyperset\n{X = G(X)} may be a subpattern in S. We will see some concrete examples of this in the\nfollowing.\nWhether one thing is a pattern in another depends not only on production but also on\nrelative simplicity. So, if a system is studied by an observer who is able to judge some hypersets\nas simpler than some computational entities, then there is the possibility for hypersets to be\nsubpatterns in computational entities, according to that observer. For such an observer, there\nis the possibility to model mental phenomena like will, self and reflective consciousness as\nhypersets, consistently with the conceptualization of mind as pattern."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.3",
          "title": "A Hyperset Model of Reflective Consciousness",
          "text": "Now we proceed to use hypersets to model the aspect of mind we call \"reflective consciousness.\"\nWhatever your view of the ultimate nature of consciousness, you probably agree that different\nentities in the universe manifest different kinds of consciousness or “awareness.\" Worms are\naware in a different way than rocks; and dogs, pigs, pigeons and people are aware in a different\nway from worms. In [?] it is argued that hypersets can be used to model the sense in which\nthe latter beasts are conscious whereas worms are not – i.e. what might be called \"reflective\nconsciousness.\"\nWe begin with the old cliché that\nConsciousness is consciousness of consciousness\nNote that this is nicely approximated by the series\nA\nConsciousness of A\nConsciousness of consciousness of A\n...\nThis is conceptually elegant, but doesn’t really serve as a definition or precise characterization\nof consciousness. Even if one replaces it with\nReflective consciousness is reflective consciousness of reflective consciousness\nit still isn’t really adequate as a model of most reflectively conscious experience – although it\ndoes seem to capture something meaningful.\nIn hyperset theory, one can write an equation\nf = f(f)\nwith complete mathematical consistency. You feed f as input: f ... and you receive as output: f.\nBut while this sort of anti-foundational recursion may be closely associated with consciousness,\nthis simple equation itself doesn’t tell you much about consciousness. We don’t really want to\nsay\nReflectiveConsciousness = ReflectiveConsciousness(ReflectiveConsciousness)\nIt’s more useful to say:\nReflective consciousness is a hyperset, and reflective consciousness is contained in its\nmembership scope\nHere by the \"membership scope\" of a hyperset S, what we mean is the members of S, plus\nthe members of the members of S, etc. However, this is no longer a definition of reflective\nconsciousness, merely a characterization. What it says is that reflective consciousness must be\ndefined anti-foundationally as some sort of construct via which reflective consciousness builds\nreflective consciousness from reflective consciousness – but it doesn’t specify exactly how.\nPutting this notion together with the discussion from Chapter ?? on patterns, correlations\nand experience, we arrive at the following working definition of reflective consciousness. Assume\nthe existence of some formal language with enough power to represent nested logical predicates,\ne.g. standard predicate calculus will suffice; let us refer to expressions in this language as\n\"declarative content.\" Then we may say\n\nDefinition C.1. \"S is reflectively conscious of X\" is defined as:\nThe declarative content that {\"S is reflectively conscious of X\" correlates with \"X is a pattern\nin S\"}\n\nFor example: Being reflectively conscious of a tree means having in one’s mind declarative\nknowledge of the form that one’s reflective consciousness of that tree is correlated with that tree\nbeing a pattern in one’s overall mind-state. Figure C.1 graphically depicts the above definition.\n\nFig. C.1: Graphical depiction of \"Ben is reflectively conscious of his inner image of a money\ntree\"\n\nNote that this declarative knowledge doesn’t have to be explicitly represented in the ex periencer’s mind as a well-formalized language – just as pigeons, for instance, can carry out\ndeductive reasoning without having a formalization of the rules of Boolean or probabilistic\nlogic in their brains. All that is required is that the conscious mind has an internal \"informal,\npossibly implicit\" language capable of expressing and manipulating simple hypersets. Boolean\nlogic is still a subpattern in the pigeon’s brain even though the pigeon never explicitly applies\na Boolean logic rule, and similarly the hypersets of reflective consciousness may be subpatterns\nin the pigeon’s brain in spite of its inability to explicitly represent the underlying mathematics.\nTurning next to the question of how these hyperset constructs may emerge from finite sys tems, Figures C.2, C.3 and C.4 show the first few iterates of a series of structures that would\nnaturally be computed by a pattern containing as a subpattern Ben’s reflective consciousness\nof his inner image of a money tree. The presence of a number of iterates in this sort of series,\nas patterns or subpatterns in Ben, will lead to the presence of the hyperset of \"Ben’s reflective\nconsciousness of his inner image of a money tree\" as a subpattern in his mind.\n\nFig. C.2: First iterate of a series that converges to Ben’s reflective consciousness of his inner\nimage of a money tree\n\nFig. C.3: Second iterate of a series that converges to Ben’s reflective consciousness of his inner\nimage of a money tree\n\nFig. C.4: Third iterate of a series that converges to Ben’s reflective consciousness of his inner\nimage of a money tree"
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.4",
          "title": "A Hyperset Model of Will",
          "text": "The same approach can be used to define the notion of \"will,\" by which is meant the sort of\nwilling process that we carry out in our minds when we subjectively feel like we are deciding to\nmake one choice rather than another [?].\nIn brief:\n\nDefinition C.2. \"S wills X\" is defined as:\nThe declarative content that {\"S wills X\" causally implies \"S does X\"}\n\nFigure C.5 graphically depicts the above definition.\nTo fully explicate this is slightly more complicated than in the case of reflective consciousness,\ndue to the need to unravel what’s meant by \"causal implication.\" For sake of the present\ndiscussion we will adopt the view of causation presented in [?], according to which causal\nimplication may be defined as: Predictive implication combined with the existence of a plausible\ncausal mechanism.\nMore precisely, if A and B are two classes of events, then A \"predictively implies B\" if it’s\nprobabilistically true that in a situation where A occurs, B often occurs afterwards. (Of course,\nthis is dependent on a model of what is a \"situation\", which is assumed to be part of the mind\nassessing the predictive implication.)\nAnd, a \"plausible causal mechanism\" associated with the assertion \"A predictively implies\nB\" means that, if one removed from one’s knowledge base all specific instances of situations\nproviding direct evidence for \"A predictively implies B\", then the inferred evidence for \"A\npredictively implies B\" would still be reasonably strong. (In PLN lingo, this means there is\nstrong intensional evidence for the predictive implication, along with extensional evidence.)\nIf X and Y are particular events, then the probability of \"X causally implies Y\" may be\nassessed by probabilistic inference based on the classes (A, B, etc.) of events that X and Y\nbelong to.\n\nFig. C.5: Graphical depiction of \"Ben wills himself to kick the soccer ball\"",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "C.4.1",
              "title": "In What Sense Is Will Free?",
              "text": "Briefly, what does this say about the philosophical issues traditionally associated with the notion\nof \"free will\"?\nIt doesn’t suggest any validity for the idea that will somehow adds a magical ingredient\nbeyond the familiar ingredients of \"rules\" plus \"randomness.\" In that sense, it’s not a very\nradical approach. It fits in with the modern understanding that free will is to a certain extent\nan \"illusion\", and that some sort of \"natural autonomy\" [?] is a more realistic notion.\nHowever, it also suggests that \"illusion\" is not quite the right word. An act of will may have\ncausal implication, according to the psychological definition of the latter, without this action of\nwill violating the notion of deterministic/stochastic equations of the universe. The key point is\nthat causality is itself a psychological notion (where within \"psychological\" I include cultural as\nwell as individual psychology). Causality is not a physical notion; there is no branch of science\nthat contains the notion of causation within its formal language. In the internal language\nof mind, acts of will have causal impacts – and this is consistent with the hypothesis that\nmental actions may potentially be ultimately determined via determistic/stochastic lower-level\ndynamics. Acts of will exist on a different level of description than these lower-level dynamics.\nThe lower-level dynamics are part of a theory that compactly explains the behavior of cells,\nmolecules and particles; and some aspects of complex higher-level systems like brains, bodies\nand societies. Will is part of a theory that compactly explains the decisions of a mind to itself."
            },
            {
              "type": "section",
              "level": 2,
              "id": "C.4.2",
              "title": "Connecting Will and Consciousness",
              "text": "Connecting back to reflective consciousness, we may say that:\nIn the domain of reflective conscious experiences, acts of will are experienced as causal.\nThis may seem a perfectly obvious assertion. What’s nice is that, in the present perspective, it\nseems to fall out of a precise, abstract characterization of consciousness and will."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.5",
          "title": "A Hyperset Model of Self",
          "text": "Finally, we posit a similar characterization for the cognitive structure called the \"phenomenal\nself\" – i.e. the psychosocial model that an organism builds of itself, to guide its interaction with\nthe world and also its own internal choices. For a masterfully thorough treatment of this entity,\nsee Thomas Metzinger’s book Being No One [?]).\nOne way to conceptualize self is in terms of the various forms of memory comprising a\nhumanlike intelligence [?], which include procedural, semantic and episodic memory.\nIn terms of procedural memory, an organism’s phenomenal self may be viewed as a predictive\nmodel of the system’s behavior. It need not be a wholly accurate predictive model; indeed many\nhuman selves are wildly inaccurate, and aesthetically speaking, this can be part of their charm.\nBut it is a predictive model that the system uses to predict its behavior.\nIn terms of declarative memory, a phenomenal self is used for explanation – it is an ex planatory model of the organism’s behaviors. It allows the organism to carry out (more or less\nuncertain) inferences about what it has done and is likely to do.\nIn terms of episodic memory, a phenomenal self is used as the protagonist of the organism’s\nremembered and constructed narratives. It’s a fictional character, \"based on a true story,\"\nsimplified and sculpted to allow the organism to tell itself and others (more or less) sensible\nstories about what it does.\nThe simplest version of a hyperset model of self would be:\n\nDefinition C.3. \"X is part of S’s phenomenal self\" is defined as the declarative content that\n{\"X is a part of S’s phenomenal self\" correlates with \"X is a persistent pattern in S over time\"}\n\nFig. C.6: Graphical depiction of \"Ben’s representation-of/adaptation to his parrot is a part of\nhis phenomenal self\" (Image of parrot is from a painting by Scheherazade Goertzel)\n\nFigure C.6 graphically depicts the above definition.\nA subtler version of the definition would take into account the multiplicity of memory types:\n\nDefinition C.4. \"X is part of S’s phenomenal self\" is defined as the declarative content that\n{\"X is a part of S’s phenomenal self\" correlates with \"X is a persistent pattern in S’s declarative,\nprocedural and episodic memory over time\"}\n\nOne thing that’s nice about this definition (in both versions) is the relationship that it applies\nbetween self and reflective consciousness. In a formula:\nSelf is to long-term memory as reflective consciousness is to short-term memory\nAccording to these definitions:\n\n• A mind’s self is nothing more or less than its reflective consciousness of its persistent being.\n• A mind’s reflective consciousness is nothing more or less than the self of its short-term\nbeing."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.6",
          "title": "Validating Hyperset Models of Experience",
          "text": "We have made some rather bold hypotheses here, regarding the abstract structures present\nin physical systems corresponding to the experiences of reflective consciousness, free will and\nphenomenal self. How might these hypotheses be validated or refuted?\nThe key is the evaluation of hypersets as subpatterns in physical systems. Taking reflective\nconsciousness as an example, one could potentially validate whether, when a person is (or,\nin the materialist view, reports being) reflectively conscious of a certain apple being in front\nof them, the hypothetically corresponding hyperset structure is actually a subpattern in their\nbrain structure and dynamics. We cannot carry out this kind of data analysis on brains yet,\nbut it seems within the scope of physical science to do so.\nBut, suppose the hypotheses presented here are validated, in the sense proposed above. Will\nthis mean that the phenomena under discussion – free will, reflective consciousness, phenomenal\nself – have been \"understood\"?\nThis depends on one’s philosophy of consciousness. According to a panpsychist view, for\ninstance, the answer would seem to be \"yes,\" at least in a broad sense – the hyperset models\npresented would then constitute a demonstratively accurate model of the patterns in physical\nsystems corresponding to the particular manifestations of universal experience under discussion.\nAnd it also seems that the answer would be \"yes\" according to a purely materialist perspective,\nsince in that case we would have figured out what classes of physical conditions correspond to\nthe \"experiential reports\" under discussion. Of course, both the panpsychist and materialist\nviews are ones in which the \"hard problem\" is not an easy problem but rather a non-problem!\nThe ideas presented here have originated within a patternist perspective, in which what’s\nimportant is to identify the patterns constituting a given phenomenon; and so we have sought\nto identify the patterns corresponding to free will, reflective consciousness and phenomenal self.\nThe \"hard problem\" then has to do with the relationships between various qualities that these\npatterns are hypothesized to possess (experiential versus physical) ... but from the point of\nview of studying brains, building AI systems or conducting our everyday lives, it is generally\nthe patterns (and their subpatterns) that matter.\nFinally, if the ideas presented above are accepted as a reasonable approach, there is certainly\nmuch more work to be done. There are many different states of consciousness, many different\nvarieties of self, many different aspects to the experience of willing, and so forth. These differ ent particulars may be modeled using hypersets, via extending and specializing the definitions\nproposed above. This suggested research program constitutes a novel variety of consciousness\nstudies, using hypersets as a modeling language, which may be guided from a variety of direc tions including empirics and introspection."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.7",
          "title": "Implications for Practical Work on Machine Consciousness",
          "text": "But what are the implications of the above ideas for machine consciousness in particular? One\nvery clear implication is that digital computers probably can be just as conscious as humans\ncan. Why the hedge \"probably\"? One reason is the possibility that there are some very odd,\nunanticipated restrictions on the patterns realizable in digital computers under the constraints\nof physical law. It is possible that special relativity and quantum theory, together, don’t allow\na digital computer to be smart enough to manifest self-reflective patterns of the complexity\ncharacteristic of human consciousness. (Special relativity means that big systems can’t think\nas fast as small ones; quantum theory means that systems with small enough components\nhave to be considered quantum computers rather than classical digital computers.) This seems\nextremely unlikely to me, but it can’t be rated impossible at this point. And of course, even if\nit’s true, it probably just means that machine consciousness needs to use quantum machines,\nor whatever other kind of machines the brain turns out to be.\nSetting aside fairly remote possibilities, then, it seems that the patterns characterizing re flective consciousness, self and will can likely emerge from AI programs running on digital\ncomputers. But then, what more can be said about how these entities might emerge from the\nparticular cognitive architectures and processes at play in the current AI field?\nThe answer to this question turns out to depend fairly sensitively on the particular AI archi tecture under consideration. Here we will briefly explore this issue in the context of CogPrime.\nHow do our hyperset models of reflective consciousness, self and will reflect themselves in\nthe CogPrime architecture?\nThere is no simple answer to these questions, as CogPrime is a complex system with multiple\ninteracting structures and dynamics, but we will give here a broad outline.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "C.7.1",
              "title": "Attentional Focus in CogPrime",
              "text": "The key to understanding reflective consciousness in CogPrime is the ECAN (Economic Atten tion Networks) component, according to which each Atom in the system’s memory has certain\nShortTermImportance (STI) and LongTermImportance (LTI) values. These spread around the\nmemory in a manner vaguely similar to activation spreading in a neural net, but using equa tions drawn from economics. The equations are specifically tuned so that, at any given time, a\ncertain relatively small subset of Atoms will have significantly higher STI and LTI values than\nthe rest. This set of important Atoms is called the AttentionalFocus, and represents the \"mov ing bubble of attention\" mentioned above, corresponding roughly to the Global Workspace in\nglobal workspace theory.\nAccording to the patternist perspective, if some set of Atoms remains in the AttentionalFocus\nfor a sustained period of time (which is what the ECAN equations are designed to encourage),\nthen this Atom-set will be a persistent pattern in the system, hence a significant part of the\nsystem’s mind and consciousness. Furthermore, the ECAN equations encourage the formation\nof densely connected networks of Atoms which are probabilistic attractors of ECAN dynamics,\nand which serve as hubs of larger, looser networks known as \"maps.\" The relation between\nan attractor network in the AttentionalFocus and the other parts of corresponding maps that\nhave lower STI, is conceptually related to the feeling humans have that the items in their\nfocus of reflective consciousness are connected to other dimly-perceived items \"on the fringes\nof consciousness.\"\nThe moving bubble of attention does not in itself constitute humanlike \"reflective conscious ness\", but it prepares the context for this. Even a simplistic, animal-like CogPrime system with\nalmost no declarative understanding of itself or ability to model itself, may still have intensely\nconscious patterns, in the sense of having persistent networks of Atoms frequently occupying\nits AttentionalFocus, its global workspace."
            },
            {
              "type": "section",
              "level": 2,
              "id": "C.7.2",
              "title": "Maps and Focused Attention in CogPrime",
              "text": "The relation between focused attention and distributed cognitive maps in CogPrime bears some\nemphasis, and is a subtle point related to CogPrime knowledge representation, which takes both\nexplicit and implicit forms. The explicit level consists of Atoms with clearly comprehensible\nmeanings, whereas the implicit level consists of “maps” as mentioned above – collections of\nAtoms that become important in a coordinated manner, analogously to cell assemblies in an\nattractor neural net.\nFormation of small maps seems to follow from the logic of focused attention, along with\nhierarchical maps of a certain nature. But the argument for this is somewhat subtle, involving\ncognitive synergy between PLN inference and economic attention allocation.\nThe nature of PLN is that the effectiveness of reasoning is maximized by (among other strate gies) minimizing the number of incorrect probabilistic independence assumptions. If reasoning\non N nodes, the way to minimize independence assumptions is to use the full inclusion-exclusion\nformula to calculate interdependencies between the N nodes. This involves 2^N terms, one for\neach subset of the N nodes. Very rarely, in practical cases, will one have significant information\nabout all these subsets. However, the nature of focused attention is that the system seeks to\nfind out about as many of these subsets as possible, so as to be able to make the most accurate\npossible inferences, hence minimizing the use of unjustified independence assumptions. This\nimplies that focused attention cannot hold too many items within it at one time, because if N\nis too big, then doing a decent sampling of the subsets of the N items is no longer realistic.\nSo, suppose that N items have been held within focused attention, meaning that a lot of\npredicates embodying combinations of N items have been constructed and evaluated and rea soned on. Then, during this extensive process of attentional focus, many of the N items will be\nuseful in combination with each other - because of the existence of predicates joining the items.\nHence, many HebbianLinks (Atoms representing statistical association relationships) will grow\nbetween the N items - causing the set of N items to form a map.\nBy this reasoning, focused attention in CogPrime is implicitly a map formation process – even\nthough its immediate purpose is not map formation, but rather accurate inference (inference\nthat minimizes independence assumptions by computing as many cross terms as is possible\nbased on available direct and indirect evidence). Furthermore, it will encourage the formation\nof maps with a small number of elements in them (say, N<10). However, these elements may\nthemselves be ConceptNodes grouping other nodes together, perhaps grouping together nodes\nthat are involved in maps. In this way, one may see the formation of hierarchical maps, formed\nof clusters of clusters of clusters..., where each cluster has N<10 elements in it.\nIt is tempting to postulate that any intelligent system must display similar properties - so\nthat focused consciousness, in general, has a strictly limited scope and causes the formation of\nmaps that have central cores of roughly the same size as its scope. If this is indeed a general\nprinciple, it is an important one, because it tells you something about the general structure\nof concept networks associated with intelligent systems, based on the computational resource\nconstraints of the systems. Furthermore this ties in with the architecture of the self."
            },
            {
              "type": "section",
              "level": 2,
              "id": "C.7.3",
              "title": "Reflective Consciousness, Self and Will in CogPrime",
              "text": "So far we have observed the formation of simple maps in OpenCogPrime systems, but we haven’t\nyet observed the emergence of the most important map: the self-map. According to the theory\nunderlying CogPrime, however, we believe this will ensue once an OpenCogPrime -controlled\nvirtual agent is provided with sufficiently rich experience, including diverse interactions with\nother agents.\nThe self-map is simply the network of Nodes and Links that a CogPrime system uses to\npredict, explain and simulate its own behavior. \"Reflection\" in the sense of cognitively reflecting\non oneself, is modeled in CogPrime essentially as \"doing PLN inference, together with other\ncognitive operations, in a manner heavily involving one’s self-map.\"\nThe hyperset models of reflective consciousness and self presented above, appear in the con text of CogPrime as approximative models of properties of maps that emerge in the system due\nto ECAN AttentionalFocus/map dynamics and its relationship with other cognitive processes\nsuch as inference. Our hypothesis is that, once a CogPrime system is exposed to the right\nsort of experience, it will internally evolve maps associated with reflective cognition and self,\nwhich possess an internal recursive structure that is effectively approximated using the hyperset\nmodels given above.\nWill, then, emerges in CogPrime in part due to logical Atoms known as CausalImplication Links. A link of this sort is formed between A and B if the system finds it useful to hypothesis\nthat \"A causes B.\" If A is an action that the system itself can take (a GroundedSchemaNode,\nin CogPrime lingo) then this means roughly that \"If I chose to do A, then B would be likely\nto ensue.\" If A is not an action the system can take, then the meaning may be interpreted\nsimilarly via abductive inference (i.e. via heuristic reasoning such as \"If I could do A, and I did\nit, then B would likely ensue\").\nThe self-map is a distributed network phenomenon in CogPrime’s AtomSpace, but the cog nitive process called MapFormation may cause specific ConceptNodes to emerge that serve as\nhubs for this distributed network. These Self Nodes may then get CausalImplicationLinks point ing out from them – and in a mature CogPrime system, we hypothesize, these will correlate\nwith the system’s feeling of willing. The recursive structure of will emerges directly from the\nrecursive structure of self, in this case – if the system ascribes cause to its self, then within\nitself there is also a model of its ascription of cause to its self (so that the causal ascription\nbecomes part of the self that is being ascribed causal power), and so forth on multiple levels.\nThus one has a finite-depth recursion that is approximatively modeled by the hyperset model\nof will described above.\nAll this goes well beyond what we have observed in the current CogPrime system (we have\ndone some causal inference, but not yet in conjunction with self-modeling), but it follows from\nthe CogPrime design on a theoretical level, and we will be working over the next years to bring\nthese abstract notions into practice."
            },
            {
              "type": "section",
              "level": 2,
              "id": "C.7.4",
              "title": "Encouraging the Recognition of Self-Referential Structures in the AtomSpace",
              "text": "Finally, we consider the possibility that a CogPrime system might explicitly model its own self\nand behavior using hypersets.\nThis is quite an interesting possibility, because, according to the same logic as map formation:\nif these hyperset structures are explicitly recognized when they exist, they can then be reasoned\non and otherwise further refined, which may then cause them to exist more definitively ... and\nhence to be explicitly recognized as yet more prominent patterns ... etc. The same virtuous\ncycle via which ongoing map recognition and encapsulation leads to concept formation, might\npotentially also be made to occur on the level of complex self-referential structures, leading to\ntheir refinement, development and ongoing complexity.\nOne relatively simple way to achieve this in CogPrime would be to encode hyperset struc tures and operators in the set of primitives of the \"Combo\" language that CogPrime uses to\nrepresent procedural knowledge (a simple LISP-like language with carefully crafted hooks into\nthe AtomSpace and some other special properties). If this were done, one could then recognize\nself-referential patterns in the AtomTable via standard CogPrime methods like MOSES and\nPLN.\nThis is quite possible, but it brings up a number of other deep issues that go beyond the\nscope of this appendix. For instance, most knowledge in CogPrime is uncertain, so if one is\nto use hypersets in Combo, one would like to be able to use them probabilistically. The most\nnatural way to assign truth values to hyperset structure turns is to use infinite order probability\ndistributions, as described in [?]. Infinite-order probability distributions are partially-ordered,\nand so one can compare the extent to which two different self-referential structures apply\nto a given body of data (e.g. an AtomTable), via comparing the infinite-order distributions\nthat constitute their truth values. In this way, one can recognize self-referential patterns in an\nAtomTable, and carry out encapsulation of self-referential maps. This sounds very abstract and\ncomplicated, but the class of infinite-order distributions defined in the above-referenced papers\nactually have their truth values defined by simple matrix mathematics, so there is really nothing\nthat abstruse involved in practice.\nClearly, with this subtle, currently unimplemented aspect of the CogPrime design we are\nveering rather far from anything the human brain could plausibly be doing in detail. This is\nfine, as CogPrime is not intended as a brain emulation. But yet, some meaningful connections\nmay be drawn to neuroscience. In ?? we have discussed how probabilistic logic might emerge\nfrom the brain, and also how the brain may embody self-referential structures like the ones\nconsidered here, via (perhaps using the hippocampus) encoding whole neural nets as inputs\nto other neural nets. Regarding infinite-order probabilities, the brain is effective at carrying\nout various dynamics equivalent to matrix manipulations, and one can mathematically reduce\ninfinite-order probabilities to such manipulations, so that it’s not completely outlandish to posit\nthe brain could be doing something mathematically analogous. Thus, all in all, it seems at least\nplausible that the brain could be doing something roughly analogous to what we’ve described\nhere, though the details would obviously be very different."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.8",
          "title": "Algebras of the Social Self",
          "text": "In the remainder of this appendix we will step even further out on our philosophico-mathematical\nlimb, and explore the possibility that the recursive structures of the self involve mutual recursion\naccording to the pattern of the quaternionic and octonionic algebras.\nThe argument presented in favor of this radical hypothesis has two steps. First, it is ar gued that much of human psychodynamics consists of “internal dialogue” between separate\ninternal actors – some of which may be conceived as subselves a la [?], some of which may\nbe “virtual others” intended to explicitly mirror other humans (or potentially other entities\nlike animals or software programs). Second, it is argued that the structure of inter-observation\namong multiple inter-observing actors naturally leads to quaternionic and octonionic algebras.\nSpecifically, the structure of inter-observation among three inter-observers is quaternionic; and\nthe structure of inter-observation among four inter-observers is octonionic. This mapping be tween inter-observation and abstract algebra is made particularly vivid by the realization that\nthe quaternions model the physical situation of three mirrors facing each other in a triangle;\nwhereas the octonions model the physical situation of four mirrors facing each other in a tetra hedron, or more complex packing structures related to tetrahedra. Using these facts, we may\nphrase the main thesis to be pursued in the remainder of the appendix in a simple form: The\nstructure of the self of an empathic social intelligence is that of a quaternionic or\noctonionic mirrorhouse.\nThere is an intriguing potential tie-in with recent developments in neurobiology, which sug gest that empathic modeling of other minds may be carried out in part via a “mirror neuron\nsystem” that enables a mind to experience another’s actions, in a sense, “as if they were its\nown” [?]. There are also echoes here of Buckminster Fuller’s [?] philosophy, which viewed the\ntetrahedron as an essential structure for internal and external reality (since the tetrahedron is\nclosely tied with the quaternion algebra)."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.9",
          "title": "The Intrinsic Sociality of the Self",
          "text": "We begin the next step of our journey with a theme that is generally neglected within AI yet is\nabsolutely critical to humanlike intelligence: the social nature of the individual mind. In what\nsense may it be said that the self of an individual human being is a “social” system?\nA 2001 special issue of “Journal of Consciousness Studies” [?] provided an excellent summary\nof recent research and thinking on this topic. A basic theme spanning several papers in the\nissue was as follows:\n\n1. The human brain contains structures specifically configured to respond to other humans’\nbehaviors (these appear to involve “mirror neurons” and associated “mirror neuron systems,”\non which we will elaborate below).\n2. these structures are also used internally when no other people (or other agents) are present,\nbecause human self is founded on a process of continual interaction between “phenome nal self” and “virtual other(s)”, where the virtual others are reflected by the same neural\nprocesses used to mirror actual others\n3. so, the iteration between phenomenal self and actual others is highly wrapped up with the\ninteraction between phenomenal self and virtual others\n\nIn other words, there is a complex dynamics according to which self is fundamentally\ngrounded in sociality and social interactions. The social interactions that structure the self are\nin part grounded in the interactions between the brain structures generating the phenomenal\nself and the brain structures generating the virtual others. They are part of the dynamics of the\nself as well as part of the interactions between self and actual others. Human self is intrinsically\nnot autonomous and independent, but rather is intrinsically dialogic and intersubjective.\nAnother way to phrase this is in terms of “empathy.” That is, one can imagine an intelligence\nthat attempted to understand other minds in a purely impersonal way, simply by reasoning\nabout their behavior. But that doesn’t seem to be the whole story of how humans do it.\nRather, we do it, in part, by running simulations of the other minds internally – by spawning\nvirtual actors, virtual selves within our own minds that emulate these other actors (according\nour own understanding). This is why we have the feeling of empathy – of feeling what another\nmind is feeling. It’s because we actually are feeling what the other mind is feeling – in an\napproximation, because we’re feeling what our internal simulation of the other mind is feeling.\nThus, one way to define “empathy” is as the understanding of other minds via internal simulation\nof them. Clearly, internal simulation is not the only strategy the human mind takes to studying\nother minds – the patterns of errors we make in predicting others’ behaviors indicates that\nthere is also an inferential, analytical component to a human’s understanding of others [?].\nBut empathic simulation is a key component, and we suggest that, in normal humans (autistic\nhumans may be a counterexample; see [?], it is the most central aspect of other-modeling, the\nframework upon which other sorts of other-modeling such as inferencing are layered.\nThis perspective has some overlap with John Rowan’s theory of human subpersonalities [?],\naccording to which each person is analyzed as possessing multiple subselves representing differ ent aspects of their nature appropriate to different situations. Subselves may possess different\ncapabilities, sometimes different memories, and commonly differently biased views of the com mon memory store. Numerous references to this sort of “internal community” of the mind exist\nin literature, e.g. Proust’s reference to “the several gentlemen of whom I consist.”\nPutting these various insights together, we arrive at a view of the interior of the human\nmind as consisting of not a single self but a handful of actors representing subselves and virtual\nothers. In other words, we arrive at a perspective of human mind as social mind, not only in the\nsense that humans define themselves largely in terms of their interactions with others, but also\nin the sense that humans are substantially internally constituted by collections of interacting\nactors each with some level of self-understanding and autonomy. In the following sections we\nfollow this general concept up by providing a specific hypothesis regarding the structure of\nthis internal social mind: that it corresponds to the structures of certain physical constructs\n(mirrorhouses) and certain abstract algebras (quaternions, octonions and Clifford algebras)."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.10",
          "title": "Mirror Neurons and Associated Neural Systems",
          "text": "An increasingly popular line of thinking ties together the above ideas regarding self-as-social system, with recent neurobiological results regarding the role of mirror neurons and associated\nneural systems in allowing human and animal minds to interpret, predict and empathize with\nother human and animal minds with which they interact. The biology of mirror neuron systems\nis still only partially understood, so that the tie-in between mirror neurons and psychological\nstructures as discussed here must be viewed as highly subject to revision based on further re finement of our understanding in the biology of mirror neurons. Ultimately, the core AI and\ncognitive science ideas of this appendix would remain equally valid if one replaced “mirror neu rons” and associated systems with some other, functionally similar neural mechanism. However,\ngiven that we do have some reasonably solid biological data – and some additional, associated\ndetailed biological hypotheses – regarding the role of mirror neurons in supporting the functions\nof empathy and self, it is interesting to investigate what these data and hypotheses suggest.\nIn simplest terms, a mirror neuron is a neuron which fires both when an animal acts and\nwhen the animal observes the same action performed by another animal, especially one of the\nsame species. Thus, the neuron is said to “mirror” the behavior of another animal – creating a\nsimilar neuronal activation patterns as if the observer itself were acting. Mirror neurons have\nbeen directly observed in primates, and are believed to exist in humans as well as in some other\nmammals and birds [?]. Evidence suggestive of mirror neuron activity has been found in human\npremotor cortex and inferior parietal cortex. V.S. Ramachandran [?] has been among the more\nvocal advocates of the important of mirror neurons, arguing that they may be one of the most\nimportant findings of neuroscience in the last decade, based on the likelihood of their playing\na strong role in language acquisition via imitative learning.\nThe specific conditions under which mirror neuron activity occurs are still being investi gated and are not fully understood. Among the classic examples probed in lab experiments are\ngrasping behavior, and facial expressions indicating emotions such as disgust. When an ape\nsees another ape grasp something, or make a face indicating disgust, mirror neurons fire in the\nobserving ape’s brain, similar to what would happen if the observing ape were the one doing the\ngrabbing or experiencing the disgust. This is a pretty powerful set of facts – what it says is that\nshared experience among differently embodied minds is not a purely cultural or psychological\nphenomenon, it’s something that is wired into our physiology. We really can feel each others’\nfeelings as if they were our own; to an extent, we may even be able to will each others’ actions\nas if they were our own [?].\nEqually interesting is that mirror neuron response often has to do with the perceived inten tion or goal of an action, rather than the specific physical action observed. If another animal is\nobserved carrying out an action that is expected to lead to a certain goal, the observing animal\nmay experience neural activity that it would experience if it had achieved this goal. Further more, mere visual observation of actions doesn’t necessarily elicit mirror neuron activity. Recent\nstudies [?, ?] involved scanning the brains of various human subjects while they were observing\nvarious events, such another person speaking or biting something, a monkey lip-smacking or a\ndog barking. The mirror neurons were not activated by the sight of the barking dog – presum ably because this was understood visually and not empathically (since people don’t bark), but\nwere activated by the sight of other people as well as of monkeys.\nThere is also evidence that mirror neurons may come to be associated with learned rather\nthan just inherited capabilities. For instance, monkeys have mirror neurons corresponding to\nspecific activities such as tearing paper, which are learned in the lab and have no close correlate\nin the wild [?].",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "C.10.1",
              "title": "Mirror Systems",
              "text": "Perhaps the most ambitious hypothesis regarding the role of mirror neurons in cognition is\nRizzolatti and Arbib’s [?] Mirror System Hypothesis, which conjectures that neural assemblies\nreliant on mirror neurons played a key role in the evolution of language. These authors sug gest that Broca’s area (associated with speech production) evolved on top of a mirror system\nspecialized for grasping, and inherited from this mirror system a robust capacity for pattern\nrecognition and generation, which was then used to enable imitation of vocalizations, and to\nencourage “parity” in which associations involving vocalizations are roughly the same for the\nspeaker as for the hearer. According to the MSH, the evolution of language proceeded according\nto the following series of steps [?]:\n\n1. S1: Grasping.\n2. S2: A mirror system for grasping, shared with the common ancestor of human and monkey.\n3. S3: A system for simple imitation of grasping shared with the common ancestor of human\nand chimpanzee. The next 3 stages distinguish the hominid line from that of the great apes:\n4. S4: A complex imitation system for grasping.\n5. S5: Protosign, a manual-based communication system that involves the breakthrough from\nemploying manual actions for praxis to using them for pantomime (not just of manual\nactions), and then going beyond pantomime to add conventionalized gestures that can\ndisambiguate pantomimes.\n6. S6: Protospeech, resulting from linking the mechanisms for mediating the semantics of\nprotosign to a vocal apparatus of increasing flexibility. The hypothesis is not that S5 was\ncompleted before the inception of S6, but rather that protosign and protospeech evolved\ntogether in an expanding spiral.\n7. S7: Language: the change from action-object frames to verb-argument structures to syntax\nand semantics.\n\nAs we will point out below, one may correlate this series of stages with a series of mirrorhouses\ninvolving an increasing number of mirrors. This leads to an elaboration of the MSH, which\nposits that evolutionarily, as the mirrorhouse of self and attention gained more mirrors, the\ncapability for linguistic interaction became progressively more complex."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.11",
          "title": "Quaternions and Octonions",
          "text": "In this section, as a preparation for our mathematical treatment of mirrorhouses and the self, we\nreview the basics of the quaternion and octonion algebras. This is not original material, but it\nis repeated here because it is not well known outside the mathematics and physics community.\nReaders who want to learn more should follow the references.\nMost readers will be aware of the real numbers and the complex numbers. The complex\nnumbers are formed by positing an “imaginary number” i so that i*i=-1, and then looking at\n“complex numbers” of the form a+bi, where a and b are real numbers. What is less well known\nis that this approach to extending the real number system may be generalized further. The\nquaternions are formed by positing three imaginary numbers i, j and k with i*i=j*j=k*k=-1,\nand then looking at “quaternionic numbers” of the form a + bi + cj + dk. The octonions\nare formed similarly, by positing 7 imaginary numbers i,j,k,E,I,J,K and looking at “octonionic\nnumbers” defined as linear combinations thereof.\nWhy 3 and 7? This is where the math gets interesting. The trick is that only for these di mensionalities can one define a multiplication table for the multiple imaginaries so that unique\ndivision and length measurement (norming) will work. For quaternions, the “magic multiplica tion table” looks like\ni ∗ j = k, j ∗ i = −k\nj ∗ k = i, k ∗ j = −i\nk ∗ i = j, i ∗ k = −j\nUsing this multiplication table, for any two quaternionic numbers A and B, the equation\nx ∗ A = B\nhas a unique solution when solved for x. Quaternions are not commutative under multiplication,\nunlike real and complex numbers: this can be seen from the above multiplication table in which\ne.g. i*j is not equal to j*i. However, quaternions are normed: one can define ||A||for a quaternion\nA, in the familiar root-mean-square manner, and get a valid measure of length fulfilling the\nmathematical axioms for a norm.\nNote that you can also define an opposite multiplication for quaternions: from i*j = k you\ncan reverse to get j*i = k, which is an opposite multiplication, that still works, and basically\njust constitutes a relabeling of the quaternions. This is different from the complex numbers,\nwhere there is only one workable way to define multiplication.\nThe quaternion algebra is fairly well known due to its uses in classical physics and computer\ngraphics; the octonion algebra, also known as Cayley’s octaves, is less well known but is adeptly\nreviewed by John Baez [?].\nThe magic multiplication table for 7 imaginaries that leads to the properties of unique division\nand normed-ness is shown in Table C.1. Actually this is just one of 480 basically equivalent (and\nequally “magical”) forms of the octonionic multiplication table (as opposed to the 2 varieties\nfor quaternions, mentioned above). Note that, according to this or any of the other 479 tables,\noctonionic multiplication is neither commutative nor associative; but octonions do satisfy a\nweaker form of associativity called alternativity, which means that the subalgebra generated by\nany two elements is associative.\n\nTable C.1: Octonion multiplication table\n      1  i  j  k  E  I  J  K\n    i -1  k -j  I -E -K  J\n    j -k -1  i  J  K -E -I\n    k  j -i -1  K -J  I -E\n    E -I -J -K -1  i  j  k\n    I  E -K  J -i -1 -k  j\n    J  K  E -I -j  k -1 -i\n    K -J  I  E -k -j  i -1\n\nAs it happens, the only normed division algebras over the reals are the real, complex, quater nionic and octonionic number systems. These four algebras also form the only alternative,\nfinite-dimensional division algebras over the reals. These theorems are nontrivial to prove, and\nfascinating to contemplate – and even more fascinating when one considers their possible con nection to the emergent structures of general intelligence."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.12",
          "title": "Modeling Mirrorhouses Using Quaternions and Octonions",
          "text": "Now let’s move from algebras to mirrors – houses of mirrors, to be precise. Interestingly shaped\nhouses of mirrors!\nMirrorhouses are structures built up from mutually facing mirrors which reflect each others’\nreflections. The simplest mirrorhouse possible to construct is made of two facing mirrors, X and\nY. X reflects Y and Y reflects X.\nIn terms of hypersets, a simple 2-mirror mirrorhouse may be crudely described as:\nX = {Y }\nY = {X}\n(ignoring the inversion effect of mirroring).\nNote that if we try to unravel this hyperset by inserting one element into the other we arrive\nat an infinite regress:\nY = {X = {Y = {X = {Y = {X = {Y = {{X = {Y = {...}}}}}}}}} This corresponds to the illusory infinite tube which interpenetrates both mirrors.\nSuppose now that we constructed a mirrorhouse from three mirrors instead of two. What\nhyper-structure would this have? Amazingly it turns out that it has precisely the structure of\nthe quaternion imaginaries.\nLet i, j and k be hypersets representing three facing mirrors. We then have that\ni = {j, k}\nj = {k, i}\nand\nk = {i, j}\nwhere the notation i={j,k} means, e.g. that mirror i reflects mirrors j and k in that order.\nWith three mirrors ordering now starts playing a vital role because mirroring inverts\nleft/right-handedness. If we denote the mirror–inversion operation by “-” we have that\ni = {j, k} = −{k, j}\nj = {k, i} = −{i, k}\nk = {i, j} = −{j, i}\nBut the above is exactly the structure of the quaternion triple of imaginaries:\ni = j ∗ k = −k ∗ j\nj = k ∗ i = −i ∗ k\nk = i ∗ j = −j ∗ i\nThe quaternion algebra therefore is the precise model of three facing mirrors, where we see\nmirror inversion as the quaternionic anti-commutation. The two versions of the quaternion\nmultiplication table correspond to the two possible ways of arranging three mirrors into a\ntriangular mirrorhouse.\nWhen we move on to octonions, things get considerably subtler – though no less elegant,\nand no less conceptually satisfying. While there are 2 possible quaternionic mirrorhouses, there\nare 480 possible octonionic mirrorhouses, corresponding to the 480 possible variant octonion\nmultiplication tables!\nRecall that the octonions have 7 imaginaries i,j,k,E,I,J,K, which have 3 algebraic generators\ni,j,E (meaning that combining these three imaginaries can give rise to all the others). The third\ngenerator E is distinguished from the others, and we can vary it to get the 480 multiplication s/mirrorhouses.\nThe simplest octonionic mirrorhouse is simply the tetrahedron (see Figure ??). More complex\noctonionic mirrorhouses correspond to tetrahedra with extra mirrors placed over their internal\ncorners, as shown in Figure ??. This gives rise to very interesting geometric structures, which\nhave been explored by Buckminster Fuller and also by various others throughout history.\n[Image of a simple tetrahedron]\nStart with a 3-dimensional tetrahedron of 4 facing mirrors. Let the floor be the distinguished\nthird generator E and the 3 walls be I,J,K (with a specific assignment of walls to imaginaries,\nof course). Then, by reflection through the E floor, the reflected I J K become i j k, and we now\nhave all 7 imaginary octonions. This relatively simple tetrahedral mirrorhouse corresponds to\none of the 480 different multiplications; the one given in the table above.\nTo get another we truncate the tetrahedron. Truncation puts a mirror parallel to the floor,\nmaking a mirror roof. Then, when you look up at the mirror roof, you see the triangle roof\nparallel to the floor E. The triangle roof parallel to the floor E represents the octonion -E, and\nreflection in the roof -E gives 7 imaginary octonions with the multiplication rule in which -E is\nthe distinguished third generator.\nLooking up from the floor, you will also see 3 new triangles having a common side with the\ntriangle roof -E, and 6 new triangles having a common vertex with the triangle roof -E.\nThe triangle roof + 9 triangles = 10 triangles form half of the faces (one hemisphere) of\na 20-face quasi-icosahedron. The quasi-icosahedron is only qualitatively an icosahedron, and\nis not exact, since the internal angle of the pentagonal vertex figure of the reflected quasi icosahedron is not 108 degrees, but is 109.47 degrees (the octahedral dihedral angle), and the\nvertex angle is not 72 degrees, but is 70.53 degrees (the tetrahedral dihedral angle). (To get\nan exact icosahedral kaleidoscope, three of the triangles of the tetrahedron should be golden\nisosceles triangles.)\nEach of the 9 new triangles is a “reflection roof” defining another multiplication. Now, look\ndown at the floor E to see 9 new triangles reflected from the 9 triangles adjoining the roof -E.\nEach of these 9 new triangles is a “reflection floor” defining another multiplication. We have\nnow 1 + 1 + 9 + 9 = 20 of the 480 multiplications.\nJust as we put a roof parallel to the floor E by truncating the top of the tetrahedral pyramid,\nwe can put in 3 walls parallel to each of the 3 walls I, J, K by truncating the other 3 points of\nthe tetrahedron, thus getting 3x20 = 60 more multiplications. That gives us 20 + 60 = 80 of\nthe 480 multiplications.\n[Images of complex polyhedral mirror structures]\nTo get the rest, recall that we fixed the walls I, J, K in a particular order with respcet to the\nfloor E. There are 3! = 6 permutations of the walls I, J, K Taking them into account, we get\nall 6x80 = 480 multiplications.\nIn mathematical terms, this approach effectively fixes the 20-face quasi-icosahedron and\nvaries the 4 faces of the EIJK tetrahedron according to the 24-element binary tetrahedral\ngroup {3,3,2} = SL(2,3) to get the 20x24 = 480 multiplications.\nNote that the truncated tetrahedron with a quasi-icosahedron at each vertex combines two\ntypes of symmetries:\n\n1. tetrahedral, related to the square and the square root of 2, which gives open systems like:\nan arithmetic series overtone acoustic musical scale with common difference 1/8; the Roman\nSacred Cut in architecture; and multilayer space-filling cuboctahedral crystal growth.\n2. icosahedral, related to the pentagon, the Golden Mean (aka Golden Section), and Fi bonacci sequences, which gives closed systems like: a harmonic pentatonic musical scale; Le\nCorbusier’s Modulor; and single-layer icosahedral crystals.\n\nIt is interesting to observe that the binary icosahedral group is isomorphic to the binary\nsymmetry group of the 4-simplex, which may be called the pentahedron and which David\nFinkelstein and Ernesto Rodriguez (1984) have called the “Quantum Pentacle.” A pentahedron\nhas 5 vertices, 10 edges, 10 areas, and 5 cells. The 10 areas of a pentahedron correspond to the\n10 area faces of one hemisphere of an icosahedron.\nThe pentahedron projected into 3 dimensions looks like a tetrahedron divided into 4 quarter tetrahedra (Figure ??). If you add a quarter-tetrahedron to each truncation of a truncated\ntetrahedron, you get a space-filling polytope (Figure ??) that can be centered on a vertex of\na 3-dimensional diamond packing to form a Dirichlet domain of the 3-dimensional diamond\npacking (Figure ??). (A Dirichlet domain of a vertex in a packing is the set of points in the\nspace in which the packing is embedded that are nearer to the given vertex than to any other.)\nThe 4 most distant vertices of the Dirichlet domain polytope are vertices of the dual diamond\npacking in 3-dimensional space.\n[Image of a pentahedron projected into 3D]\n[Image of a space-filling polytope from truncated tetrahedra]\nAll in all, we conclude that:\n\n1. In its simplest form the octonion mirrorhouse is a tetrahedral mirrorhouse\n2. In its more general form, the octonion mirrorhouse shows a tetrahedral diamond packing\nnetwork of quasi-icosahedra, or equivalently, of quasi-pentahedra\n\nObservation as Mirroring\nNow we proceed to draw together the threads of the previous sections: mirror neurons and\nsubselves, mirrorhouses and normed division algebras.\nTo map the community of actors inside an individual self into the mirrorhouse/algebraic\nframework of the previous section, it suffices to interpret the above\n[Image of a complex polyhedral structure]\nX = {Y }\nY = {X}\nas\n“X observes Y”\n“Y observes X”\n(e.g. we may have X= primary subself, Y=inner virtual other), and the above\ni = {j, k}\nj = {k, i}\nk = {i, j}\nas\n“i observes {j observing k}”\n“j observes {k observing i}”\n“k observes {i observing j}”\nThen we can define the - observation as an inverter of observer and observed, so that e.g.\n{j, k} = −{k, j}\nWe then obtain the quaternions\ni = j ∗ k = −k ∗ j\nj = k ∗ i = −i ∗ k\nk = i ∗ j = −j ∗ i\nwhere multiplication is observation and negation is reversal of the order of observation. Three\ninter-observers = quaternions.\nThe next step is mathematically natural: if there are four symmetric inter-observers, one\nobtains the octonions, according to the logic of the above-described tetrahedral/tetrahedral diamond-packing mirrorhouse. Octonions may also be used to model various situations involv ing more than four observes with particular asymmetries among the observers (the additional\nobservers are the corner-mirrors truncating the tetrahedron.)\nWhy not go further? Who’s to say that the internal structure of a social mind isn’t related to\nmirrorhouses obtained from more complex shapes than tetrahedra and truncated tetrahedra?\nThis is indeed not impossible, but intuitively, we venture the hypothesis that where human\npsychology is concerned, the octonionic structure is complex enough. Going beyond this level\none loses the normed division-algebra structure that makes the octonions a reasonably nice\nalgebra, and one also gets into a domain of dramatically escalated combinatorial complexity.\nBiologically, what this suggests is that the MSH of Rizzolatti and Arbib just scratches the\nsurface. The system of mirror neurons in the human mind may in fact be a “mirrorhouse system,”\ninvolving four different cell assemblies, each involving substantial numbers of mirror neurons,\nand arranged in such a manner as to recursively reflect and model one another. This is a concrete\nneurological hypothesis which is neither strongly suggested nor in any way refuted by available\nbiological data: the experimental tools at our current disposal are simply not adequate to allow\nempirical exploration of this sort of hypothesis. The empirical investigation of cell assembly\nactivity is possible now only in a very primitive way, using crude tools such as voltage-sensitive\ndyes which provide data with a very high noise level (see e.g. [?]. Fortunately though, the\naccuracy of neural measurement technology is increasing at an exponential rate [?], so there is\nreason to believe that within a few decades hypotheses such as the presently positive “neural\nmirrorhouse” will reside in the domain of concretely-explorable rather than primarily-theoretical\nscience.\nAnd finally, we may take this conceptual vision one more natural step. The mirrorhouse\ninside an individual person’s mind is just one small portion of the overall social mirrorworld.\nWhat we really have is a collection of interlocking mirrorhouses. If one face of the tetrahedron\ncomprising my internal mirrorhouse at a certain moment corresponds to one of your currently\nactive subselves, then we may view our two selves at that moment as two adjacent tetrahedra.\nWe thus arrive at a view of a community of interacting individuals as a tiling of part of space\nusing tetrahedra, a vision that would have pleased Buckminster Fuller very much indeed."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.13",
          "title": "Specific Instances of Mental Mirrorhousing",
          "text": "We’ve voyaged fairly far out into mathematical modeling land – what does all this mean in\nterms of our everyday lives, or in terms of concrete AGI design?\nMost examples of mental mirrorhousing, I suggest, are difficult for us to distinguish intro spectively from other aspects of our inner lives. Mirroring among multiple subselves, simulations\nof others and so forth is so fully woven into our consciousness that we don’t readily distinguish\nit from the rest of our inner life. Because of this, the nature of mental mirroring is most easily\nunderstood via reference to “extreme cases.”\nFor instance, consider the following rather mundane real-life situation: Ben needs to estimate\nthe time-duration of a software project that has been proposed for the consulting division of\nhis AI software company. Ben knows he typically underestimates the amount of time required\nfor a project, but that he can usually arrive at a more accurate estimate via conversation with\nhis colleague Cassio. But Cassio isn’t available at the moment; or Ben doesn’t want to bother\nhim. So, Ben simulates an “internal Cassio,” and they dialogue together, inside Ben’s “mind’s\neye.” This is a mirror facing a mirror – an internal Ben mirroring an internal Cassio.\nBut this process in itself may be more or less effective depending on the specifics –depending\non, for example, which aspects of Ben or Cassio are simulated. So, an additional internal ob serving mind may be useful for, effectively, observing multiple runs of the “Ben and Cassio\nconversation simulator” and studying and tuning the behavior. Now we have a quaternionic\nmirrorhouse.\nBut is there a deeper inner observer watching over all this? In this case we have an octonionic,\ntetrahedral mirrorhouse.\nThe above is a particularly explicit example – but we suggest that much of everyday life ex perience consists of similar phenomena, where the different inter-mirroring agents are not neces sarily associated with particular names or external physical agents, and thus are more difficult\nto tangibly discussed. As noted above, this relates closely to Rowan’s analysis of human person ality as consisting largely of the interactional dynamics of various never-explicitly-articulated\nand usually-not-fully-distinct subpersonalities.\nFor another sort of example, consider the act of creativity, which in [?] is modeled in terms\nof a “creative subself”: a portion of the mind that is specifically devoted to creative activity in\none more more media, and has its own life and awareness and memory apart from the primary\nself-structure. The creative subself may create a work, and present it to the main subself for\nconsideration. The three of these participants – the primary subself, the creative subself and the\ncreative work – may stand in a relationship of quaternionic mirroring. And then the meta-self\nwho observes this threefold interaction completes the tetrahedral mirrorhouse.\nNext, let us briefly consider the classic Freudian model of personality and motivation. Ac cording to Freud, much of our psychology consists of interaction between ego, superego and id.\nRather than seeking to map the precise Freudian notions into the present framework, we will\nbriefly comment on how ideas inspired by these Freudian notions might play a role in the present\nframework. The basic idea is that, to the extent that there are neuropsychological subsystems\ncorresponding to Freudian ego, superego and id, these subsystems may be viewed as agents that\nmirror each other, and hence as a totality may be viewed as a quaternionic mirrorhouse. More\nspecifically we may correlate\n\n1. ego with the neuropsychological structure that Thomas Metzinger (2004) has identified as\nthe “phenomenal self”\n2. superego with the neuropsychological structure that represents the mind’s learned goal\nsystem – the set of goals that the system has created\n3. id with the neuropsychological structure that represents the mind’s in-built goal system,\nwhich largely consists of basic biological drives\n\nUsing this interpretation, we find that a quaternionic ego/superego/id mirrorhouse may in deed play a role in human psychology and cognition. However, there is nothing in the theoretical\nframework being pursued here to suggest that this particular configuration of inter-observers\nhas the foundational significance Freud ascribed to it. Rather, from the present perspective,\nthis Freudian triarchy appears as important configuration (but not the only one) that may\narise within the mirrorhouse of focused attention.\nAnd, of course, if we add in the internal observing eye that allowed Freud to identify this\nsystem in the first place, and we have an octonionic, tetrahedral mirrorhouse.\nFinally, let us consider the subjective experience of meditation, as discussed e.g. in [?]. Here\nwe have “consciousness without an object” [?], which may be understood as the infusion of the\nmental mirrorhouse with attention but not content. Each mirror is reflecting the others, without\nany image to reflect except the mirrors themselves."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.14",
          "title": "Mirroring in Development",
          "text": "Another naturally arising question regards the origin of the mental mirrorhouse faculty, both\nevolutionarily and developmentally. In both cases, the obvious hypothesis is that during the\ncourse of growth, the inner mirrorhouse gains the capability for using more and more mirrors.\nFirst comes the capability to internally mirror an external agent; then comes the capability\nto internally encapsulate an inter-observation process; then comes the capability to internally\nobserve an inter-observation process; then comes the capability to internally observe the ob servation of an inter-observation process. Of course, the hierarchy need not terminate with the\noctonionic mirrorhouse; but qualitatively, our suggestion is that levels beyond the octonionic\nmay generally beyond the scope of what the human brain/mind needs to deal with given its\nlimited environment and computational processing power.\nTo get a better grip on this posited growth process, let us return to Rizzolatti and Arbib’s\nhypothesized role for mirror neurons in language learning. Their stage S5, as described above,\ninvolves “proto-signs,” which may have initially consisted of pantomime used indirectly (i.e.\nused, not necessarily to denote specific motor actions, but to denote other activities loosely\nresembling those motor actions). A mental mirrorhouse corresponding to proto-signs may be\nunderstood to comprise 3 mirrors\n\n1. Observer\n2. Pantomimer (carrying out manual actions)\n3. Object of pantomime (carrying out non-manual actions)\n\nThe hypothesis becomes that, via recollecting instances of pantomime using a quaternionic\nmirrorhouse, the mind imprints pantomimes on the long-term memory, so that they become\npart of the unconscious in a manner suitable to encourage the formation of new and richer\npantomimes.\nIn general, going beyond the particular example of pantomime, we may posit a quaternionic\nmirrorhouse corresponding to\n\n1. Observer\n2. Symbols\n3. Referent\n\nThe addition of a fourth mirror then corresponds to reflection on the process of symbol ization, which is not necessary for use of language but is necessary for conscious creation of\nlanguage, as is involved for instance in formalizing grammar or creating abstract mathematics.\nThere is a clear and fascinating connection here with Piagetan developmental psychology,\nas reviewed in Chapter ?? above, in which the capability for symbolization is posited to come\nalong with the “concrete operational” stage of development (between ages 7-14 in the average\nchild); and the capability for abstract formal reasoning comes later in the “formal” stage of\ndevelopment. The natural hypothesis in this connection is that the child’s mind during the\nconcrete operational stage possesses only a quaternionic mirrorhouse (or at least, that only the\nquaternionic mirrorhouse is highly functional at this stage); and that the advent of the formal\nstage corresponds to the advent of the octonionic mirrorhouse.\nThis hypothesis has interesting biological applications, in the context of the previously hy pothesized relationship between mirror neurons and mental mirroring. In this case, if the hy pothesized correspondence between number-of-mirrors and developmental stages exists, then\nit should eventually be neurologically observable via studying the patterns of interaction of\ncell assemblies whose dynamics are dominated by mirror neurons, in the brains of children at\ndifferent stages of cognitive development. As noted above, however, experimental neuroscience\nis currently nowhere near being able to validate or refute such hypotheses, so we must wait at\nleast a couple decades before pursuing this sort of empirical investigation."
        },
        {
          "type": "section",
          "level": 1,
          "id": "C.15",
          "title": "Concluding Remarks",
          "text": "Overall, the path traced in this appendix has been a somewhat complex one, but the broad\noutline of the story is summarizable compactly.\nFirstly, there may well be elegant recursive, self-referential structures underlying reflective\nconsciousness, will and self.\nAnd secondly, there may plausibly be elegant abstract-algebraic symmetries lurking within\nthe social substructures of the self. The notion of \"emergent structures of mind\" may include\nemergent algebraic structures arising via the intrinsic algebra of reflective processes.\nWe have some even more elaborate and speculative conjectures extending the ideas given\nhere, but will not burden the reader with them – we have gone as far as we have here, largely\nto indicate the sort of ideas that arise when one takes the notion of emergent mind structures\nseriously.\nUltimately, abstract as they are, these ideas must be pursued empirically rather than via\nconceptual argumentation and speculation. If the CogPrime engineering program is successful,\nthe emergence or otherwise of the structures discussed here, and others extending them, will\nbe discoverable via the mundane work of analyzing system logs."
        }
      ]
    },
    {
      "id": "D",
      "title": "GOLEM: Toward an AGI Meta-Architecture Enabling Both Goal Preservation and Radical Self-Improvement",
      "content": [
        {
          "type": "section",
          "level": 1,
          "id": "D.1",
          "title": "Introduction",
          "text": "One question that looms large when thinking about the ultimate roadmap for AGI and the\npotential for self-modifying AGI systems is: How to create an AGI system that will maintain\nsome meaningful variant of its initial goals even as it dramatically revises and\nimproves itself – and as it becomes so much smarter via this ongoing improvement that in\nmany ways it becomes incomprehensible to its creators or its initial condition. We would like to\nbe able to design AGI systems that are massively intelligent, creatively self-improving, probably\nbeneficial, and almost surely not destructive.\nAt this point, it’s not terribly clear whether an advanced CogPrime system would have this\ndesirable property or not. It’s certainly not implausible that it would, since CogPrime does\nhave a rich explicit goal system and is oriented to spend a significant percentage of its effort\nrationally pursuing its goals. And with its facility for reinforcement and imitation learning,\nCogPrime is well suited to learn ethical habits from its human teachers. But all this falls very\nfar short of any kind of guarantee.\nIn this appendix we’ll outline a general AGI meta-architecture called GOLEM (the Goal\nOriented LEarning Meta-architecture), that can be used as a \"wrapper\" for more detailed\nAGI architectures like CogPrime, and that appears (but hasn’t been formally proved) to have\nmore clearly desirable properties in terms of long-term ethical behavior. From a CogPrime\nperspective, GOLEM may be viewed as a specific CogPrime configuration, which has powerful\n\"AGI safety\" properties but also demands a lot more computational resources than many other\nCogPrime configurations would.\nTo specify these notions a bit further, we may define an intelligent system as steadfast if,\nover a long period of time, it either continues to pursue the same goals it had at the start of the\ntime period, or stops acting altogether. In this terminology, one way to confront the problem of\ncreating probably-beneficial, almost surely non-destructive AGI, is to solve the two problems\nof:\n\n• How to encapsulate the goal of beneficialness in an AGI’s goal system\n• How to create steadfast AGI, in a way that applies to the \"beneficialness\" goal among\nothers\n\nOf course, the easiest way to achieve steadfastness is to create a system that doesn’t change or\ngrow much. And the interesting question raised is how to couple steadfastness with ongoing,\nradical, transformative learning.\nIn this appendix we’ll present a careful semi-formal argument that, under certain reasonable\nassumptions (and given a large, but not clearly long-term infeasible amount of computer power),\nthe GOLEM meta-architecture is likely to be both steadfast and massively, self-improvingly\nintelligent. Full formalization of the argument is left for later, and may be a difficult task even\nif the argument is correct.\nAn alternate version of GOLEM is also described, which possesses more flexibility to adapt\nto an unknown future, but lacks a firm guarantee of steadfastness.\nDiscussion of the highly nontrivial problem of \"how to encapsulate the goal of beneficialness\nin an AGI’s goal system\" is also left for elsewhere (see [?] for some informal discussion). As\nreviewed already in Chapter ?? we suspect this will substantially be a matter of interaction\nand education rather than mainly a matter of explicitly formulating ethical content and telling\nor feeding it to an AGI system."
        },
        {
          "type": "section",
          "level": 1,
          "id": "D.2",
          "title": "The Goal Oriented Learning Meta-Architecture",
          "text": "The Goal Oriented LEarning Meta-architecture (GOLEM) refers to an AGI system S with the\nfollowing high-level meta-architecture, depicted roughly in Figure D.1:\n\n[Figure D.1: The GOLEM meta-architecture. Single-pointed errors indication information flow;\ndouble-pointed arrows indicate more complex interrelationships.]\n\n• Goal Evaluator = component that calculates, for each possible future world (including\nenvironment states and internal program states), how well this world fulfills the goal (i.e.\nit calculates the \"utility\" of the possible world)\n  – it may be that the knowledge supplied to the GoalEvaluator initially (the \"base GEOP\"\n  i.e. \"base GoalEvaluator Operating Program\") is not sufficient to determine the goal satisfaction provided by a world-state; in that case the GoalEvaluator may produce a\n  probability distribution over possible goal-satisfaction values\n  – initially the GoalEvaluator may be supplied with an inefficient algorithm encapsulating\n  the intended goals, which may then be optimized and approximated by application of\n  the Searcher (thus leading to a GEOP different from the base GEOP)\n  – if the GoalEvaluator uses a GEOP produced by the Searcher, then there may be an\n  additional source of uncertainty involved, which may be modeled by having the GoalE valuator output a second-order probability distribution (a distribution over distributions\n  over utility values), or else by collapsing this into a first-order distribution\n• HistoricalRepository = database storing the past history of S’s internal states and ac tions, as well as information about the environment during S’s past\n• Operating Program = the program that S is governing its actions by, at a given point\nin time\n  – chosen by the Metaprogram as the best program the Searcher has found, where \"best\" is\n  judged as \"highest probability of goal achievement\" based on the output of the Predictor\n  and the Goal Evaluator\n• Predictor = program that estimates, given a candidate operating program P and a possible\nfuture world W, the odds of P leading to W\n• Searcher = program that searches through program space to find a new program optimizing\na provided objective function\n• Memory Manager program = program that decides when to store new observations and\nactions in the Historical Repository, and which ones to delete in order to do so; potentially\nit may be given some hard-wired constraints to follow, such as \"never forget human history,\nor the previous century of your life.\"\n• Tester = hard-wired program that estimates the quality of a candidate Predictor, using a\nsimple backtesting methodology\n  – That is, the Tester assesses how well a Predictor would have performed in the past,\n  using the data in the HistoricalRepository\n• Metaprogram = fixed program that uses Searcher program to find a good\n  – Searcher program (judged by the quality of the programs it finds, as judged by the\n  Predictor program)\n  – Predictor program (as judged by the Tester’s assessments of its predictions)\n  – Operating Program (judged by Predictor working with Goal Evaluator, according to the\n  idea of choosing an Operating Program with the maximum expected goal achievement)\n  – GoalEvaluator Operating Program (judged by the Tester, evaluating whether a candi date program effectively predicts goal-satisfaction given program-executions, according\n  to the HistoricalRepository)\n  – Memory Manager (as judged by Searcher, which rates potential memory management\n  strategies based on the Predictor’s predictions of how well the system will fare under\n  each one)\nThe Metaprogram’s choice of Operating Program, Goal Evaluator Operating Program and\nMemory Manager may all be interdependent, as the viability of a candidate program for\neach of these roles may depend on what program is playing each of the other roles. The\nmetaprogram also determines the amount of resources to allocate to searching for a Searcher\nversus a Predictor versus an OP, according to a fixed algorithm for parameter adaptation.\nWhile this is a very abstract \"meta-architecture\", it’s worth noting that it could be imple mented using CogPrime or any other practical AGI architecture as a foundation – in this case,\nCogPrime is \"merely\" the initial condition for the OP, the Memory Manager, the Predictor\nand the Searcher. However, demonstrating that self-improvement can proceed at a useful rate\nin any particular case like this, may be challenging.\nNote that there are several fixed aspects in the above: the MetaProgram, the Tester, the\nGoalEvaluator, and the structure of the HistoricalRepository. The standard GOLEM, with\nthese aspects fixed, will also be called the fixed GOLEM, in contrast to an adaptive GOLEM in\nwhich everything is allowed to be adapted based on experience.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "D.2.1",
              "title": "Optimizing the GoalEvaluator",
              "text": "Note that the GoalEvaluator may need to be very smart indeed to do its job. However, an\nimportant idea of the architecture is that the optimization of the GoalEvaluator’s functionality\nmay be carried out as part of the system’s overall learning 1.\nIn its initial and simplest form, the GoalEvaluator’s internal Operating Program (GEOP)\ncould basically be a giant simulation engine, that tells you, based on a codified definition of\nthe goal function: in world-state W, the probability distribution of goal-satisfaction values is\nas follows. It could also operate in various other ways, e.g. by requesting human input when\nit gets confused in evaluating the desirability of a certain hypothetical world-state; by doing\nsimilarity matching according to a certain codified distance measure against a set of desirable\nworld-states; etc.\nHowever, the Metaprogram may supplement the initial \"base GEOP\" with an intelligent\nGEOP, which is learned by the Searcher, after the Searcher is given the goal of finding a\nprogram that will\n\n• accurately agree with the base GEOP across the situations in the HistoricalRepository, as\ndetermined by the Tester\n• be as compact as possible\n\nIn this approach, there is a \"base goal evaluator\" that may use simplistic methods, but then the\nsystem learns programs that do approximately the same thing as this but perhaps faster and\nmore compactly, and potentially embodying more abstraction. Since this program learning has\nthe specific goal of learning efficient approximations to what the GoalEvaluator does, it’s not\nsusceptible to \"cheating\" in which the system revises its goals to make them easier to achieve\n(unless the whole architecture gets broken).\nWhat is particularly interesting about this mechanism is: it provides a built-in mechanism for\nextrapolation beyond the situations for which the base GEOP was created. The Tester requires\nthat the learned GEOPs must agree with the base GEOP on the HistoricalRepository, but for\ncases not considered in the HistoricalRepository, the Metaprogram is then doing Occam’s Razor\nbased program learning, seeing a compact and hence rationally generalizable explanation of the\nbase GEOP.\n\n1 this general idea was introduced by Abram Demski upon reading an earlier draft of this appendix, though he\nmay not agree with the particular way I have improvised on his idea here"
            },
            {
              "type": "section",
              "level": 2,
              "id": "D.2.2",
              "title": "Conservative Meta-Architecture Preservation",
              "text": "Next, the GOLEM meta-architecture assumes that the goal embodied by the GoalEvaluator\nincludes, as a subgoal, the preservation of the overall meta-architecture described above (with\na fallback to inaction if this seems infeasible). This may seem a nebulous assumption, but it’s\nnot hard to specify if one thinks about it the right way.\nFor instance, one can envision each of the items in the above component list as occupying a\nseparate hardware component, with messaging protocols established for communicating between\nthe components along cables. Each hardware component can be assumed to contain some control\ncode, which is connected to the I/O system of the component and also to the rest of the\ncomponent’s memory and processors.\nThen what we must assume is that the goal includes the following criteria, which we’ll call\nconservative meta-architecture preservation:\n\n1. No changes to the hardware or control code should be made except in accordance with the\nsecond criterion\n2. If changes to the hardware or control code are found, then the system should stop act ing (which may be done in a variety of ways, ranging from turning off the power to self destruction; we’ll leave that unspecified for the time being as that’s not central to the point\nwe want to make here)\n\nAny world-state that violates these criteria, should be rated extremely low by the GoalEvalua tor."
            },
            {
              "type": "section",
              "level": 2,
              "id": "D.2.3",
              "title": "Complexity and Convergence Rate",
              "text": "One might wonder why such a complex architecture is necessary. Why not just use, say, Schmid huber’s Godel Machine [?] ? This is an architecture that, in theory, can take an arbitrary goal\nfunction, and figure out how to achieve it in a way that is provably optimal given its current\nknowledge and capabilities – including figuring out how to modify itself so that it can better\nachieve the goal in future, after the modifications take hold. If the specifics of the GOLEM\narchitecture are a good idea, then a Godel Machine should eventually transform itself into a\nGOLEM.\nThe catch, however, lies in the word ”eventually.” Depending on the situation and the com putational resources available, a Godel Machine might take quite a long time to form itself\ninto a GOLEM or something similar. In the real world, while this time is passing, the Godel\nMachine itself could be accidentally doing bad things due to reasoning short-cuts it’s forced to\ntake in order to get actions produced within a reasonable time-frame given its limited resources.\nThe finite but potentially large time-frame that a Godel Machine would take to converge to a\nGOLEM-like state might be a big deal in real-life terms; just as the large constant overhead\ninvolved in simulating a human brain on a 2012 Macbook plus a lot of hard drives, is a big deal\nin practice in spite of its being a triviality from a computing theory perspective.\nThis may seem like hair-splitting, because in order to work at all, GOLEM would also require\na lot of computing resources. The hope with which GOLEM is presented, is that it will be able\nto work with merely a humongous amount of computing resource, rather than, like the Godel\nMachine in its simple and direct form, an infeasible amount of computing resource. This has\nnot been proved and currently remains a tantalizing conjecture."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "D.3",
          "title": "The Argument For GOLEM’s Steadfastness",
          "text": "Our main goal here is to argue that a program with (fixed) GOLEM meta-architecture will be\nsteadfast, in the sense that it will maintain its architecture (or else stop acting) while seeking\nto maximize the goal function implicit in its GoalEvaluator.\nWhy do we believe GOLEM can be steadfast? The basic argument, put simply, is that: If\n\n• the GoalEvaluator and environment together have the property that:\n  – world-states involving conservative meta-architecture preservation tend to have very\n  high fitness\n  – world-states not involving conservative meta-architecture preservation tend to have very\n  low fitness\n  – world-states approximately involving conservative meta-architecture preservation tend\n  to have intermediate fitness\n• the initial Operating Program has a high probability of leading to world-states involving\nconservative meta-architecture preservation (and this is recognized by the GoalEvaluator)\n\nthen the GOLEM meta-architecture will be preserved. Because: according to the nature of the\nmetaprogram, it will only replace the initial Operating Program with another program that is\npredicted to be more effective at achieving the goal, which means that it will be unlikely to\nreplace the current OP with one that doesn’t involve conservative meta-architecture preserva tion.\nObviously, this approach doesn’t allow full self-modification; it assumes certain key parts of\nthe AGI (meta)architecture are hard-wired. But the hard-wired parts are quite basic and leave\na lot of flexibility. So the argument covers a fairly broad and interesting class of goal functions."
        },
        {
          "type": "section",
          "level": 1,
          "id": "D.4",
          "title": "A Partial Formalization of the Architecture and Steadfastness Argument",
          "text": "To partially formalize the above conceptual argument, we will assume the formal agents model\nintroduced earlier.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "D.4.1",
              "title": "Toward a Formalization of GOLEM",
              "text": "We will use the notation [A → B] to denote the space of functions mapping space A to space B.\nAlso, in cases where we denote a function signature via ΦX, we will use X to denote the space\nof all programs embodying functions of that signature; e.g. GE is the space of all functions\nfulfilling the specification given for ΦGE.\nThe GOLEM architecture may be formally defined as follows.\n\n• The Historical Repository Ht is a subset of the history x0−t\n• An Operating Program is a program embodying a function ΨOP : H → A. That is, based\non a history (specifically, the one contained in the Historical Repository at a given point in\ntime), it generates actions\n• A Memory Manager is a program embodying a function so that ΨMM(Ht, xt) = Ht+1\n• A Goal Evaluator is a program embodying a function ΨGE : H → [0, 1]. That is, it maps\nhistories (hypothetical future histories, in the GOLEM architecture) into real numbers\nrepresenting utilities\n• A Goal Evaluator Operating Program is an element of class GE\n• A Searcher is a program embodying a function ΨSR : [P → [0, 1]] → P. That is, it maps\n\"fitness functions\" on program space into programs.\n• A Predictor is a program embodying a function ΨP R : OP × GE × H → [0, 1]\n• A Tester is a program embodying a function ΨT R : P R× H → [0, 1], where the output [0, 1]\nis to be interpreted as the output of the prediction\n• A Metaprogram is a program embodying a function ΨMP : SR × H × P R × T R × GE2 ×\nMM → SR × P R × OP × GE × MM. The GE in the output, and one of the GEs in the\ninput, are GEOPs.\n\nThe operation of the Metaprogram is as outlined earlier; and the effectiveness of the architecture\nmay be assessed as its average level of goal achievement as evaluated by the GE, according to\nsome appropriate averaging measure.\nAs discussed above, a fixed GOLEM assumes a fixed GoalEvaluator, Tester and Metaprogram,\nand a fixed structure for the Historical Repository, and lets everything else adapt. One may\nalso define an adaptive GOLEM variant in which everything is allowed to adapt, and this will\nbe discussed below, but the conceptual steadfastness argument made above applies only to the\nfixed-architecture variant, and the formal proof below is similarly restricted.\nGiven the above formulation, it may be possible to prove a variety of theorems about\nGOLEM’s steadfastness under various assumptions. We will not pursue this direction very far\nhere, but will only make a few semi-formal conjectures, proposing some semi-formal propositions\nthat we believe may result in theorems after more work."
            },
            {
              "type": "section",
              "level": 2,
              "id": "D.4.2",
              "title": "Some Conjectures About GOLEM",
              "text": "The most straightforward cases in which to formally explore the GOLEM architecture are not\nparticularly realistic ones. However, it may be worthwhile to begin with less realistic cases that\nare more analytically tractable, and then proceed with the more complicated and more realistic\ncases.\n\nConjecture D.1. Suppose that\n• The Predictor is optimal (for instance an AIXI type system)\n• Memory management is not an issue: there is enough memory for the system to store all\nits experiences with reasonable access time\n• The GE is sufficiently efficient that no approximative GEOP is needed\n• The HR contains all relevant information about the world, so that at any given time, the\nPredictor’s best choices based on the HR are the same as the best choices it would make\nwith complete visibility into the past of the universe\nThen, there is some time T so that from T onwards, GOLEM will not get any worse at achieving\nthe goals specified in the GE, unless it shuts itself off.\n\nThe basic idea of Conjecture D.1 is that, under the assumptions, GOLEM will replace its\nvarious components only if the Predictor predicts this is a good idea, and the Predictor is\nassumed optimal (and the GE is assumed accurate, and the Historical Repository is assumed\nto contain as much information as needed). The reason one needs to introduce a time T > 0 is\nthat the initial programs might be clever or lucky for reasons that aren’t obvious from the HR.\nIf one wants to ensure that T = 0 one needs some additional conditions:\n\nConjecture D.2. In addition to the assumptions of Conjecture D.1, assume GOLEM’s initial\nchoices of internal programs are optimal based on the state of the world at that time. Then,\nGOLEM will never get any worse at achieving the goals specified in the GE, unless it shuts\nitself off.\n\nBasically, what this says is: If GOLEM starts off with an ideal initial state, and it knows\nvirtually everything about the universe that’s relevant to its goals, and the Predictor is ideal\n– then it won’t get any worse as new information comes in; it will stay ideal. This would be\nnice to know as it would be verification of the sensibleness of the architecture, but, isn’t much\npractical use as these conditions are extremely far from being achievable.\nFurthermore, it seems likely that\n\nConjecture D.3. Suppose that\n• The Predictor is nearly optimal (for instance an AIXItl type system)\n• Memory management is not a huge issue: there is enough memory for the system to store\na reasonable proportion of its experiences with reasonable access time\n• The approximative GEOP is place is very close to accurate\n• The HR contains a large percentage of the relevant information about the world, so that at\nany given time, the Predictor’s best choices based on the HR are roughly same as the best\nchoices it would make with complete visibility into the past of the universe\nThen, there is some time T so that from T onwards, GOLEM is very unlikely to get signifi cantly worse at achieving the goals specified in the GE, unless it shuts itself off.\n\nBasically, this says that if the assumptions of Conjecture D.1 are weakened to approximations,\nthen the conclusion also holds in an approximate form. This also would not be a practically\nuseful result, as the assumptions are still too strong to be realistic.\nWhat might we be able to say under more realistic assumptions? There may be results such\nas\n\nConjecture D.4. Assuming that the environment is given by a specific probability distribution\nµ, let\n• δ1 be the initial expected error of the Predictor assuming µ, and assuming the initial GOLEM\nconfiguration\n• δ2 be the initial expected deviation from optimality of the MM, assuming µ and the initial\nGOLEM configuration\n• δ3 be the initial expected error of the GEOP assuming µ and the initial GOLEM configura tion\n• δ4 be the initial expected deviation from optimality of the HR assuming µ and the initial\nGOLEM configuration\nThen, there are  > 0 and p ∈ [0, 1] so that GOLEM has odds < p of getting worse at achieving\nthe goals specified in the GE by more than , unless it shuts itself off. The values  and p may be\nestimated in terms of the δ values, using formulas that may perhaps be made either dependent\non or independent of the environment distribution µ.\n\nMy suspicion is that, to get reasonably powerful results of the above form, some particular\nassumptions will need to be made about the environment distribution µ – which leads up\nto the interesting and very little explored problem of formally characterizing the probability\ndistributions describing the \"human everyday world.\""
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "D.5",
          "title": "Comparison to a Reinforcement Learning Based Formulation",
          "text": "Readers accustomed to reinforcement learning (RL) approaches to AI [?] may wonder why\nthe complexity of the GOLEM meta-architecture is necessary. Instead of using a \"goal based\narchitecture\" like this, why not just simplify things to\n\n• Rewarder\n• Operating Program\n• Searcher\n• Metaprogram\n\nwhere the Rewarder issues a certain amount of reward at each point in time, and the Metapro gram: invokes the Searcher to search for a program that maximizes expected future reward, and\nthen installs this program as the Operating Program (and contains some parameters balancing\nresource expenditure on the Searcher versus the Operating Program)?\nOne significant issue with this approach is that ensuring conservative meta-architecture\npreservation, based on reward signals, seems problematic. Put simply: in a pure RL approach,\nin order to learn that mucking with its own architecture is bad, the system would need to\nmuck with its architecture and observe that it got a negative reinforcement signal. This seems\nneedlessly dangerous! One can work around the problem by assuming an initial OP that has a\nbias toward conservative meta-architecture preservation. But then if one wants to be sure this\nbias is retained over time, things get complicated. For the system to learn via RL that removing\nthis bias is bad, it would need to try it and observe that it got a negative reinforcement signal.\nOne could try to achieve the GOLEM within a classical RL framework by stretching the\nframework somewhat (RL++ ?) and\n\n• allowing the Rewarder to see the OP, and packing the Predictor and GoalEvaluator into\nthe Rewarder. In this case the Rewarder is tasked with giving the system a reward based\non the satisfactoriness of the predicted outcome of running its Operating Program.\n• allowing the Searcher to query the Rewarder with hypothetical actions in hypothetical\nscenarios (thus allowing the Rewarder to be used like the GoalEvaluator!)\n\nThis RL++ approach is basically the GOLEM in RL clothing. It requires a very smart Rewarder,\nsince the Rewarder must carry out the job of predicting the probability of a given OP giving rise\nto a given world-state. The GOLEM puts all the intelligence in one place, which seems simpler.\nIn RL++, one faces the problem of how to find a good Predictor, which may be solved by\nputting another Searcher and Metaprogram inside the Rewarder; but that complicates things\ninelegantly.\nNote that the Predictor and GoalEvaluator are useful in RL++ specifically because we are\nassuming that in RL++ the Rewarder can see the OP. If the Rewarder can see the OP, it can\nreward the system for what it’s going to do in the future if it keeps running the same OP,\nunder various possible assumptions about the environment. In a strict RL design, the Rewarder\ncannot see the OP, and hence it can only reward the system for what it’s going to do based on\nchancier guesswork. This guesswork might include guessing the OP from the system’s actions\n– but note that, if the Rewarder has to learn a good model of what program the system is\nrunning via observing the system’s actions, it’s going to need to observe a lot of actions to get\nwhat it could get automatically by just seeing the OP. So the learning of the system can be\nmuch, much faster in many cases, if the Rewarder gets to see the OP and make use of that\nknowledge. The Predictor and GoalEvaluator are a way of making use of this knowledge.\nAlso, note that in GOLEM the Searcher can use the Rewarder to explore hypothetical sce narios. In a strict RL architecture this is not possible directly; it’s possible only via the system\nin effect building an internal model of the Rewarder, and using it to explore hypothetical sce narios. The risk here is that the system builds a poor model of the Rewarder, and thus learns\nless efficiently.\nIn all, it seems that RL is not the most convenient framework for thinking about architecture preserving AGI systems, and looking at \"goal-oriented architectures\" like GOLEM makes things\nsignificantly easier."
        },
        {
          "type": "section",
          "level": 1,
          "id": "D.6",
          "title": "Specifying the Letter and Spirit of Goal Systems (Are Both Difficult Tasks)",
          "text": "Probably the largest practical issue arising with the GOLEM meta-architecture is that, given\nthe nature of the real world, it’s hard to estimate how well the Goal Evaluator will do its job!\nIf one is willing to assume GOLEM, and if a proof corresponding to the informal argument\ngiven above can be found, then the \"predictably beneficial\" part of the problem of \"creating\npredictably beneficial AGI\" is largely pushed into the problem of the GoalEvaluator.\nThis makes one suspect that the hardest problem of making predictably beneficial AGI\nprobably isn’t \"preservation of formally-defined goal content under self-modification.\" This\nmay be hard if one enables total self-modification, but it seems it may not be that hard if one\nplaces some fairly limited restrictions on self-modification, as is done in GOLEM, and begins\nwith an appropriate initial condition.\nThe really hard problem, it would seem, is how to create a GoalEvaluator that implements\nthe desired goal content – and that updates this goal content as new information about the\nworld is obtained, and as the world changes – in a way that preserves the spirit of the original\ngoals even if the details of the original goals need to change as the world is explored and better\nunderstood. Because the \"spirit\" of goal content is a very subtle and subjective thing.\nThe intelligent updating of the GEOP, including in the GOLEM design, will not update\nthe original goals, but it will creatively and cleverly apply them to new situations as they\narise – but it will do this according to Occam’s Razor based on its own biases rather than\nnecessarily according to human intuition, except insofar as human intuition is encoded in the\nbase GEOP or the initial Searcher. So it seems sensible to expect that, as unforeseen situations\nare encountered, a GOLEM system will act according to learned GEOPs that are rationally\nconsidered \"in the spirit of the base GEOP\", but that may interpret that \"spirit\" in a different\nway than most humans would. These are subtle issues, and important ones; but in a sense\nthey’re \"good problems to have\", compared to problems like evil, indifferent or wireheaded 2\nAGI systems."
        },
        {
          "type": "section",
          "level": 1,
          "id": "D.7",
          "title": "A More Radically Self-Modifying GOLEM",
          "text": "It’s also possible to modify the GOLEM design so as to enable it to modify the GEOP more\nradically – still with the intention of sticking to the spirit of the base GEOP, but allowing it to\nmodify the \"letter\" of the base GEOP so as to preserve the \"spirit.\" In effect this modification\nallows GOLEM to decide that it understands the essence of the base GEOP better than those\nwho created the particulars of the base GEOP. This is certainly a riskier approach, but it seems\nworth exploring at least conceptually.\nThe basic idea here is that, where the base GEOP is uncertain about the utility of a world state, the \"inferred GEOP\" created by the Searcher is allowed to be more definite. If the base\nGEOP comes up with a probability distribution P in response to a world-state W, then the\ninferred GEOP is allowed to come up with Q so long as Q is sensibly considered a refinement\nof P.\nTo see how one might formalize this, imagine P is based on an observation-set O1 containing\nN observations. Given another distribution Q over utility values, one may then ask: What is\nthe smallest number K so that one can form an observation set O2 containing O1 plus K more\nobservations, so that Q emerges from O2? For instance, if P is based on 100 observations, are\nthere 10 more observations one could make so that from the total set of 110 observations, Q\nwould be the consequence? Or would one need 200 more observations to get Q out of O2?\nGiven an error  > 0, let the minimum number K of extra observations needed to create\nan O2 yielding Q within error , be denoted obs(P, Q). If we assume that the inferred GEOP\noutputs a confidence measure along with each of its output probabilities, we can then explore\nthe relationship between these confidence values and the obs values.\nIntuitively, if the inferred GEOP is very confident, this means it has a lot of evidence about\nQ, which means we can maybe accept a somewhat large obs(P, Q). On the other hand, if the\ninferred GEOP is not very confident, then it doesn’t have much evidence supporting Q, so we\ncan’t accept a very large obs(P, Q).\n\n2 A term used to refer to situations where a system rewires its reward or goal-satisfaction mechanisms to directly\nenable its own maximal satisfaction\n\nThe basic idea intended with a \"confidence measure\" here is that if inferred_geop(W) is\nbased on very little information pertinent to W, then inferred_geop(W).conf idence is small.\nThe Tester could then be required to test the accuracy of the Searcher at finding inferred GEOPs\nwith accurate confidence assessments: e.g. via repeatedly dividing the HistoricalRepository into\ntraining vs. test sets, and for each training set, using the test set to evaluating the accuracy of\nthe confidence estimates produced by inferred GEOPs obtained from that training set.\nWhat this seems to amount to is a reasonably elegant method of allowing the GEOP to\nevolve beyond the base GEOP in a way that is basically \"in the spirit of the base GEOP.\" But\nwith this kind of method, we’re not necessarily going to achieve a long-term faithfulness to the\nbase GEOP. It’s going to be more of a \"continuous, gradual, graceful transcendence\" of the base\nGEOP, it would seem. There seems not to be any way to let the inferred_GEOP refine the\nbase_GEOP without running some serious risk of the inferred_GEOP violating the \"spirit\"\nof the base_GEOP. But what one gets in exchange for this risk is a GOLEM capable of having\ncrisper goal evaluations, moving toward lower-entropy utility distributions, in those cases where\nthe base GEOP is highly uncertain.\nThat is, we can create a GOLEM that knows what it wants better than its creators did – but\nthe cost is that one has to allow the system some leeway in revising the details of its creators’\nideas based on the new evidence it’s gathered, albeit in a way that respects the evidence its\ncreators brought to bear in making the base GEOP."
        },
        {
          "type": "section",
          "level": 1,
          "id": "D.8",
          "title": "Concluding Remarks",
          "text": "What we’ve sought to do here, in this speculative futuristic appendix, is to sketch a novel\napproach to the design of AGI systems that can massively improve their intelligence yet without\nlosing track of their initial goals. While we have not proven rigorously that the GOLEM meta architecture fulfills this specification, have given what seems to be a reasonable, careful informal\nargument, along with some semi-formal conjectures; and proofs along these lines will be pursued\nfor later publications.\nIt’s clear that GOLEM can be wrapped around practical AGI architectures like CogPrime\n– and in that sense GOLEM is a natural extension of the remarks on self-modifying CogPrime\nsystems from Chapter ??. But the major open question is, how powerful do these architectures\nneed to be in order to enable GOLEM to fulfill its potential as a meta-architecture for yielding\nsignificant ongoing intelligence improvement together with a high probability of goal system\nstability. The risk is that the rigors of passing muster with the Tester are sufficiently difficult\nthat the base AGI architecture (CogPrime or whatever) simply doesn’t pass muster, so that\nthe base operating programs are never replaced, and one gets goal-system preservation without\nself-improvement. Neither our theory nor our practice is currently advanced enough to resolve\nthis question, but it’s certainly an important one. One approach to exploring these issues is to\nseek to derive a variant of CogPrime or some other practical AGI design as a specialization of\nGOLEM, rather than trying to study the combination of GOLEM with a separately defined\nAGI system serving as its subcomponent.\nThere is also the open worry of what happens when the system shuts down. Hypothetically,\nif a GOLEM system as described above were in a battle situation, enemies could exploit its\npropensity to shut down when its hardware is compromised. A GOLEM system with this\nproperty would apparently be at a disadvantage in such a battle, relative to a GOLEM system\nthat avoided shutting down and instead made the best possible effort to repair its hardware,\neven if this wound up changing its goal system a bit. So, the particular safety mechanism\nused in GOLEM to prevent dangerous runaway self-improvement, would put a GOLEM at an\nevolutionary disadvantage. If a GOLEM system becomes intelligent before competing systems,\nand achieves massively greater power and intelligence than any competing \"startup\" AGI system\ncould expect to rapidly achieved, then this may be a nonissue. But such eventualities are difficult\nto foresee in detail, and devolve into generic futurist speculation.\nFinally, the dichotomy between the fixed and adaptive GOLEM architectures highlights a\nmajor strategic and philosophical issue in the development of advanced AGI systems more\nbroadly. The fixed GOLEM can grow far beyond humans in its intelligence and understanding\nand capability, yet in a sense, remains rooted in the human world, due to its retention of human\ngoals. Whether this is a positive or negative aspect of the design is a profound nontechnical\nissue. From an evolutionary perspective, one could argue that adaptive GOLEMs will have\ngreater ability to accumulate power due to their fewer limitations. However, a fixed GOLEM\ncould hypothetically be created, with part of its goal system being to inhibit the creation of\nadaptive GOLEMs or other potentially threatening AGI systems. Here however we venture yet\nfurther into the territory of science fiction and speculative futurology, and we will leave further\nsuch discussion for elsewhere."
        }
      ]
    },
    {
      "id": "E",
      "title": "Lojban++: A Novel Linguistic Mechanism for Teaching AGI Systems",
      "content": [
        {
          "type": "section",
          "level": 1,
          "id": "E.1",
          "title": "Introduction",
          "text": "Human “natural language” is unnatural to an AGI program like CogPrime. Yet, understanding\nof human language is obviously critical to any AGI system that wants to interact flexibly in\nthe human world, and/or that wants to ingest the vast corpus of knowledge that humans have\ncreated and recorded. With this in mind, it is natural to explore humanly-unnatural ways of\ngranting AGI systems knowledge of human language; and we have done much of this in the\nprevious appendices, discussing the use of linguistic resources that are clearly different in nature\nfrom the human brain’s in-built linguistic biases. In this appendix we consider yet another\nhumanly-unnatural means of providing AGI systems with linguistic knowledge: the use of the\nconstructed language Lojban (or, more specifically, its variant Lojban++), which occupies an\ninteresting middle ground between formal languages like logic and programming languages, and\nhuman natural languages. We will argue that communicating with AGI systems in Lojban++\nmay provide a way of\n\n• providing AGI systems with experientially-relevant commonsense knowledge, much more\neasily than via explicitly encoding this knowledge in logic\n• teaching AGI systems natural language much more quickly than would otherwise be possi ble, via communicating with AGIs in parallel using natural language and Lojban++\n\nTo put it more precisely: the essential goal of Lojban++ is to constitute a language for effi cient, minimally ambiguous, and user-friendly communications between humans and suitably constructed AI software agents such as CogPrime’s. Another way to think about the Lojban++\napproach is that it allows an AGI learning/teaching process that dissociates, to a certain ex tent, “learning to communicate with humans” from “learning to deal with the peculiarities of\nhuman languages.” Similar to Lojban on which it is based, Lojban++ may also be used for\ncommunication between humans, but this interesting possibility will not be our focus here.\nSome details on the particulars of the Lojban++ language proposal, aimed at readers familiar\nwith Lojban, are given at the end of this appendix. In the initial portions of the appendix we\ndescribe Lojban++ and related ideas at a more abstract level, in a manner comprehensible to\nreaders without prior Lojban background."
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.2",
          "title": "Lojban versus Lojban++",
          "text": "Lojban is itself an outgrowth of another constructed language, Loglan, created by Dr. James\nCooke Brown around 1955 and first widely announced in a 1960 Scientific American article\n[?]. Loglan is still under development but now is not used nearly as widely as Lojban. First\nseparated from Loglan in 1987, Lojban is a constructed language that lives at the border between\nnatural language and computing language. It is a “natural-like language” in that it is speakable\nand writeable by humans and may be used by humans to discuss the same range of topics as\nnatural languages. Lojban has a precise, specified formal syntax that can be parsed in the same\nmanner as a programming language, and it has a semantics, based on predicate logic, in which\nambiguity is carefully controlled. Lojban semantics is not completely unambiguous, but it is far\nless ambiguous than that of any natural language, and the careful speaker can reduce ambiguity\nof communication almost to zero with far less effort than in any natural language. On the other\nhand, Lojban also permits the speaker to utilize greater ambiguity when this is desirable in\norder to allow compactness of communication.\nMany individuals attempting to learn and use Lojban have found, however, that it has two\nlimitations. The Lojban vocabulary is unfamiliar and difficult to learn - though no more so than\nthat of any other language belonging to a language family unfamiliar to the language learner.\nAnd, more seriously, the body of existing Lojban vocabulary is limited compared to that of\nnatural languages, making Lojban communication sometimes slow and difficult. When using\nLojban, one must sometimes pause to concoct new words (according to the Lojban principles\nof word construction), which can be fun, but is much like needing to stop over and over to build\nnew tools in the context of using one’s toolkit to build something; and is clearly not optimal\nfrom the perspective of teaching AGI systems.\nTo address these issues, Lojban++ constitutes a combination of Lojban syntax and Lojban\nvocabulary, extended with English vocabulary. So in a very rough sense, it may perhaps be\nunderstood as a pidgin of Lojban and English. Lojban++ is less elegant than Lojban but\nsignificantly easier to learn, and much easier to use in domains to which Lojban vocabulary has\nnot yet been extended. In short, the goal of Lojban++ is to combine the mathematical precision\nand pragmatic ontology that characterize Lojban, with the usability of a natural language like\nEnglish with its extensive vocabulary.\nAn extensive formal treatment of Lojban grammar has been published [?], and while there is\nno published hard-copy Lojban dictionary, there is a website jbovlaste.lojban.org/ that\nserves this purpose and which is frequently updated as new coinages are created and approved\nby the Logical Language Group, a standing body charged with the maintenance of the language.\nAlthough Lojban has not been adopted nearly as widely as Esperanto (an invented language\nwith several hundred thousand speakers), the fact that there is a community of several hundred\nspeakers, including several dozen who are highly fluent at least in written Lojban, is important.\nThe decades of communicative practice that have occurred within the Lojban community have\nbeen invaluable for refining the language. This kind of practice buys a level of maturity that\ncannot be obtained in a shorter period of time via formal analysis or creative invention. For\nexample, the current Lojban treatment of quantifiers is arguably vastly superior to that of\nany natural language [?], but that was not true in 1987 when it excelled more in mathematical\nprecision than in practical usability. The current approach evolved through a series of principled\nrevisions suggested from experience with practical conversation in Lojban. Any new natural-like\nlanguage that was created for human-CogPrime or CogPrime -CogPrime communication would\nneed to go through a similar process of iterative refinement through practical use to achieve a\nsimilar level of usability."
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.3",
          "title": "Some Simple Examples",
          "text": "Now we give some examples of Lojban++. While these may be somewhat opaque to the reader\nwithout Lojban experience, we present them anyway just to give a flavor of what Lojban++\nlooks like; it would seem wrong to leave the discussion purely abstract.\nConsider the English sentence,\nWhen are you going to the mountain?\nWhen written in Lojban, it looks like:\ndo cu’e klama le cmana\nIn Lojban++, with the judicious importation of English vocabulary, it takes a form more\nrecognizable to an English speaker:\nyou cu’e go le cmana\nA fairly standard predicate logic rendition of this, derived by simple, deterministic rules from\nthe Lojban++ version, would be\natTime(go(you, mountain), ?X)\nNext, consider the more complex English sentence,\nWhen are you going to the small obsidian mountain?\nIn Lojban, there is no word for obsidian, so one needs to be invented (perhaps by compounding\nthe Lojban words for “glass” and “rock,” for example), or else a specific linguistic mechanism\nfor quoting non-Lojban words needs to be invoked. But in Lojban++ one could simply say,\nyou cu’e go le small obsidian mountain\nThe construct “small obsidian mountain” is what is called a Lojban tanru, meaning a compound\nof words without a precisely defined semantics (though there are recognized constraints on tanru\nsemantics based on the semantics of the components [?]). Alternatively, using the Lojban word,\nmarji, which incorporates explicit place structure (x1= material/stuff/matter of composition\nx2), a much less ambiguous translation is achieved:\nyou cu’e go le small mountain poi marji loi obsidian\nin which “poi marji loi obsidian” means “that is composed of [a mass of] obsidian.” This illustrates\nthe flexible ambiguity achievable in Lojban. One can use the language in a way that minimizes\nambiguity, or one can selectively introduce ambiguity in the manner of natural languages, when\ndesirable.\nThe differences between Lojban and Lojban++ are subtler than it might appear at first. It\nis key to understand hat Lojban++ is not simply a version of Lojban with English character sequences substituted for Lojban character-sequences. A critical difference lies in the rigid,\npre-determined argument structures associated with Lojban words. For instance, the Lojban\nphrase\nklama fi la .atlantas. fe la bastn. fu le karce\ncorresponds to the English phrase\nthat which goes from Atlanta to Boston by car\nTo say this in Lojban++ without using “klama” would require\ngo fi’o source Atlanta fi’o destination Boston fi’o vehicle car\nwhich is much more awkward. On the other hand, one could also avoid the awkward Lojban\ntreatment of English proper nouns and say\nklama fi la Atlanta fe la Boston fu le car\nor\nklama fi la Atlanta fe la Boston fu le karce\nIt’s somewhat a matter of taste, but according to ours, the latter most optimally balances\nsimplicity with familiarity. The point is that the Lojban word “klama” comes with the convention\nthat its second argument (indexed by “fi”) refers to the source of the going, its third argument\n(indexed by “fe”) refers to the destination of the going, and its fifth argument (indexed by “fu”)\nrefers to the method of conveyance. No such standard argument-structure template exists in\nEnglish for “go”, and hence using “go” in place of “klama” requires the use of the “fi’o” construct\nto indicate the slot into which each of the arguments of “go” is supposed to fall.\nThe following table gives additional examples, both in Lojban and Lojban++.\nEnglish | I eat the salad with croutons\nLojban  | mi citka le salta poi mixre lo sudnabybli\nLojban++| mi eat le salad poi mixre lo crouton\n        | mi eat le salad poi contain lo crouton\nEnglish | I eat the salad with a fork\nLojban  | mi citka le salta sepi’o lo forca\nLojban++| mi eat le salad sepi’o lo fork\nEnglish | I will drive along the road with the big trees\nLojban  | mi litru le dargu poi lamji lo barda tricu\nLojban++| mi ba travel fi’o vehicle lo car fi’o route le road poi adjacent lo so’i big tree\n        | mi ba litru fi lo car fe le road poi adjacent lo so’i big tree\n        | mi ba drive fi’o route le road poi adjacent lo so’i big tree\nEnglish | I will drive along the road with great care\nLojban  | mi litru le dargu ta’i lo nu mi mutce kurji\nLojban++| mi ba drive fi’o route le road ta’i lo nu mi much careful\n        | mi ba litru le road ta’i lo nu mi much careful\nEnglish | I will drive along the road with my infrared sensors on\nLojban  | mi ba litru le dargu lo karce gi’e pilno le miktrebo’a terzga\nLojban++| mi litru le road lo car gi’e use le infrared sensor\n        | mi litru le road lo car gi’e pilno le infrared te zgana\n        | mi drive fi’o vehicle lo car fi’o route le road gi’e use le infrared sensor\nEnglish | I will drive along the road with the other cars\nLojban  | mi litru le dargu fi’o kansa lo drata karce\nLojban++| mi ba drive fi’o route le road fi’o kansa lo so’i drata car\n        | mi ba drive fi’o route le road fi’o with lo so’i drata car\n        | mi ba litru le road fi’o kansa lo so’i drata car"
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.4",
          "title": "The Need for Lojban Software",
          "text": "In order that Lojban++ be useful for human-CogPrime communications, parsing and semantic\nmapping software need to be produced for the language, building on existing Lojban software.\nThere is a fully functional Lojban parser based on a parsing expression grammar (Powell, no\ndate specified), as well as an earlier parser based on BNF grammar. (And, parenthetically, the\nobservation that Lojban is more conveniently formulated in PEG (Parsing Expression Gram mar) form is in itself a nontrivial theoretical insight.) The creation of a Lojban++ parser based\non the existing Lojban parser, is a necessary and a relatively straightforward though not trivial\ntask.\nOn the other hand, no software has yet been written for formal semantic interpretation\n(“semantic mapping”) of Lojban expressions - which is mainly because Lojban has primarily\nbeen developed as an experimental language for communication between humans rather than as\na language for human-CogPrime communication. Such semantic mapping software is necessary\nto complete the loop between humans and AI reasoning programs, enabling powerful cognitive\nand pragmatic interplay between humans and CogPrime’s. For Lojban++ to be useful for\nhuman-CogPrime interaction, this software must be created and must go in both directions:\nfrom Lojban++ to predicate logic and back again. As Lojban++ is a superset of Lojban,\ncreating such software for Lojban++ will automatically include the creation of such software\nfor Lojban proper.\nThere promises to be some subtlety in this process, but not on the level that’s required to\nsemantically map human language. What is required to connect a Lojban++ parser with the\nRelEx NLP framework as described in Chapter ?? is essentially a mapping between\n\n• the Lojban cmavo (structure word) and the argument-structure of lojban gismu (root word)\n• FrameNet frame-elements, and a handful of other CogPrime relationships (e.g. for dealing\nwith space, time and inheritance)\n\nThese mappings must be built by hand, which should be time-consuming, but on the order of\nman-weeks rather than man-years of effort.1 Once this is done, then Lojban++ can be entered\ninto CogPrime essentially as English would be if the RelEx framework worked perfectly. The\ndifficulties of human language processing will be bypassed, though still – of course – leaving the\ndifficulties of commonsense reasoning and contextual interpretation.\nFor example, the Lojban root word klama is defined as\nx1 comes/goes to destination x2 from origin x3 via route x4 using means/vehicle x5.\nThis corresponds closely to the FrameNet frame Motion, which has elements\n\n1 Carrying out the following mapping took a few minutes, so carrying out similar mappings for 800 FrameNet\nframes should take no more than a couple weeks of effort."
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.5",
          "title": "Lojban and Inference",
          "text": "Both Lojban and Lojban++ can be straightforwardly translated into predicate logic format\n(though the translation is less trivial in the case of Lojban++, as a little bit of English-word\ndisambiguation must be done). This means that as soon as Lojban++ semantic mapping soft ware is constructed, it will almost immediately be possible for CogPrime systems to reason about\nknowledge communicated to them in Lojban. This aspect of Lojban has already been explored\nin a preliminary way by Speer and Havasi’s [?] JIMPE software application, which involves a\nsemantic network guiding logical reasoning, Lojban parsing and Lojban language production.\nWhile JIMPE is a relatively simplistic prototype application, it is clear that more complex\nexample of Lojban-based artificial inference are also relatively straightforwardly achievable via\na conceptually similar methodology.\nAn important point to consider in this regard is that Lojban/Lojban++ contains two distinct\naspects:\n\n1. an ontology of predicates useful for representing commonsense knowledge (represented by\nthe Lojban cmavo along with the most common Lojban content words)\n2. a strategy for linearizing nested predicates constructed using these cmavo into human pronounceable and -readable strings of letters or phonemes.\n\nThe second aspect is of no particular value for inference, but the first aspect is. We suggest that\nthe Lojban++ ontology provides a useful framework for knowledge representation that may be\nincorporated at a fundamental level into any AI system that centrally utilizes predicate logic\nor a similar representation. While overlapping substantially with FrameNet, it has a level of\ncommonsensical completeness that FrameNet does not, because it has been refined via practice\nto be useful for real-world communication. Similarly, although it is smaller than Cyc, it is more\njudiciously crafted. Cyc contains a lot of knowledge not useful for everyday communication,\nyet has various lacunae regarding the description of everyday objects and events – because no\ncommunity has ever seriously tried to use it for everyday communication.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "E.5.1",
              "title": "Lojban versus Predicate Logic",
              "text": "In the context of Lojban++ and inference, it is interesting to compare Lojban++ formulations\nwith corresponding predicate logic formulations. For example, consider the English sentence\nHey, I just saw a bunch of troops going into the woods. What do you want me to do?\ntranslates into the Lojban\nju’i do’u mi pu zi viska lo nu so’i lo sonci cu nenkla le ricfoi .i do djica lo nu mi mo\nor the Lojban++\nHey do’u mi pu zi see lo nu so’i lo soldier cu enter le forest .i do want lo nu mi mo\nwhich literally transcribed into English would be something like\nHey! [vocative terminator] I [past] [short time] see an event of (many soldiers enter forest).\nYou want event (me what?)\nOmitting the “hey,” a simple and accurate predicate logic rendition of this sentence would be\npast($X) ∧ short_time($X) ∧ ($X = see(me, $Y ))∧\n($Y = event(enter($Z, forest))) ∧ soldier($Z) ∧ many($Z)∧\nwant(you, event(?W(me))\nwhere ?W refers to a variable being posed as a question be answered, and X and so forth\nrefer to internal variables. The Lojban and Lojban++ versions have the same semantics as the\npredicate logic version, but are much simpler to speak, hear and understand due to the lack of\nexplicit variables."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.6",
          "title": "Discussion",
          "text": "Hopefully the above exposition of Lojban++, though incomplete, was sufficient to convince you\nthat teaching “infant-level” or “child-level” AGIs about the world using Lojban++ would be sig nificantly easier than teaching doing so using English or other natural languages. The question\nthen is whether this difference makes any difference. One could counter-argue that, if an AGI\nwere smart enough to really learn to interpret Lojban++, then it would be smart enough to\nlearn to interpret English as well, with only minor additional effort. In sympathy with this\ncounter-argument is the fact that successfully mapping Lojban++ utterances into predicate\nlogic expressions, and representing these predicate logic expressions in an AI’s knowledge base,\ndoes not in itself constitute any serious “understanding” of the Lojban++ utterances on the\npart of the AI system. However, this counter-argument ignores the “chicken and egg problem” of\ncommon-sense knowledge and language understanding. If an AGI understands natural language\nthen it can be taught human common-sense knowledge via direct linguistic instruction. On the\nother hand, it is also clear that a decent amount of common-sense knowledge is a prerequi site for adequate natural language understanding (for such tasks as parse selection, semantic\ndisambiguation and reference resolution, for example). One response to this is to appeal to feed back, and argue that common-sense knowledge and linguistic understanding are built to arise\nand grow together. We believe this is largely true, and yet that there may also be additional\ndynamics at play in the developing human mind that accelerate the process, such as inbuilt in ductive biases regarding syntax. In an AGI context, one way to accelerate the process may be to\nuse Lojban++ to teach the young AGI system commonsense knowledge, which then may help it\nto more easily penetrate the complexities and ambiguities of natural language. This assumes, of\ncourse, that the knowledge gained by the system from being instructed in Lojban++ is genuine\nknowledge rather than merely empty, ungrounded tokens. For this reason, we suggest, one vi able learning project may be to teach an AGI system using Lojban++ in the context of shared\nembodied experience in a real or simulated environment. This way Lojban++ expressions may\nbe experientially grounded and richly understood, potentially allowing commonsense knowledge\nto form in an AGI’s knowledge base, in a way that can be generalized and utilized to aid with\nvarious learning tasks including learning natural language. Another interesting teaching strat egy may be to present an AGI system with semantically roughly equivalent English and Lojban\nsentences, especially ones that are pertinent to the system’s experiences. Since the system can\ninterpret the Lojban sentences with minimal ambiguity (especially by using the experienced\ncontext to reduce any ambiguities remaining after parsing, due to tanru), it will then know the\ncorrect interpretation of the English sentences, which will provide it with very helpful “training\ndata” that it can then generalize from to help it understand other English sentences."
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.7",
          "title": "Postscript: Basic Principles for Using English Words in Lojban++",
          "text": "This section reviews the key principles by which Lojban++ incorporates English words into\nLojban, and discusses some other small additions that Lojban++ makes to Lojban. It is in tended mainly for readers who are familiar with Lojban. A note is perhaps appropriate here\nregarding the right approach to learning Lojban++ at present. Lojban++ is a variant of Lo jban, and no systematic teaching materials for Lojban++ yet exist. Therefore, at the moment\nthe only way to learn Lojban++ is to learn the basics of Lojban, and then learn the differences\n(note however, that the “basics of Lojban” as meant here does not necessarily include a broad\nmastery of Lojban vocabulary beyond the cmavo or “structure words”). Assuming Lojban++\nis used for teaching AGI systems as proposed here, relevant teaching materials should also be\ndeveloped. There is no need to write a book-length grammar for Lojban++ comparable to\n[?], however, since the prinicples of Lojban++ grammar are all drawn from Lojban. Finally, a\nnecessary caveat: Lojban++ is not yet refined through practice, so it should be assumed that\nthe specifics described in this appendix are likely to be subjected to change through experience,\nas the language is used and developed. This list of principles will likely be extended and refined\nthrough usage.\n\n1. Content words only! English words that are about syntactic relationship have no place in\nLojban++.\n2. No “being verbs” or “helping verbs.” The English “is” and its conjugations have no place in\nLojban++, for example.\n3. All Lojban++ cares about is the main part of an English word. None of the English mark ers for tense, person or number should be used, when importing an English word into\nLojban++. For instance, English verbs used must be in the infinitival form; English nouns\nmust be used in the singular form. For instance, “run” not “runs” or “ran”; “pig” not “pigs.”\n4. English adverbs are not used except in rare cases where there is no adjectival form; where\nthere is an adjectival form it is used instead – e.g. “scary” not “scarily.”\n\nTo lapse into Lojban lingo, English words must be used in Lojban++ as brivla. Tense, number\nand so forth are supposed to be added onto these brivla using the appropriate Lojban cmavo.\nThe Lojban++ parser will assume that any non-Lojban word encountered, if not specifically\nflagged as a proper name (by the cmavo “la”), is an English word intended to be interpreted as\na brivla. It will not do any parsing of the word to try to interpret tense, number, adverbiality,\netc. Next, English idiomatic collocations, if used in written Lojban++, should be used with an\nunderscore between the component words. For example: New_York, run_wild, big_shot, etc.\nWithout the underscore, the Lojban++ parser will assume that it is seeing a tanru (so that\ne.g. “big shot” is a type of “shot” that is modified by “big”). In spoken Lojban, the formally\ncorrect thing is to use the new cmavo “quay” to be discussed below; but in practice when\nusing Lojban++ for human-human communication this may often be omitted. Finally, a less\nformal guideline concerns the use of highly ambiguous English words, the use of obscure senses\nof English words, and the use of English words in metaphorical senses. All of these should be\navoided. They won’t confuse the Lojban++ parsing process, but they will confuse the Lojban++\nsemantic mapping process. If a usage seems like it would confuse an AI program without much\nhuman cultural experience, then try to avoid it. Don’t say\nyou paint ti\nto mean “paint” in the sense of “portray vividly”, when you could say\nyou cu vivid bo describe ti\nThe latter will tell an AI exactly what’s happening; the former may leave the AI wondering\nwhether what’s being depicted is an instance of description, or an instance of painting with an\nactual paintbrush and oils. Similarly, to say\nyou kill me\nwhen you mean\nyou much amuse me\nis not in the Lojban++ spirit. Yes, an AI may be able to figure this out by reference to\ndictionaries combined with contextual knowledge and inference, but the point of Lojban++ is to\nmake communication simple and transparent so as to reduce the possibility for communication\nerror."
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.8",
          "title": "Syntax-based Argument Structure Conventions for English Words",
          "text": "Next, one of the subtler points of Lojban++ involves the automatic assignment of Lojban\nargument-structures to English words. This is done via the following rules:\n\n1. Nouns are interpreted to have one argument, which is interpreted as a member of the\ncategory denoted by the noun\n  a. la Ben human\n2. Adjectives/adverbs are taken to have two arguments: the first is the entity modified by the\nadjective/adverb, the second is the extent to which the modification holds\n  a. la Ben fat le slight\n3. Intransitive verbs are interpreted to have at least one argument, which is interpreted as the\nargument of the predicate represented by the verb\n  a. le cockroach die\n4. Transitive verbs are interpreted to have at least two arguments, the subject and then the\nobject\n  a. la Ben kill le cockroach\n5. Ditransitive verbs are interpreted to have three arguments, and conventions must be made\nfor each of these cases, e.g.\n  a. give x y z may be interpreted as “x give y to z”\n    i. la Ben give le death le cockroach\n  b. take x y z may be interpreted as “x takes y from z”\n    i. la Ben take le life le cockroach\n\nA rule of thumb here is that the agent comes first, the recipient comes last, and the object\ncomes inbetween."
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.9",
          "title": "Semantics-based Argument Structure Conventions for English Words",
          "text": "The above syntax-based argument-structure conventions are valuable, but not sufficiently\nthorough to allow for fluent Lojban++ usage. For this reason a collection of semantics based argument-structure conventions have been created, based mostly on porting argument structures from related Lojban words to English vocabulary. The following list is the current\nworking version, and is likely to be extended a bit during actual usage.\n\n1. Plant or animal (moss, cow, pig)\n  a. x1 is a W of species x2\n2. Spatial relation (beneath, above, right, left)\n  a. x1 is in relation W to x2, in reference frame x3\n3. Dimension-dependent spatial descriptor (narrow, deep, wide, etc.)\n  a. x1 is W in dimension x2, relative to standard x3\n4. Unit (foot, hour, meter, mile)\n  a. x1 is x2 W’s by standard x3\n5. Kinship or other interpersonal relationship (mother, father, uncle, boss)\n  a. x1 is the W of x2\n6. Thought-action (remember, think, intuit, know)\n  a. x1 W’s x2\n  b. x1 W’s x2 about x3\n7. Creative product (poem, painting, book)\n  a. x1 is a W about plot/theme/subject/pattern x2 by author x3 for intended audience x4\n8. Physical action undertaken by one agent on another (touch, kick, kiss)\n  a. x1 (agent) W’s x2 with x3 [a locus on x1 or an instrument] at x4 [a locus on x2]\n9. W denotes a type of substance, e.g. mush, paste, slime\n  a. x1 is a W composed of x2\n10. Instance of communication (ask, tell, command)\n   a. x1 W’s x2 with information content x3\n11. Type of utterance (comment, question)\n   a. x1 (text) is a W about subject x2 expressed by x3 to audience x4\n12. Type of movement (walking, leaping, jumping, climbing)\n   a. x1 (agent/object) W’s to x2 from x3 in direction x4\n13. Route, path, road, trail, etc.\n   a. x1 is a W to x2 from x3 via/defined by points including x4 (set)\n14. Nationality, culture etc.\n   a. x1 reflects W in aspect x2\n15. Type of event involving humans or other social agents (celebration, meeting, funeral)\n   a. x1 partakes, with purpose x2, in event x3 of type W\n16. Posture or mode of physical activity of an embodied agent (stand, sit, lie, stoop)\n   a. x1 W’s on surface x2\n17. Type of mental construct (idea, thought, dream, conjecture, etc.)\n   a. x1 is a W about x2 by mind x3\n18. Type of event done by someone, potentially to someone else (accident, disaster, injury)\n   a. x1 is a W done by x2 to x3\n19. Comparative amount (half, third, double, triple)\n   a. x1 is W of x2 in quality x3\n20. Relation between an agent and a statement (assert, doubt, refute, etc.)\n   a. x1 W’s x2\n21. Spatial relationship (far, near, close)\n   a. x1 is W from x2 in dimension x3\n22. Human emotion (happy, sad, etc.)\n   a. x1 is W about x2\n23. A physically distinct part of some physical object, including a body part\n   a. x1 is a W on x2\n24. Type of physical transformation (e.g. mash, pulverize, etc.)\n   a. x1 [force] W’s x2 into mass x3\n25. Way of transmitting an object (push, throw, toss, fling)\n   a. x1 W’s object x2 to/at/in direction x3\n26. Relative size indicator (big, small, huge)\n   a. x1 is W relative to x2 by standard x3"
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.10",
          "title": "Lojban gismu of clear use within Lojban++",
          "text": "There are some Lojban gismu (content words) which are clearly much more useful within\nLojban++ than their English counterparts. Mostly this is because their argument structures\ninvolve more than two arguments, but occasionally it is because they involve a two-argument\nstructure that happens not to be well-captured by any English word (but is usually represented\nin English by a more complex construct involving one or more prepositions).\nA list of roughly 300 gismu currently judged to be “essential” in this sense is at http://\nwww.goertzel.org/papers/gismu_essential.txt, and a list of less than 50 additional\ngismu judged potentially very useful but not quite so essential is at urlhttp://www.goertzel.org/papers/gismu_useful"
        },
        {
          "type": "section",
          "level": 1,
          "id": "E.11",
          "title": "Special Lojban++ cmavo",
          "text": "Next, there are some special cmavo (structure words) that are useful in Lojban++ but not\npresent in ordinary Lojban. A few more Lojban++ cmavo may be added as a result of practical\nexperience communicating using Lojban++; but these are it, for now.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "E.11.1",
              "title": "qui",
              "text": "Pronounced “kwee”, this is a cmavo used in Loglish to create words with unambiguous senses,\nas in the example:\npig qui animal\npig qui cop\nThe second English word in the compound is a sense-specifier. Generally this should only be\nused where the word-sense intended is not the one that would be most obviously expected given\nthe context.\nIn some rare cases one might want two modifiers, using the form\n(English word) qui (English word) qui (English word)"
            },
            {
              "type": "section",
              "level": 2,
              "id": "E.11.2",
              "title": "it, quu",
              "text": "The basic idea is that there is one special referential word in Lojban++ – “it” – which goes\nalong with a reference-target-indicator “quu” (pronounced “kwuhh”) which gives a qualitative\nindication of the referent of a given instance of “it,” intended to narrow down the scope of the\nreference resolution process.\nFor instance, you could say\nla Dr. Benjamin Goertzel cu proceed le playground. It quu man cu kill le dog. It cu eat le cat.\nIn this case, “it” is defined to refer to “Dr. Benjamin Goertzel”, not to “man” generically. The\n“man” qualifier following the “quu” is intended to merely guide the listener’s mind toward the\nright antecedent for the pronoun. It’s not intended to explicitly define the pronoun. So, basically\nit quu male\nis the rough equivalent of the English “he”, and\nit quu female\nis the rough equivalent of the English “she”\nhim/her/they\nFinally, for sake of usability, it is worthwhile within Lojban++ to introduce the following\nshorthands\n\n• him → it quu male\n• her → it quu female\n• ver → it quu person\n• they → it quu people\n\n(Note that “him” in Lojban++ thus plays the role of both “him” and “he” in English.)"
            },
            {
              "type": "section",
              "level": 2,
              "id": "E.11.3",
              "title": "quay",
              "text": "Pronounced “kway,” this cmavo separates parts of an English collocation in speech, e.g.\nbig quay shot\nIt may often be omitted in informal speech; and in writing may be replaced by an underscore\n(big_shot)."
            }
          ]
        }
      ]
    },
    {
      "id": "F",
      "title": "PLN and the Brain",
      "content": [
        {
          "type": "section",
          "level": 1,
          "id": "F.1",
          "title": "How Might Probabilistic Logic Networks Emerge from Neural Structures and Dynamics?",
          "text": "In this appendix, we digress briefly to explore how PLN constructs like inheritance and similarity\nrelationships might emerge from brainlike structures like cell assemblies and neural activation\npatterns. This is interesting as speculative neuroscience, and also potentially valuable in the\ncontext of hybrid architectures, in terms of tuning the interrelationship between CogPrime’s\nAtomspace and neural net like systems such as DeSTIN. If nothing else, the ideas of this\nsection serve as a conceptual argument why it makes sense to interface PLN representations\nand dynamics with CSDLN representations and dynamics. While conventionally formalized and\ndiscussed using different languages, these different approaches to knowledge and learning are\nactually not so far off as is commonly believed.\nWe restrict ourselves here to FOPLN, which does not involve explicit variables or quantifiers,\nand may be described as the logic of uncertain inheritance relationships. As in PLN higher order logic reduces to first-order logic, this is actually all we need to deal with. A neural\nimplementation of higher-order PLN follows from a neural representation of FOPLN plus a\nneural representation of higher-order functions such as the one suggested in Chapter ?? of Part\n1.\nAs described above, the semantics of the term logic relationship “A inherits from B” or\nA → B, is that when B is present, A is also present. The truth value of the relationship\nmeasures the percentage of the times that B is present, that A is also present. “A is similar to\nB” or A ↔ B, is a symmetrical version, whose truth value measures the percentage of the times\nthat either one is present, that both are present. These are the relations we will deal with here.\nHow can this be tied in with the brain? Suppose we have two assemblies A1 and A2, and\nthese are activated in the brain when the organism is presented with stimuli in category C1 and\nC2 respectively (to take the simplest case of concepts, i.e. perceptual categories). Then, we may\nsay that there is a neural inheritance A1 → A2, whose probabilistic strength is the number w\nso that\nP(A1’s mean activation > T at time t) ∗ w\nbest approximates\nP(A2’s mean activation > T at time t + ε)\nfor an appropriate choice of ε and T. This weight w, intuitively, represents the conditional\nprobability P(A2 is active|A1 is active).\nIn a similar way, if we have two assembly activation patterns P1 and P2, which are defined\nas specific types of activity patterns occurring in A1 and A2 respectively, and which correspond\nto categories C1 and C2 respectively, we can define a neural inheritance P1 → P2, whose\nprobabilistic truth value is the number w so that P(P1 is present in A1 at time t) ∗ w best\napproximates P(P2 is present in A2 at time t + ε), on average over various times t, and\nassuming a threshold T used to determine when a pattern is present in an assembly.\nIt is immediate that, if there is a virtual synapse between A1 and A2 or P1 and P2, there will\nalso be a neural inheritance there. Furthermore there will be a monotone increasing algebraic\nrelationship between the weight of the virtual synapse and the probability attached to the\nneural inheritance. Inhibitory virtual synapses will correspond to very low link probabilities;\nexcitatory virtual synapses will correspond to high link probabilities.\nHowever, we can have neural inheritance without any virtual synapse. This is a key point,\nas it lets us distinguish neural inheritance relationships that are explicit (that correspond to\nvirtual synapses) from those that are implicit (that do not). And this leads us directly to prob abilistic reasoning, which is about transforming implicit inheritance relationships into explicit\nones. The fundamental inference rules of term logic, as described above, create new inheritance\nlinks from old ones. The conclusions are implicit in the premises but until the inference is done,\nthey may not be explicitly contained in the knowledge base of the system doing the reason ing. Probabilistic reasoning in the brain, we suggest, is all about translating implicit neural\ninheritance relationships into explicit ones.\nIn first-order PLN the basic forms of inference are: revision (which in its simplest form is\nweighted-averaging), deduction, and inversion (which reverses the direction of a link, and in\nprobabilistic term logic is essentially Bayes rule). Let us elaborate what these mean in terms of\ncell assemblies.\nSuppose that A1 and A2 are two assemblies, and there is a neural inheritance between A1\nand A2. Then, there will also be a neural inheritance between A2 and A1, with a truth value\ngiven by Bayes rule. And according to Hebbian learning if there is a virtual synapse A1 → A2,\nthere is likely to grow a virtual synapse A2 → A1. And according to the approximate correlation\nbetween virtual synapse weight and neural inheritance probability, this new virtual synapse from\nA2 → A1 will have a weight corresponding to a probability approximating the one corresponding\nto the weight of A1 → A2.\nSimilarly, suppose that A1, A2 and A3 are three assemblies. Then, if we have virtual synapses\nbetween A1 → A2 and A2 → A3, Hebbian learning suggests that a virtual synapse will grown\nbetween A1 → A3. And what will the probability of the neural inheritance corresponding to\nthis virtual synapse be? On average, it will be the probability one obtains by assuming the\nprobabilities associated with A1 → A2 and A2 → A3 are independent. But, this means that\non average, the probability associated with A1 → A3 will accord with the value produced by\nthe PLN deduction formula, which embodies precisely this independence assumption. Here, we\nhave an additional source of error beyond what exists in the Bayes rule case; but, in the mean,\nthe desired correspondence does hold.\nSo, according to the above arguments – which admittedly have been intuitive rather than\nmathematically rigorous – it would seem that we can build term logic inference between con cepts out of Hebbian learning between neurons, if we assume cell assembly based knowledge\nrepresentation, via introducing the conceptual machinery of virtual synapses and neural inher itance."
        },
        {
          "type": "section",
          "level": 1,
          "id": "F.2",
          "title": "Avoiding Issues with Circular Inference",
          "text": "When one develops the ideas from the previous section, connecting uncertain term logic inference\nwith neurodynamics, in more detail, only one possible snag arises. Existing computational\nframeworks for uncertain term logic inference utilize special mechanisms for controlling circular\ninference, and these mechanisms have no plausible neurological analogues. In this section we\nexplore this issue and argue that it’s not necessarily a big deal. In essence, our argument is that\nthese biologically unnatural circularity-avoidance mechanisms are unnecessary in a probabilistic\nterm logic system whose operations are guided by appropriate adaptive attention-allocation\nmechanisms. It’s only when operating probabilistic term logic inference in isolation, in a manner\nthat’s unnatural for a resource-constrained intelligent system, that these circular-inference issues\nbecome severe.\nWe note however that this conclusion seems to be specific to probabilistic term logic, and\ndoesn’t seem to hold for NARS term logic, in which the circular inference problem may be more\nsevere, and may in fact require a trail mechanism more strongly. We have not investigated this\nissue carefully.\nTo understand the circular inference problem, look at the triangles in Figure ??. It’s easy\nto see that by performing deduction, induction and abduction in sequence, we can go around\nand around an inference triangle forever, combining the links in different orders, inferring each\nlink in the triangle from the two others in different orders over and over again. What often\nhappens when you do this in a computer program performing uncertain term logic inference,\nhowever, is that after long enough the inference errors compound, and the truth values descend\ninto nonsense. The solution taken in the NARS and PLN uncertain term logic inference engines\nis something called inference trails. Basically, each inheritance link maintains a trail, which is\na list of the nodes and links used as premises in inferences determining its truth value. And a\nrule is put in place that the link L should not be used to adjust the truth value of the link M\nif M is in L’s trail.\nTrails work fine for computer programs implementing uncertain term logic, though managing\nthem properly does involve various complexities. But, from the point of view of the brain, trails\nseem quite unacceptable. It would seem implausible to hypothesize that the brain somehow\nstores a trail along with each virtual synapse. The brain must have some other method of\navoiding circular inferences leading to truth value noisification.\nIn order to explore these issues, we have run a number of experiments with trail-free prob abilistic inference. The first of these involved doing inferences on millions of nodes and links\n(with nodes representing words and links derived via word co-occurrence probabilities across a\ntext corpus). What we found was that, in practice, the severity of the circular-inference prob lem depended on the inference control strategy. When one implemented a strategy in which the\namount of attention devoted to inference about a link L was proportional to an estimate of the\namount of information recently gained by doing inference about L, then one did not run into\nparticularly bad problems with circular inference. On the other hand, if one operated with a\nsmall number of nodes and links and repeatedly ran the same inferences over and over again on\nthem, one did sometimes run into problems with truth value degeneration, in which the term\nlogic formulas would cause link strengths to spuriously converge to 1 or 0.\nTo better understand the nature of these phenomena, we ran computer simulations of small\nAtomspaces involving nodes and Inheritance relations, according to the following idea:\n\n1. Each node is assumed to denote a certain perceptual category\n2. For simplicity, we assume an environment in which the probability distribution of co occurrences between items in the different categories is stationary over the time period\nof the inference under study\n3. We assume the collection of nodes and links has its probabilistic strengths updated period ically, according to some “inference” process\n4. We assume that the results of the inference process in Step 3 and the results of incorporating\nnew data from the environment (Step 2) are merged together ongoingly via a weighted averaging belief-revision process\n\nIn our simulations Step 3 was carried out via executions of PLN deduction and inversion\ninference rules. The results of these simulations were encouraging: most of the time, the strengths\nof the nodes and links, after a while, settled into a “fixed point” configuration not too distant\nfrom the actual probabilistic relationships implicit in the initial data. The final configuration\nwas rarely equivalent to the initial configuration, but, it was usually close.\nFor instance one experiment involved 1000 random “inference triangles” involving 3 links,\nwhere the nodes were defined to correspond to random subsets of a fixed finite set (so that\ninheritance probabilities were defined simply in terms of set intersection). Given the specific\ndefinition of the random subsets, the mean strength of each of the three inheritance relationships\nacross all the experiments was about .3. The Euclidean distance between the 3-vector of the\nfinal (fixed point) link strengths and the 3-vector of the initial link strengths was roughly .075.\nSo the deviation from the true probabilities caused by iterated inference was not very large.\nQualitatively similar results were obtained with larger networks.\nThe key to these experiments is the revision in Step 4: It is assumed that, as iterated inference\nproceeds, information about the true probabilities is continually merged into the results of\ninference. If not for this, Step 3 on its own, repeatedly iterated, would lead to noise amplification\nand increasingly meaningless results. But in a realistic inference context, one would never simply\nrepeat Step 3 on its own. Rather, one would carry out inference on a node or link only when there\nwas new information about that node or link (directly leading to a strength update), or when\nsome new information about other nodes/links indirectly led to inference about that node-link.\nWith enough new information coming in, an inference system has no time to carry out repeated,\nuseless cycles of inference on the same nodes/links – there are always more interesting things\nto assign resources to. And the ongoing mixing-in of new information about the true strengths\nwith the results of iterated inference prevents the pathologies of circular inference, without the\nneed for a trail mechanism.\nWhat we see from these various experiments is that if one uses an inference control mechanism\nthat avoids the repeated conduction of inference steps in the absence of infusion of new data,\nissues with circular inference are not severe, and trails are not necessary to achieve reasonable\nnode and link strengths via iterated inference. Circular inference can occur without great harm,\nso long as one only does it when relevant new data is coming in, or when there is evidence that it\nis generating information. This is not to say that trail mechanisms are useless in computational\nsystems – they provide an interesting and sometimes important additional layer of protection\nagainst circular inference pathologies. But in an inference system that is integrated with an\nappropriate control mechanism they are not required. The errors induced by circular inference,\nin practice, may be smaller than many other errors involved in realistic inference. For instance,\nin the mapping between the brain and uncertain term logic proposed above, we have relied\nupon a fairly imprecise proportionality between virtual synapse weight and neural inheritance.\nWe are not attempting to argue that the brain implements precise probabilistic inference, but\nonly an imprecise analogue. Circular inference pathologies are probably not the greatest source\nof imprecision."
        },
        {
          "type": "section",
          "level": 1,
          "id": "F.3",
          "title": "Neural Representation of Recursion and Abstraction",
          "text": "The material of the previous subsection comprises a speculative but conceptually coherent\nconnection between brain structures and dynamics on the one hand, and probabilistic logic\nstructures and dynamics on the other. However, everything we have discussed so far deals only\nwith first-order term logic, i.e. the logic of inheritance relationships between terms.\nExtension to handle similarity relationships, intensional inheritance and so forth is straight forward – but what about more complex term logic constructs, such as would conventionally be\nexpressed using variables and quantifiers. In this section we seek to address this shortcoming,\nvia proposing a hypothesis as to how probabilistic term logic in its full generality might be\ngrounded in neural operations. This material is even more speculative than the above ideas, yet\nsomething of this nature is critically necessary for completing the conceptual picture.\nThe handling of quantifiers, in itself, is not the hard part. We have noted above that, in a\nterm logic framework, if one can handle probabilistic variable-bearing expressions and functions,\nthen one can handle quantifiers attached to the variables therein. So the essence of the problem\nis how to handle variables and functions. And we suggest that, when one investigates the\nissue in detail, a relatively simple hypothesis emerges clearly as essentially the only plausible\nexplanation, if one adopts the neural assembly theory as a working foundational assumption.\nIn the existing body of mathematical logic and theoretical computer science, there are two\nmain approaches to handling higher-order expressions: variable-based, or combinator-based [?,?]. It seems highly implausible, to us, that the human brain is implementing some sort of\nintricate variable-management scheme on the neural-assembly level. Lambda-calculus and other\nformal schemes for manipulating variables, appear to us to require complexity and precision\nof a style that self-organizing neural networks are ill-suited to produce via their own style of\ncomplex dynamics. Of course it is possible to engineer neural nets that do lambda calculus (as\nneural nets are Turing-complete), but this sort of neural-net structure seems unlikely to emerge\nvia evolution, and unlikely to get created via known epigenetic processes.\nBut what about combinators? Here, it seems to us, things are a bit more promising. Combi nators are higher-order functions; functions that map functions into functions; functions that\nmap {functions that map functions into functions} into {functions that map functions into\nfunctions}, and so forth. There are specific sets of combinators that are known to give rise to\nuniversal computational capability; indeed, there are many such specific sets, and one approach\nto the implementation of functional programming languages is to craft an appropriate set of\ncombinators that combines universality with tractability (the latter meaning, basically, that the\ncombinators have relatively simple definitions; and that pragmatically useful logic expressions\ntend to have compact representations in terms of the given set of combinators).\nWe lack the neurodynamic knowledge to say, at this point, that any particular set of combi nators seems likely to map into brain function. However, we may still explore the fundamental\nneural functionalities that would be necessary to give rise to a combinatory-logic-style foun dation for abstract neural computation. Essentially, what is needed is the capability to supply\none neural assembly as an input to another. Note that what we are talking about here is quite\ndifferent from the standard notion of chaining together neural assemblies, so that the output\nof assembly A becomes the input of assembly B. Rather, what we are talking about is that\nassembly A itself – as a mapping from inputs to outputs – is fed as an input to assembly B. In\nthis case we may call B a higher-order neural assembly.\nOf course, there are numerous possible mechanisms via which higher-order neural assemblies\ncould be implemented in the brain. Here we will discuss just one. Consider a neural assembly\nA1 with certain input neurons, certain output neurons, and certain internal “hidden layer”\nneurons. Then, suppose there exists a “router” neural assembly X, which is at the receiving end\nof connections from many neurons in A1, including input, output and hidden neurons. Suppose\nX is similarly connected to many other neural assemblies: A2, A3, ... and so forth; and suppose\nX contains a “control switch” input that tells it which of these assemblies to pay attention (so,\nfor instance, if the control input is set to 3, then X receives information about A3). When X\nis paying attention to a certain assembly, it routes the information it gets from that assembly\nto its outputs. (Going further, we may even posit a complex control switch, that accepts more\ninvolved commands; say, a command that directs the router to a set of k of its input assemblies,\nand also points it to small neural assembly implementing a combination function that tells it\nhow to combine these k assemblies to produce a composite.)\nFinally, suppose the input neurons of assembly B are connected to the router assembly X.\nThen, depending on how the router switch is set, B may be said to receive one of the assemblies\nAk as input. And, next, suppose B’s output is directed to the control switch of the router. Then,\nin effect, B is mapping assemblies to assemblies, in the manner of a higher-order function. And\nof course, B itself is “just another neural assembly,” so that B itself may be routed by the router,\nallowing for assemblies that map {assemblies mapping assemblies} into assemblies, and so forth.\nWhere might this kind of “router” assembly exist in the brain? We don’t know, at the mo ment. Quite possibly, the brain may implement higher-order functions by some completely\ndifferent mechanism. The point we want to make however, is that there are concrete possi bilities via which the brain could implement higher-order logic according to combinatory-logic\ntype mechanisms. Combinators might be neurally represented as neural assemblies interacting\nwith a router assembly, as hypothesized above, and in this way the Hebbian logic mechanisms\nproposed in the previous sections could be manifested more abstractly, allowing the full-scope\nof logical reasoning to occur among neural assemblies, with uncertainty management mediated\nby Hebbian-type synaptic modification."
        }
      ]
    },
    {
      "id": "G",
      "title": "Possible Worlds Semantics and Experiential Semantics",
      "content": [
        {
          "type": "section",
          "level": 1,
          "id": "G.1",
          "title": "Introduction",
          "text": "The relevance of logic to AGI is often questioned, on the grounds that logic manipulates abstract\nsymbols, but once you’ve figured out how to translate concrete perception and action into\nabstract symbols in an appropriate way, you’ve already solved the hard part of the AGI problem.\nIn this view, human intelligence does logic-like processing as a sort of epiphenomenon, on top\nof a deeper and more profound layer of sub symbolic processing; and logic is more suitable\nas a high-level description that roughly approximates the abstract nature of certain thought\nprocesses, than as a method of actually realizing these thought processes.\nOur own view is that logic is a flexible tool which may be used in many different ways. For\nexample, there is no particular reason not to use logic directly on sensory and actuator data,\nor on fairly low-level abstractions thereof. This hasn’t been the tradition in logic or logic-based\nAI, but this is a matter of culture and historical accident more than anything else. This would\ngive rise to difficult scalability problems, but so does the application of recurrent neural nets\nor any other powerful learning approach. In CogPrime we propose to handle the lowest level of\nsensory and actuator data in a different way, using a CSDLN such as DeSTIN, but we actually\nbelieve a PLN approach could be used in place of a system like DeSTIN, without significant\nloss of efficiency, and only moderate increase in complexity. For example, one could build a\nCSDLN whose internal operations were all PLN-based – this would make the compositional\nspatiotemporal hierarchical structure, in effect, into an inference control mechanism.\nIn this appendix we will explore this region of conceptual space via digging deeper into\nthe semantics of PLN, looking carefully and formally at the connection between PLN terms\nand relationships, and the concrete experience of an AI system acting in a world. As well as\nproviding a more rigorous foundation for some aspects of the PLN formalism, the underlying\nconceptual purpose is to more fully explicate the relationship between PLN and the world a\nCogPrime controlled agent lives in.\nSpecifically, what we treat here is the relation between experiential semantics (on which\nPLN, and the formal model of intelligent agents presented in Chapter ?? of Part 1, are both\nfounded) and possible-worlds semantics (which forms a more mathematically and conceptually\nnatural foundation for certain aspects of logic, including certain aspects of PLN). In “experien tial semantics”, the meaning of each logical statement in an agent’s memory is defined in terms\nof the agent’s experiences. In “possible worlds semantics”, the meaning of a statement is defined\nby reference to an ensemble of possible worlds including, but not restricted to, the one the\nagent interpreting the statement has experienced. In this appendix, for the first time, we for mally specify the relation between these two semantic approaches, via providing an experiential\ngrounding of possible worlds semantics. We show how this simplifies the interpretation of several\naspects of PLN, providing a common foundation for setting various PLN system parameters\nthat were previously viewed as distinct.\nThe reader with a logic background should note that we are construing the notion of possible\nworlds semantics broadly here, in the philosophical sense [?], rather than narrowly in the sense\nof Kripke semantics [?] and its relatives. In fact there are interesting mathematical connections\nbetween the present formulation and Kripke semantics and epistemic logic, but we will leave\nthese for later work.\nWe begin with indefinite probabilities recalled in Chapter ??, noting that the second-order\ndistribution involved therein may be interpreted using possible worlds semantics. Then we turn\nto uncertain quantifiers, showing that the third-order distribution used to interpret these in\n[?] may be considered as a distribution over possible worlds. Finally, we consider intensional\ninference, suggesting that the complexity measure involved in the definition of PLN intension\n[?] may be derived from a probability measure over possible worlds. The moral of the story is\nthat by considering the space of possible worlds implicit in an agent’s experience, one arrives at\na simpler unified view of various aspects of the agent’s uncertain reasoning, than if one grounds\nthese aspects in the agent’s experience directly. This is not an abandonment of experiential\nsemantics but rather an acknowledgement that a simple variety of possible worlds semantics is\nderivable from experiential semantics, and usefully deployable in the development of uncertain\ninference systems for general intelligence."
        },
        {
          "type": "section",
          "level": 1,
          "id": "G.2",
          "title": "Inducing a Distribution over Predicates and Concepts",
          "text": "First we introduce a little preliminary formalism. Given a distribution over environments as\ndefined in Chapter ?? of Part 1, and a collection of predicates evaluated on subsets of en vironments, we will find it useful to define distributions (induced by the distribution over\nenvironments) defining the probabilities of these predicates.\nSuppose we have a pair (F, T) where F is a function mapping sequences of perceptions into\nfuzzy truth values, and T is an integer connoting a length of time. Then, we can define the\nprior probability of (F, T) as the average degree to which F is true, over a random interval of\nperceptions of length T drawn from a random environment drawn from the distribution over\nenvironments. More generally, if one has a pair (F, f), where f is a distribution over the integers,\none can define the prior probability of (F, f) as the weighted average of the prior probability of\n(F, T) where T is drawn from f.\nWhile expressed in terms of predicates, the above formulation can also be useful for dealing\nwith concepts, e.g. by interpreting the concept cat in terms of the predicate isCat. So we can use\nthis formulation in inferences where one needs a concept probability like P(cat) or a relationship\nprobability like P(eat(cat, mouse))."
        },
        {
          "type": "section",
          "level": 1,
          "id": "G.3",
          "title": "Grounding Possible Worlds Semantics in Experiential Semantics",
          "text": "Now we explain how to ground a form of possible worlds semantics in experiential semantics.\nWe explain how an agent, experiencing a single stream of perceptions, may use this to construct\nan ensemble of possible worlds, which may then be used in various sorts of inferences. This may\nsound conceptually thorny, but on careful scrutiny it’s less so, and in fact is closely related to\na commonplace idea in the field of statistics: “subsampling.”\nThe basic idea of subsampling is that, if one has a single dataset D which one wishes to inter pret as coming from a larger population of possible datasets, and one wishes to approximately\nunderstand the distribution of this larger population, then one can generate a set of additional\ndatasets via removing various portions of D. Each time one removes a certain portion of D, one\nobtains another dataset, and one can then look at the distribution of these auxiliary datasets,\nconsidering it as a model of the population D is drawn from.\nThis notion ties in closely with the SRAM formal agents model of Chapter ?? of Part 1,\nwhich considers a probability distribution over a space of environments which are themselves\nprobability distributions. What a real agent has is actually a single series of remembered ob servations. But it can induce a hopeful approximation of this distribution over environments\nby subsampling its memory and asking: \"What would it imply about the world if the items in\nthis subsample were the only things I’d seen?\"\nIt may be conceptually useful to observe that a related notion to subsampling is found in the\nliterary methodology of science fiction (SF). Many SF authors have followed the methodology\nof starting with our everyday world, and then changing one significant aspect, and depicting the\nworld as they think it might exist if this one aspect were changed (or, a similar methodology\nmay be followed via changing a small number of aspects). This is a way of generating a large\nvariety of alternate possible worlds from the raw material of our own world.\nApplied to SRAM, the subsampling and SF analogies suggest two methods of creating a\npossible world (and hence, by repetition, an ensemble of possible worlds) from the agent’s ex perience. An agent’s interaction sequence with its environment, ay<t = ay1:t−1, forms a sample\nfrom which it wishes to infer its environment µ(yk|ay<kak). To better assess this environment,\nthe agent may, for example,\n\n• create a possible world by removing a randomly selected collection of interactions from\nthe agent’s memory. In this case, the agent’s interaction sequence would be of the form\nIg,s,t(nt) = ay(nt) where (nt) is some subsequence of 1:t − 1.\n• create a possible world via assuming a counterfactual hypothesis (i.e. assigning a statement\na truth value that contradicts the agent’s experience), and using inference to construct a\nset of observations that is as similar to its memory as possible, subject to the constraint of\nbeing consistent with the hypothesis. The agent’s interaction sequence would then look like\nbz1:t−1, where some collection of the bkzk differ from the corresponding akyk.\n• create a possible world by reorganizing portions of the interaction sequence.\n• create a possible world by some combination of the above.\n\nWe denote an alteration of an interaction sequence I_g,s,t^a for an agent a by I˜_g,s,t^a, and the set\nof all such altered interaction sequences for agent a by I^a.\nIn general, an agent’s interaction sequence will presumably be some reasonably likely se quence, and we would therefore be most interested in those cases for which dI (I_g,s,t^a, I˜_g,s,t^a) is\nsmall, where dI (·, ·) is some measure of sequence similarity, such as neighborhood correlation or\nPSI-BLAST. The probability distribution ν over environments µ will then tend to give larger\nprobabilities to nearby sequences, as measured by the chosen similarity measure, than to ones\nthat are far away. In colloquial terms, an agent would typically be interested in considering\nonly minor hypothetical changes to its interaction sequences, and would have little basis for\nunderstanding the consequences of drastic alterations.\nAny of the above methods for altering interaction sequences would alter an agent’s perception\nsequence causing changes to the fuzzy truth values mapped by the function F. This in turn would\nyield new probability distributions over the space of possible worlds, and thereby yielding altered\naverage probability values for the pair (F, T). This change, constructed from the perspective of\nthe agent based on its experience, could then cause the agent to reassess its action a. Broadly\nspeaking, we call these approaches “experiential possible worlds” or EPW.\nThe creation of altered interaction sequences may, under appropriate assumptions, provide\na basis for creating better estimates for the predicate F than we would otherwise have from a\nsingle real-world data point. More specifically we have the following results.\n\nTheorem 1 Let En represent an arbitrary ensemble of n agents chosen from A. Suppose that,\non average over the set of agents a ∈ En, the set of values F(I) for mutated interaction sequences\nI is normal and unbiased, so that,\nE[F] = (1/n) Σ_{a∈En} Σ_{I_g,s,t^a ∈ I^a} F(I_g,s,t^a) P(I_g,s,t^a).\n\nSuppose further that these agents explore their environments by creating hypothetical worlds via\naltered interaction sequences. Then an unbiased estimate for E[F] is given by\nFˆ = (1/n) Σ_{a∈En} Σ_{I˜_g,s,t^a ∈ I^a} F(I˜_g,s,t^a) P(I˜_g,s,t^a)\n= (1/n) Σ_{a∈En} Σ_{I˜_g,s,t^a ∈ I^a} F(I˜_g,s,t^a) [ Σ_{e∈E} P(e|I_g,s,t^a) P(I˜_g,s,t^a|e) ].\n\nProof. That Fˆ is an unbiased estimate for E[F] follows as a direct application of standard\nstatistical bootstrapping theorems. See, for example, [?].\n\nTheorem 2 Suppose that in addition to the above assumptions, we assume that the predicate\nF is Lipschitz continuous as a function of the interaction sequences I_g,s,t^a. That is,\ndF( F(I˜_g,s,t^a), F(I_g,s,t^a) ) ≤ K dI(I˜_g,s,t^a, I_g,s,t^a),\nfor some bound K and dF(·, ·) is a distance measure in predicate space. Then, setting both\nthe bias correction and acceleration parameters to zero, the bootstrap BCα confidence interval\nfor the mean of F satisfies\nFˆ_{BCα} [α] ⊂ [Fˆ − K z^(α) σˆ_I , Fˆ + K z^(α) σˆ_I ]\nwhere σˆ_I is the standard deviation for the altered interaction sequences and, letting Φ denote\nthe standard normal c.d.f., z^(α) = Φ^{−1}(α).\nProof. Note that the Lipschitz condition gives\nσˆ_F^2 = [1 / (n|I^a| − 1)] × Σ_{a∈En} Σ_{I˜_g,s,t^a ∈ I^a} d_F^2( F(I˜_g,s,t^a), F(I_g,s,t^a) ) P(I˜_g,s,t^a)\n≤ [K^2 / (n|I^a| − 1)] Σ_{a∈En} Σ_{I˜_g,s,t^a ∈ I^a} d_I^2( I˜_g,s,t^a, I_g,s,t^a ) P(I˜_g,s,t^a)\n= K^2 σˆ_I^2 .\nSince the population is normal and the bias correction and acceleration parameters are both\nzero, the BCα bootstrap confidence interval reduces to the standard confidence interval, and\nthe result then follows [?].\n\nThese two theorems together imply that, on average, through subsampling via altered interac tion sequences, agents can obtain unbiased approximations to F and, by keeping the deviations\nfrom their experienced interaction sequence small, the deviations of their approximations will\nalso be small.\nWhile the two theorems above demonstrate the power of our subsampling approach, the Lip schitz condition in theorem 2 is a strong assumption. This observation motivates the following\nmodification that is more in keeping with the flavor of PLN’s indefinite probabilities approach.\n\nTheorem 3 Define the set\nI^{a;b} = { I˜_g,s,t^a | d_F^2( F(I˜_g,s,t^a), F(I_g,s,t^a) ) = b },\nand assume that for every real number b the perceptions of the predicate F satisfy\n(1/n) Σ_{a∈En} P(I^{a;b}) ≤ [M(b) / (b^2 σ_I^2)]\nfor some M(b) ∈ R. Further suppose that\n∫_0^1 M(b) db = M^2 ∈ R.\nThen under the same assumptions as in Theorem 1, and again setting both the bias correction\nand acceleration parameters to zero, we have\nFˆ_{BCα} [α] ⊂ [Fˆ − M √n z^(α) σ_I , Fˆ + M √n z^(α) σ_I ]\nProof.\nσˆ_F^2 = [1 / (n · |I^a| − 1)] × Σ_{a∈En} Σ_{I˜_g,s,t^a ∈ I^a} d_F^2( F(I˜_g,s,t^a), F(I_g,s,t^a) ) P(I˜_g,s,t^a)\n= [1 / (n · |I^a| − 1)] × Σ_{a∈En} ∫_0^1 Σ_{I˜_g,s,t^a ∈ I^{a;b}} d_F^2( F(I˜_g,s,t^a), F(I_g,s,t^a) ) P(I˜_g,s,t^a) db\n≤ [1 / (n · |I^a| − 1)] × Σ_{a∈En} ∫_0^1 Σ_{I˜_g,s,t^a ∈ I^{a;b}} d_F^2( F(I˜_g,s,t^a), F(I_g,s,t^a) ) P(I˜_g,s,t^a) db\n≤ [b^2 n M^2 / (b^2 σ_I^2)] = (M √n)^2 σ_I^2 ."
        },
        {
          "type": "section",
          "level": 1,
          "id": "G.4",
          "title": "Reinterpreting Indefinite Probabilities",
          "text": "Indefinite probabilities (see Chapter ??) provide a natural fit with the experiential semantics\nof the SRAM model, as well as with the subsampling methodology articulated above. A truth value for an indefinite probability takes the form of a quadruple ([L, U], b, k). The meaning\nof such a truth-value, attached to a statement S is, roughly: There is a probability b that, after\nk more observations, the truth value assigned to the statement S will lie in the interval [L, U].\nWe interpret an interval [L, U] by assuming some particular family of distributions (usually\nBeta) whose means lie in [L, U].\nTo execute inferences using indefinite probabilities, we make heuristic distributional assump tions, assuming a “first order” distribution of means, with [L, U ] as a (100·b)% credible interval.\nCorresponding to each mean in this “first-order“ distribution is a “second order” distribution,\nproviding for an “envelope” of distributions.\nThe resulting bivariate distribution can be viewed as an heuristic approximation intended to\nestimate unknown probability values existing in hypothetical future situations. Combined with\nadditional parameters, each indefinite truth-value object essentially provides a compact repre sentation of a single second-order probability distribution with a particular, complex structure.\nIn the EPW context, the second-order distribution in an indefinite probability is most nat urally viewed as a distribution over possible worlds; whereas, each first-order distribution rep resents the distribution of values of the proposition within a given possible world.\nAs a specific example, consider the case of two virtual agents: one agent, with cat-like charac teristics, called “Fluffy” and the second a creature, with dog-like characteristics, named “Muffin.”\nUpon a meeting of the two agents, Fluffy might immediately consider three courses of action:\nFluffy might decide to flee as quickly as possible, might hiss and threaten Muffin, or might\ndecide to remain quiet and still. Fluffy might have a memory store of perception sequences\nfrom prior encounters with agents with similar characteristics to those of Muffin.\nIn this scenario, one can view the second-order distribution, as a distribution over all three\ncourses of action that Fluffy might take. Each first-order distribution would then represent\nthe probability distribution of the result from the corresponding action. By hypothetically\nconsidering all three possible courses of action and the probability distributions of the resulting\naction, Fluffy can make more rational decisions even though no result is guaranteed.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "G.4.1",
              "title": "Reinterpreting Indefinite Quantifiers",
              "text": "EPW also allows PLN’s universal, existential and fuzzy quantifiers to be expressed in terms of\nimplications on fuzzy sets. For example, if we have\nForAll $X\n  Implication\n    Evaluation F $X\n    Evaluation G $X\nthen this is equivalent to\nAverageQuantifier $X\n  Implication\n    Evaluation F* $X\n    Evaluation G* $X\nwhere e.g. F* is the fuzzy set of variations on F constructed by assuming possible errors in\nthe historical evaluations of F. This formulation yields equivalent results to the one given in\n[?], but also has the property of reducing quantifiers to FOPLN (over sets derived from special\npredicates).\nTo fully understand the equivalence of the above two expressions, first note that in [?],\nwe handle quantifiers by introducing third-order probabilities. As discussed there, the three\nlevels of distributions are roughly as follows. The first- and second-order levels play the role,\nwith some modifications, of standard indefinite probabilities. The third-order distribution then\nplays the role of “perturbing” the second-order distribution. The idea is that the second-order\ndistribution represents the mean for the statement F(x). The third-order distribution then\ngives various values of F(x) for x, and the first-order distribution gives the sub-distributions\nfor each of the second-order distributions. The final result is then found via an averaging\nprocess on all those second-order distributions that are “almost entirely” contained in some\nForAll_proxy_interval.\nNext, AverageQuantif ier F($X) is a weighted average of F($X) over all relevant inputs\n$X; and we define the fuzzy set F* as the set of perturbations of a second-order distribution\nof hypotheses, and G* as the corresponding set of perturbed implication results. With these\ndefinitions, not only does the above equivalence follow naturally, so do the “possible/perturbed\nworlds” semantics for the ForAll quantifier. Other quantifiers, including fuzzy quantifiers, can\nbe similarly recast."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "G.5",
          "title": "Specifying Complexity for Intensional Inference",
          "text": "A classical dichotomy in logic involves the distinction between extensional inference (which in volves sets with members) and intensional inference (which involves entities with properties).\nIn PLN this is handled by taking extension as the foundation (where, in accordance with expe riential semantics, sets ultimately boil down to sets of elementary observations), and defining\nintension in terms of certain fuzzy sets involving observation-sets. This means that in PLN\nintension, like higher-order inference, ultimately emerges as a subcase of FOPLN (though a\nsubcase with special mathematical properties and special interest for cognitive science and AI).\nHowever, the prior formulation of PLN intension contains a “free parameter” (a complexity mea sure) which is conceptually inelegant; EPW remedies this via providing this parameter with a\nfoundation in possible worlds semantics.\nTo illustrate how, in PLN, higher-order intensional inference reduces to first-order infer ences, consider the case of intensional inheritance. IntensionalInheritance A B measures the\nextensional inheritance between the set of properties or patterns associated with A and the\ncorresponding set associated with B. This concept is made precise via formally defining the\nconcept of “pattern,“ founded on the concept of “association.” We formally define the associa tion operator ASSOC through:\nExtensionalEquivalence\n  Member $E (ExOut ASSOC $C)\n  ExOut\n    Func\n      List\n        ExtensionalInheritance $E $C\n        ExtensionalInheritance\n          NOT $E\n          $C\nwhere Func(x, y) = [x − y]^+ and + denotes the positive part.\nWe next define a pattern in an entity A as something that is associated with, but simpler\nthan, A. Note that this definition presumes some measure c() of complexity. One can then define\nthe fuzzy-set membership function called the “pattern-intensity,\" via\nIN(F, G) = [c(G) − c(F)]^+ [P(F|G) − P(F|¬G)]^+.\nmeasuring how much G is a pattern of F. The complexity measure c has been left unspecified\nin prior explications of PLN, but in the present context we may take it as the measure over\nconcepts implied by the measure over possible worlds derived via subsampling or counterfactuals\nas described above."
        },
        {
          "type": "section",
          "level": 1,
          "id": "G.6",
          "title": "Reinterpreting Implication between Inheritance Relationships",
          "text": "Finally, one more place where possible worlds semantics plays a role in PLN is with implications\nsuch as\nImplication\n  Inheritance Ben American\n  Inheritance Ben obnoxious\nWe can interpret these by introducing predicates over possible worlds, so that e.g.\nZ_Inheritance_Ben_American(W)tv\ndenotes that tv is the truth value of Inheritance_Ben_American in world W. A prerequisite\nfor this, of course, is that Ben and American be defined in a way that spans the space of\npossible worlds in question. In the case of possible worlds defined by differing subsets of the same\nobservation-set, this is straightforward; in the case of possible worlds defined via counterfactuals\nit is subtler and we will omit details here.\nThe above implication may then be interpreted as\nAverageQuantifier $W\n  Implication\n    Evaluation Z_Inheritance_Ben_American $W\n    Evaluation Z_Inheritance_Ben_obnoxious $W\nThe weighting over possible worlds $W may be taken as the one obtained by the system through\nthe subsampling or counterfactual methods as indicated above."
        },
        {
          "type": "section",
          "level": 1,
          "id": "G.7",
          "title": "Conclusion",
          "text": "We began with the simple observation that the mind of an intelligent agent accumulates knowl edge based on experience, yet also creates hypothetical knowledge about “the world as it might\nbe,” which is useful for guiding future actions. PLN handles this dichotomy via beginning from a\nfoundation in experiential semantics, but then using a form of experientially-grounded possible worlds semantics to ground a number of particular logical constructs, which we have reviewed\nhere. The technical details we have provided illustrate the general thesis that a combination\nof experiential and possible-worlds notions may be the best approach to comprehending the\nsemantics of declarative knowledge in generally intelligent agents."
        }
      ]
    },
    {
      "id": "H",
      "title": "Propositions About Environments in Which CogPrime Components Are Useful",
      "content": [
        {
          "type": "section",
          "level": 1,
          "id": "H.1",
          "title": "Propositions about MOSES",
          "text": "Why is MOSES a good approach to automated program learning? The conceptual argument in\nfavor of MOSES may be broken down into a series of propositions, which are given here both\nin informal “slogan” form and in semi-formalized “proposition” form.\nNote that the arguments given here appear essentially applicable to other MOSES-related\nalgorithms such as Pleasure as well. The page however was originally written in regard to\nMOSES and hasn’t been revised in the light of the creation of Pleasure.\nSlogan 1 refers to “ENF”, Elegant Normal Form, which is used by MOSES as a standard\nformat for program trees. This is a way that MOSES differs from GP for example: GP does not\ntypically normalize program trees into a standard syntactic format, but leaves trees heteroge neous as to format.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "H.1.1",
              "title": "Proposition: ENF Helps to Guide Syntax-Based Program Space Search",
              "text": "Slogan 1\nIterative optimization is guided based on syntactic distance ==> ENF is good\nProposition 1:\nOn average, over a class C of fitness functions, it is better to do optimization based on a\nrepresentation in which the (average over all functions in C of the) correlation between syntactic\nand semantic distance is larger. This should hold for any optimization algorithm which makes a\nseries of guesses, in which the new guesses are chosen from the old ones in a way that is biased\nto choose new guesses that have small syntactic distance to the old one.\nNote that GA, GP, BOA, BOAP and MOSES all fall into the specified category of optimiza tion algorithms\nIt is not clear what average smoothness condition is useful here. For instance, one could look\nat the average of d(f(x),f(y))/d(x,y) for d(x,y)<A, where d is syntactic distance and A is chosen\nso that the optimization algorithm is biased to choose new guesses that have syntactic distance\nless than A from the old ones."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.1.2",
              "title": "Demes are Useful if Syntax/Semantics Correlations in Program Space Have a Small Scale",
              "text": "This proposition refers to the strategy of using “demes” in MOSES: instead of just evolving\none population of program trees, a collection of “demes” are evolved, each one a population of\nprogram trees that are all somewhat similar to each other.\nSlogan 2 Small-scale syntactic/semantic correlation ==> demes are good [If the maximal\nsyntactic/semantic correlation occurs on a small scale, then multiple demes are useful]\nProposition 2: Let d denote syntactic distance, and d1 denote semantic distance. Sup pose that the correlation between d(x,y) and d1(x,y) is much larger for d(x,y)<A than for\nA<d(x,y)<2A or A<d(x,y), as an average across all fitness functions in class C. Suppose the\nnumber of spheres of radius R required to cover the space of all genotypes is n(R). Then using\nn(R) demes will provide significantly faster optimization than using n(2R) demes or 1 deme.\nAssume here the same conditions on the optimization algorithm as in Proposition 1.\nProposition 2.1: Consider the class of fitness functions defined by\nCorrelation( d(x,y), d1(x,y) || d(x,y) = a ) = b\nThen, across this class, there is a certain number D of demes that will be optimal on average....\nI.e. the optimal number of demes depends on the scale-dependence of the correlation between\nsyntactic & semantic distance...."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.1.3",
              "title": "Probabilistic Program Tree Modeling Helps in the Presence of Cross-Modular Dependencies",
              "text": "This proposition refers to the use of BOA-type program tree modeling within MOSES. What\nit states is that this sort of modeling is useful if the programs in question have significant\ncross-modular dependences that are not extremely difficult to detect.\nSlogan 3 Cross-modular dependencies ==> BOA is good [If the genotypes possess sig nificant internal dependencies that are not concordant with the genotypes’ internal modular\nstructure, then BOA-type optimization will significantly outperform GA/GP-type optimization\nfor deme-exemplar extension.]\nProposition 3: Consider the classification problem of distinguishing fit genotypes from less\nfit genotypes, within a deme. If significantly greater classification accuracy can be obtained by\nclassification rules containing “cross-terms” combining genotype elements that are distant from\neach other within the genotypes - and these cross-terms are not too large relative to the increase\nin accuracy they provide - then BOA-type modeling will significantly outperform GA/GP-type\noptimization.\nThe catch in Proposition 3 is that the BOA-type modeling must be sophisticated enough to\nrecognize the specific cross-terms involved, of course."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.1.4",
              "title": "Relating ENF to BOA",
              "text": "Now, how does BOA learning relate to ENF?\nProposition 4: ENF decreases, on average, the number and size of cross-terms in the clas sification rules mentioned in Proposition 3."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.1.5",
              "title": "Conclusion Regarding Speculative MOSES Theory",
              "text": "What we see from the above is that:\n\n• ENF is needed in order to make the fitness landscape smoother, but can almost never\nwork perfectly so there will nearly always be some long-distance dependencies left after\nENF-ization\n• The smoother fitness landscape enabled by ENF, enables optimization using demes and\nincremental exemplar-expansion to work, assuming the number of demes is chosen intelli gently\n• Within a deme, optimization via incremental exemplar growth is more efficient using BOA\nthan straight evolutionary methods, due to the ability of BOA to exploit the long-distance\ndependencies not removed by ENF-ization\n\nThese propositions appear to capture the basic conceptual justification for the current MOSES\nmethodology. Of course, proving them will be another story, and will likely involve making the\nproposition statements significantly more technical and complex.\nAnother interesting angle on these propositions is to view them as constraints on the problem\ntype to which MOSES may be fruitfully applied. Obviously, no program learning algorithm can\noutperform random search on random program learning problems. MOSES, like any other\nalgorithm, needs to be applied to problems that match its particular biases. What sorts of\nproblems match MOSES’s biases?\nIn particular, the right question to ask is: Given a particular choice regarding syntactic\nprogram representation, what sorts of problems match MOSES’s biases as induced by this\nchoice?\nIf the above propositions are correct, the answer is, basically: Problems for which semantic\ndistance (distance in fitness) is moderately well-correlated with syntactic distance (in the chosen\nrepresentation) over a short scale but not necessarily over a long scale, and for which a significant\npercentage of successful programs have a moderate but not huge degree of internal complexity\n(as measured by internal cross-module dependencies).\nImplicit in this is an explanation of why MOSES, on its own, is likely not a good approach\nto solving extremely large and complex problems. This is because for an extremely large and\ncomplex problem, the degree of internal complexity of successful programs will likely be too\nhigh for BOA modeling to cope with. So then, in these cases MOSES will effectively operate as\na multi-start local search on normalized program trees, which is not a stupid thing, but unlikely\nto be adequately effective for most large, complex problems.\nWe see from the above that even in the case of MOSES, which is much simpler than OCP,\nformulating the appropriate theory adequately is not a simple thing, and proving the relevant\npropositions may be fairly difficult. However, we can also see from the MOSES example that\nthe creation of a theoretical treatment does have some potential for clarifying the nature of the\nalgorithm and its likely range of applicability."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "H.2",
          "title": "Propositions About CogPrime",
          "text": "We present some speculations regarding the extension of the approach to MOSES-theory pre sented above to handle OCP in general. This is of course a much more complex and subtle\nmatter, yet we suggest that in large part it may be handled in a similar way. This way of\nthinking provides a different perspective on the OCP design - one that has not yet substantially\nimpacted the practical aspects of the design, but may well be of use to us as we iteratively\nrefine the design in the future, in the course of testing and teaching OCP AGI systems.\nAs with the propositions in previous section but even more so, the details of these heuristic\npropositions will likely change a fair bit when/if rigorous proof/statement is attempted. But\nwe are intuitively fairly confident that the basic ideas described here will hold up to rigorous\nanalysis.\nFinally, one more caveat: the set of propositions listed here is not presented as complete. By\nno means! A complete theoretical treatment of OCP, along these lines, would involve a more\nsubstantial list of related propositions. The propositions given here are meant to cover a number\nof the most key points, and to serve as illustrations of the sort of AGI theory we believe/hope\nmay be possible to do in the near and medium term future.",
          "subsections": [
            {
              "type": "section",
              "level": 2,
              "id": "H.2.1",
              "title": "When PLN Inference Beats BOA",
              "text": "This proposition explains why, in some cases, it will be better to use PLN rather than BOA\nwithin MOSES, for modeling the dependencies within populations of program trees.\nSlogan 5 Complex cross-modular dependencies which have similar nature for similar fitness\nfunctions ==> PLN inference is better than BOA for controlling exemplar extension\nProposition 5: Consider the classification problem of distinguishing fit genotypes from less\nfit genotypes, within a deme. If\n\n• significantly greater classification accuracy can be obtained by classification rules containing\n“cross-terms” combining genotype elements that are distant from each other within the\ngenotypes, but\n• the search space for finding these classification rules is tricky enough that a greedy learning\nalgorithm like decision-tree learning (which is used within BOA) isn’t going to find the good\nones\n• the classification rules tend to be similar, for learning problems for which the fitness func tions are similar\n\nThen, PLN will significantly outperform BOA for exemplar extension within MOSES, due to\nits ability to take history into account."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.2",
              "title": "Conditions for the Usefulness of Hebbian Inference Control",
              "text": "Now we turn from MOSES to PLN proper. The approximate probabilistic correctness of PLN\nis handled via PLN theory itself, as presented in the PLN book. However, the trickiest part of\nPLN in practice is inference control, which in the OCP design is proposed to be handled via\n“experiential learning.” This proposition pertains to the conditions under which Hebbian-style,\ninductive PLN inference control can be useful.\nSlogan 6 If similar theorems generally have similar proofs, then inductively-controlled PLN\ncan work effectively\nProposition 6:\n\n• Let L0 = a simple “base level” theorem-proving framework, with fixed control heuristics\n• For n > 0, let Ln = theorem-proving done using Ln−1, with inference control done using\ndata mining over a DB of inference trees, utilizing Ln−1 to find recurring patterns among\nthese inference trees that are potentially useful for controlling inference\n\nThen, if T is a set of theorems so that, within T, theorems that are similar according to\n“similarity provable in Ln−1 using effort E” have proofs that are similar according to the same\nmeasure, then Ln will be effective for proving theorems within T"
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.3",
              "title": "Clustering-together of Smooth Theorems",
              "text": "This proposition is utilized within Theorem 8, below, which again has to do with PLN inference\ncontrol.\nSlogan 7 “Smooth” theorems tend to cluster together in theorem-space\nProposition 7: Define the smoothness of a theorem as the degree to which its proof is\nsimilar to the proofs of other theorems similar to it. Then, smoothness varies smoothly in\ntheorem-space. I.e., a smooth theorem tends to be close-by to other smooth theorems."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.4",
              "title": "When PLN is Useful Within MOSES",
              "text": "Above it was argued that PLN is useful within MOSES due to its capability to take account of\nhistory (across multiple fitness functions). But this is not the only reason to utilize PLN within\nMOSES; Propositions 6 and 7 above give us another theoretical reason.\nProposition 8: If similar theorems of the form “Program A is likely to have similar behavior\nto program B” tend to have similar proofs, and the conditions of Slogan 6 hold for the class\nof programs in question, then inductively controlled PLN is good (and better than BOA) for\nexemplar extension. (This is basically Proposition 6 + Proposition 7)"
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.5",
              "title": "When MOSES is Useful Within PLN",
              "text": "We have explored theoretical reasons why PLN should be useful within MOSES, as a replace ment for the BOA step used in the standalone implementation of MOSES. The next few propo sitions work in the opposite direction, and explore rasons why MOSES should be useful within\nPLN, for the specific problem of finding elements of a set given a qualitative (intensional) de scription of a set. (This is not the only use of MOSES for helping PLN, but it is a key use and\na fairly simple one to address from a theoretical perspective.)\nProposition 9: In a universe of sets where intensional similarity and extensional similarity\nare well-correlated, the problem of finding classification rules corresponding to a set S leads to a\npopulation of decently fit candidate solutions with high syntactic/semantic correlation so that\ndemes are good for this problem.\nProposition 10: In a universe of sets satisfying Proposition 9, where sets have properties\nwith complex interdependencies, BOA will be useful for exemplar extension (in the context of\nusing demes to find classification rules corresponding to sets).\nProposition 11: In a universe of sets satisfying Proposition 10, where the interdependencies\nassociated with a set S’s property-set vary “smoothly” as S varies, working inference is better\nthan BOA for exemplar extension.\nProposition 12: In a universe of sets satisfying Proposition 10, where the proof of theorems\nof the form “Both the interdependencies of S’s properties, and the interdependencies of T’s\nproperties, satisfy predicate F” depends smoothly on the theorem statement, then inductively\ncontrolled PLN will be effective for exemplar extension."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.6",
              "title": "On the Smoothness of Some Relevant Theorems",
              "text": "We have talked a bit about smooth theorems, but what sorts of theorems will tend to be\nsmooth? If the OCP design is to work effectively, the “relevant” theorems must be smooth; and\nthe following proposition gives some evidence as to why this may be the case.\nProposition 13: In a universe of sets where intensional similarity and extensional similarity\nare well-correlated, probabilistic theorems of the form “A is a probabilistic subset of B” and “A\nis a pattern in B” tend to be smooth.\nNote that: For a set S of programs, to say “intensional similarity and extensional similarity\nare well-correlated” among subsets of S, means the same thing as saying that syntactic and\nsemantic similarity are well-correlated among members of S.\nProposition 14: The set of motor control programs, for a set of standard actuators like\nwheels, arms and legs, displays a reasonable level of correlation between syntactic and semantic\nsimilarity.\nProposition 15: The set of sentences that are legal in English displays a high level of\ncorrelation between syntactic and semantic similarity.\n(The above is what, in Chaotic Logic [?], was called the “principle of continuous composi tionality”, extending Frege’s Principle of Compositionality. It implies that language is learnable\nvia OCP-type methods.... Unlike the other Propositions formulated here, it is more likely to be\naddressable via statistical than formal mathematical means; but insofar as English syntax can\nbe formulated formally, it may be considered a roughly-stated mathematical proposition.)"
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.7",
              "title": "Recursive Use of “MOSES with PLN” to Help With Attention Allocation",
              "text": "Proposition 16: The set of propositions of the form “When thinking about A is useful, think ing about B is often also useful” tends to be smooth - if “thinking” consists of MOSES plus\ninductively controlled PLN, and the universe of sets is such that this cognitive approach is\ngenerally a good one.\nThis (Prop. 16) implies that adaptive attention allocation can be useful for a MOSES+PLN\nsystem, if the attention allocation itself utilizes MOSES+PLN."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.8",
              "title": "The Value of Conceptual Blending",
              "text": "Proposition 17: In a universe of sets where intensional similarity and extensional similarity\nare well-correlated, if two sets A and B are often useful in proving theorems of the form “C is\na (probabilistic) subset of D”, then “blends” of A and B will often be useful for proving such\ntheorems as well.\nThis is a justification of conceptual blending for concept formation."
            },
            {
              "type": "section",
              "level": 2,
              "id": "H.2.9",
              "title": "A Justification of Map Formation",
              "text": "Proposition 18: If a collection of terms A is often used together in MOSES+PLN, then similar\ncollections B will often be useful as well, for this same process ... assuming the universe of sets\nis so that intensional and extensional similarity are correlated, and MOSES+PLN works well.\nThis is a partial justification of map formation, in that finding collections B similar to A is\nachieved by encapsulating A into a node A’ and then doing reasoning on A’."
            }
          ]
        },
        {
          "type": "section",
          "level": 1,
          "id": "H.3",
          "title": "Concluding Remarks",
          "text": "The above set of propositions is certainly not complete. For instance, one might like to throw in\nconjunctive pattern mining as a rapid approximation to MOSES; and some specific justification\nof artificial economics as a path to effectively utilizing MOSES/PLN for attention allocation;\netc.\nBut, overall, it seems fair to say that the above set of propositions smells like a possibly\nviable path to a theoretical justification of the OCP design.\nTo summarize the above ideas in a nutshell, we may say that the effectiveness of the OCP\ndesign appears intuitively to follow from the assumptions that:\n\n• within the space of relevant learning problems, problems defined by similar predicates tend\nto have somewhat similar solutions\n• according to OCP’s knowledge representation, procedures and predicates with very similar\nbehaviors often have very similar internal structures, and vice versa (and this holds to a\ndrastically lesser degree if the “very” is removed)\n• for relevant theorems (“theorems” meaning Atoms whose truth values need to be evaluated,\nor whose variables or SatisfyingSets need to be filled in, via PLN): similar theorems tend\nto have similar proofs, and the degree to which this holds varies smoothly in proof-space\n• the world can be well modeled using sets for which intensional and extensional similarity\nare well correlated: meaning that the mind can come up with a system of “extensional\ncategories” useful for describing the world, and displaying characteristic patterns that are\nnot too complex to be recognized by the mind’s cognitive methods\n\nTo really make use of this sort of theory, of course, two things would need to be done. For\none thing, the propositions would have to be proved (which will probably involve some serious\nadjustments to the proposition statements). For another thing, some detailed argumentation\nwould have to be done regarding why the “relevant problems” confronting an embodied AGI\nsystem actually fulfill the assumptions. This might turn out to be the hard part, because\nthe class of “relevant problems” is not so precisely defined. For very specific problems like - to\nname some examples quasi-randomly - natural language learning, object recogntion, learning to\nnavigate in a room with obstacles, or theorem-proving within a certain defined scope, however,\nit may be possible to make detailed arguments as to why the assumptions should be fulfilled.\nRecall that what makes OCP different from huge-resources AI designs like AIXI (including\nAIXItl) and the Gödel Machine is that it involves a number of specialized components, each with\ntheir own domains and biases and some with truly general potential as well, hooked together in\nan integrative architecture designed to foster cross-component interaction and overall synergy\nand emergence. The strength and weakness of this kind of architecture is that it is specialized\nto a particular class of environments. AIXItl and the Gödel Machine can handle any type of\nenvironment roughly equally well (which is: very, very slowly), whereas, CogPrime has the\npotential to be much faster when it’s in an environment that poses it learning problems that\nmatch its particular specializations. What we have done in the above series of propositions is to\npartially formalize the properties an environment must have to be “CogPrime-friendly.” If the\npropositions are essentially correct, and if interesting real-world environments largely satisfy\ntheir assumptions, then OCP is a viable AGI design."
        }
      ]
    }
  ]
}